src,src_title,src_abs,src_label,one_hop,one_hop_title,one_hop_abs,one_hop_label,two_hop,two_hop_title,two_hop_abs,two_hop_label,src_to_one,one_to_two,link,domain,src_keywords,one_hop_keywords,two_hop_keywords,src_to_one_relevance,one_to_two_relevance
411005,Title: GUESSING CAN OUTPERFORM MANY LONG TIME LAG ALGORITHMS,"Abstract: Numerous recent papers focus on standard recurrent nets' problems with long time lags between relevant signals. Some propose rather sophisticated, alternative methods. We show: many problems used to test previous methods can be solved more quickly by random weight guessing.",Neural_Networks,131318,Title: FLAT MINIMA Neural Computation 9(1):1-42 (1997),"Abstract: We present a new algorithm for finding low complexity neural networks with high generalization capability. The algorithm searches for a ""flat"" minimum of the error function. A flat minimum is a large connected region in weight-space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to ""simple"" networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require Gaussian assumptions and does not depend on a ""good"" weight prior instead we have a prior over input/output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second order derivatives, it has backprop's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms (1) conventional backprop, (2) weight decay, (3) ""optimal brain surgeon"" / ""optimal brain damage"". We also provide pseudo code of the algorithm (omitted from the NC-version).",Neural_Networks,13212,"Title: DISCOVERING NEURAL NETS WITH LOW KOLMOGOROV COMPLEXITY AND HIGH GENERALIZATION CAPABILITY Neural Networks 10(5):857-873, 1997","Abstract: Many neural net learning algorithms aim at finding ""simple"" nets to explain training data. The expectation is: the ""simpler"" the networks, the better the generalization on test data (! Occam's razor). Previous implementations, however, use measures for ""simplicity"" that lack the power, universality and elegance of those based on Kolmogorov complexity and Solomonoff's algorithmic probability. Likewise, most previous approaches (especially those of the ""Bayesian"" kind) suffer from the problem of choosing appropriate priors. This paper addresses both issues. It first reviews some basic concepts of algorithmic complexity theory relevant to machine learning, and how the Solomonoff-Levin distribution (or universal prior) deals with the prior problem. The universal prior leads to a probabilistic method for finding ""algorithmically simple"" problem solutions with high generalization capability. The method is based on Levin complexity (a time-bounded generalization of Kolmogorov complexity) and inspired by Levin's optimal universal search algorithm. For a given problem, solution candidates are computed by efficient ""self-sizing"" programs that influence their own runtime and storage size. The probabilistic search algorithm finds the ""good"" programs (the ones quickly computing algorithmically probable solutions fitting the training data). Simulations focus on the task of discovering ""algorithmically simple"" neural networks with low Kolmogorov complexity and high generalization capability. It is demonstrated that the method, at least with certain toy problems where it is computationally feasible, can lead to generalization results unmatchable by previous neural net algorithms. Much remains do be done, however, to make large scale applications and ""incremental learning"" feasible.",Neural_Networks,0,0,1,"
Summarization Title: [Neural Networks]
Summarization information: Guessing Can Outperform Many Long Time Lag Algorithms challenges sophisticated recurrent nets by demonstrating that random weight guessing can solve certain problems more quickly. Flat Minima presents an algorithm for finding low complexity neural networks with high generalization capability by searching for ""flat"" minima in weight-space, outperforming conventional methods in various experiments. Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability introduces a probabilistic search algorithm based on algorithmic complexity theory, aiming to discover ""algorithmically simple"" neural networks with low Kolmogorov complexity and superior generalization results.
Summarization keywords: [Random Weight Guessing, Flat Minima, Kolmogorov Complexity]
Domains difference: [The domains of the src paper and two hop paper are both in the Neural Networks category.]","
Guessing, Long Time Lags, Recurrent Neural Networks, Alternative Methods, Random Weight Guessing, Signal Processing, Algorithm Performance, Neural Networks","Flat Minima, Neural Computation, Algorithm, Low Complexity, Neural Networks, Generalization Capability, Error Function, MDL-based, Bayesian Argument, Gibbs Algorithm Variant, Generalization Error, Underfitting, Overfitting Error, Gaussian Assumptions, Weight Prior, Input/Output Functions, Net Architecture, Training Set, Second Order Derivatives, Backpropagation, Complexity, Pruning, Feedforward Nets, Recurrent Nets, Stock Market Prediction, Conventional Backprop, Weight Decay, Optimal Brain Surgeon, Optimal Brain Damage, Pseudo Code.","
Neural Networks, Kolmogorov Complexity, Solomonoff's Algorithmic Probability, Generalization Capability, Levin Complexity, Universal Prior, Machine Learning, Algorithmically Simple Solutions, Probabilistic Search Algorithm.","
Relevance: Relatively relevant

Reason: Both papers share common keywords related to neural networks, algorithm performance, and generalization capabilities. However, there are also differences in specific topics covered. Paper 1 focuses on guessing, long time lags, and alternative methods, while Paper 2 delves into flat minima, MDL-based approaches, Bayesian arguments, and various aspects of neural network training. The overlap in general keywords makes them relatively relevant, but the specific areas of focus differ.","
Relevance: Highly relevant
Reason: Both papers share keywords related to neural networks, algorithm performance, and machine learning, indicating a strong thematic connection between them. The overlap in keywords suggests a close relevance in terms of the topics discussed."
108962,Title: BUCKET ELIMINATION: A UNIFYING FRAMEWORK FOR PROBABILISTIC INFERENCE,"Abstract: Probabilistic inference algorithms for belief updating, finding the most probable explanation, the maximum a posteriori hypothesis, and the maximum expected utility are reformulated within the bucket elimination framework. This emphasizes the principles common to many of the algorithms appearing in the probabilistic inference literature and clarifies the relationship of such algorithms to nonserial dynamic programming algorithms. A general method for combining conditioning and bucket elimination is also presented. For all the algorithms, bounds on complexity are given as a function of the problem's structure.",Probabilistic_Methods,35778,Title: Context-Specific Independence in Bayesian Networks,"Abstract: Bayesiannetworks provide a languagefor qualitatively representing the conditional independence properties of a distribution. This allows a natural and compact representation of the distribution, eases knowledge acquisition, and supports effective inference algorithms. It is well-known, however, that there are certain independencies that we cannot capture qualitatively within the Bayesian network structure: independencies that hold only in certain contexts, i.e., given a specific assignment of values to certain variables. In this paper, we propose a formal notion of context-specific independence (CSI), based on regularities in the conditional probability tables (CPTs) at a node. We present a technique, analogous to (and based on) d-separation, for determining when such independence holds in a given network. We then focus on a particular qualitative representation schemetree-structured CPTs for capturing CSI. We suggest ways in which this representation can be used to support effective inference algorithms. In particular, we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms, and an alternative algorithm based on cutset conditioning.",Probabilistic_Methods,108983,Title: Exploiting Causal Independence in Bayesian Network Inference,"Abstract: A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as or, sum or max, on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms.",Probabilistic_Methods,0,1,1,"Summarization Title: [Probabilistic Methods]
Summarization information: Bucket Elimination provides a unifying framework for probabilistic inference algorithms, emphasizing common principles and relationships to nonserial dynamic programming. Context-Specific Independence (CSI) in Bayesian Networks is introduced, allowing representation of conditional independence properties in certain contexts, with implications for effective inference algorithms. Exploiting Causal Independence in Bayesian Network Inference proposes a method for exact inference, achieving a finer-grain factorization of joint probability through a novel formulation of causal independence.
Summarization keywords: [Bucket Elimination, Context-Specific Independence, Causal Independence]
Domains difference: [The domains of the src paper and two hop paper are both in the Probabilistic Methods category.]","Bucket Elimination, Probabilistic Inference, Belief Updating, Most Probable Explanation, Maximum a Posteriori Hypothesis, Maximum Expected Utility, Unifying Framework, Algorithms, Nonserial Dynamic Programming, Conditioning, Complexity Bounds, Probabilistic Methods","Context-Specific Independence, Bayesian Networks, Conditional Independence, Distribution Representation, Knowledge Acquisition, Inference Algorithms, Regularities, Conditional Probability Tables (CPTs), D-Separation, Qualitative Representation, Tree-Structured CPTs, Inference Algorithms, Structural Decomposition, Clustering Algorithms, Cutset Conditioning, Probabilistic Methods.","Bayesian Networks, Causal Independence, Exact Inference, Factorization, Conditional Probabilities, Joint Probability, Algorithm, Associative and Commutative Operator, Medical Diagnosis, Empirical Studies, Efficiency, Network Inference.","
Relevance: Highly relevant

Reason: Both papers focus on probabilistic inference, algorithms, and probabilistic methods in the context of Bayesian networks and belief updating. The keywords indicate a shared emphasis on topics like conditional independence, distribution representation, inference algorithms, and complexity bounds. The commonality in specific keywords and overarching themes makes these papers highly relevant to each other.","
Relevance: Highly relevant
Reason: Both papers focus on probabilistic inference, algorithms, and Bayesian networks, indicating a strong thematic connection. The shared keywords, such as probabilistic methods, conditional probabilities, and algorithms, suggest a close relevance in terms of the topics discussed, especially in the context of probabilistic reasoning and network inference."
235678,Title: Stage Scheduling: A Technique to Reduce the Register Requirements of a Modulo Schedule,"Abstract: Modulo scheduling is an efficient technique for exploiting instruction level parallelism in a variety of loops, resulting in high performance code but increased register requirements. We present a set of low computational complexity stage-scheduling heuristics that reduce the register requirements of a given modulo schedule by shifting operations by multiples of II cycles. Measurements on a benchmark suite of 1289 loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels shows that our best heuristic achieves on average 99% of the decrease in register requirements obtained by an optimal stage scheduler.",Rule_Learning,235683,Title: Minimizing Register Requirements under Resource-Constrained Rate-Optimal Software Pipelining,"Abstract: In this paper we address the following software pipelin-ing problem: given a loop and a machine architecture with a fixed number of processor resources (e.g. function units), how can one construct a software-pipelined schedule which runs on the given architecture at the maximum possible iteration rate (a la rate-optimal) while minimizing the number of registers? The main contributions of this paper are: * First, we demonstrate that such problem can be described by a simple mathematical formulation with precise optimization objectives under periodic linear scheduling framework. The mathematical formulation provides a clear picture which permits one to visualize the overall solution space (for rate-optimal schedules) under different sets of con straints. * Secondly, we show that a precise mathematical formulation and its solution does make a significant performance difference! We evaluated the performance of our method against three other leading contemporary heuristic methods: Huff 's Slack Scheduling [9], Wang, Eisenbeis, Jourdan and Su's FRLC [23], and Gasperoni and Schwiegelshohn's modified list scheduling [6]. Experimental results show that the method described in this paper performed significantly better than these methods.",Rule_Learning,235679,Title: Minimum Register Requirements for a Modulo Schedule,"Abstract: Modulo scheduling is an efficient technique for exploiting instruction level parallelism in a variety of loops, resulting in high performance code but increased register requirements. We present a combined approach that schedules the loop operations for minimum register requirements, given a modulo reservation table. Our method determines optimal register requirements for machines with finite resources and for general dependence graphs. This method demonstrates the potential of lifetime-sensitive modulo scheduling and is useful in evaluating the performance of lifetime-sensitive modulo scheduling heuristics.",Rule_Learning,0,1,1,"Summarization Title: [Rule Learning]
Summarization information: Stage Scheduling introduces low computational complexity heuristics for modulo scheduling, reducing register requirements in loops. Minimizing Register Requirements under Resource-Constrained Rate-Optimal Software Pipelining formulates and solves the software pipelining problem mathematically, achieving superior performance compared to contemporary heuristic methods. Minimum Register Requirements for a Modulo Schedule presents a combined approach for scheduling loop operations with minimum register requirements, particularly focusing on lifetime-sensitive modulo scheduling.
Summarization keywords: [Stage Scheduling, Software Pipelining, Minimum Register Requirements]
Domains difference: [The domains of the src paper and two hop paper are both in the Rule Learning category.]","Stage Scheduling, Modulo Scheduling, Register Requirements, Instruction Level Parallelism, Computational Complexity, Heuristics, Shift Operations, II Cycles, Benchmark Suite, Perfect Club, SPEC-89, Livermore Fortran Kernels, Optimal Scheduler","Minimizing Register Requirements, Resource-Constrained, Rate-Optimal, Software Pipelining, Loop, Machine Architecture, Processor Resources, Mathematical Formulation, Optimization Objectives, Periodic Linear Scheduling, Solution Space, Constraints, Performance Evaluation, Heuristic Methods, Slack Scheduling, FRLC, Modified List Scheduling, Rule Learning.","Modulo Scheduling, Register Requirements, Instruction Level Parallelism, High-Performance Code, Combined Approach, Modulo Reservation Table, Optimal Register Requirements, Finite Resources, Dependence Graphs, Lifetime-Sensitive Modulo Scheduling, Heuristics, Performance Evaluation.","Relevance: Relatively relevant

Reason: Both papers focus on scheduling, register requirements, and optimization objectives in the context of instruction level parallelism and software pipelining. The keywords indicate a shared interest in heuristics, computational complexity, and performance evaluation. However, there are differences in specific aspects such as shift operations in Paper 1 and slack scheduling in Paper 2. The common keywords make these papers relatively relevant, but the variations in specific topics suggest some divergence in focus.","Relevance: Highly relevant
Reason: Both papers share keywords related to modulo scheduling, register requirements, instruction level parallelism, and heuristics, indicating a strong thematic connection in the field of high-performance code optimization. The overlap in keywords suggests a close relevance in terms of the topics discussed, especially in the context of optimizing instruction scheduling and resource utilization in compilers."
628815,Title: Optimal Attitude Control of Satellites by Artificial Neural Networks: a Pilot Study,"Abstract: A pilot study is described on the practical application of artificial neural networks. The limit cycle of the attitude control of a satellite is selected as the test case. One of the sources of the limit cycle is a position dependent error in the observed attitude. A Reinforcement Learning method is selected, which is able to adapt a controller such that a cost function is optimised. An estimate of the cost function is learned by a neural `critic'. In our approach, the estimated cost function is directly represented as a function of the parameters of a linear controller. The critic is implemented as a CMAC network. Results from simulations show that the method is able to find optimal parameters without unstable behaviour. In particular in the case of large discontinuities in the attitude measurements, the method shows a clear improvement compared to the conventional approach: the RMS attitude error decreases approximately 30%.",Reinforcement_Learning,4584,"Title: References elements that can solve difficult learning control problems. on Simulation of Adaptive Behavior, pages","Abstract: Miller, G. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. The Psychological Review, 63(2):81-97. Schmidhuber, J. (1990b). Towards compositional learning with dynamic neural networks. Technical Report FKI-129-90, Technische Universitat Munchen, Institut fu Informatik. Servan-Schreiber, D., Cleermans, A., and McClelland, J. (1988). Encoding sequential structure in simple recurrent networks. Technical Report CMU-CS-88-183, Carnegie Mellon University, Computer Science Department.",Reinforcement_Learning,6213,"Title: Machine Learning Learning to Predict by the Methods of Temporal Differences Keywords: Incremental learning, prediction,","Abstract: This article introduces a class of incremental learning procedures specialized for prediction|that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods; and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",Reinforcement_Learning,0,1,1,"Summarization Title: [Reinforcement Learning]
Summarization information: Optimal Attitude Control of Satellites explores the practical application of artificial neural networks, specifically using reinforcement learning to adapt a controller for attitude control of satellites, showing a 30% improvement in RMS attitude error compared to conventional approaches. Machine Learning Learning to Predict by the Methods of Temporal Differences introduces incremental learning procedures specialized for prediction, demonstrating convergence, optimality, and improved efficiency in real-world prediction problems compared to conventional methods.
Summarization keywords: [Artificial Neural Networks, Reinforcement Learning, Temporal Differences]
Domains difference: [The domains of the src paper and two hop paper are both in the Reinforcement Learning category.]","Optimal Attitude Control, Satellites, Artificial Neural Networks, Pilot Study, Limit Cycle, Reinforcement Learning, Controller Adaptation, Cost Function Optimization, Neural Critic, CMAC Network, Simulations, Attitude Error, Discontinuities, Conventional Approach, RMS Attitude Error Reduction","Learning Control Problems, Simulation, Adaptive Behavior, References, Elements, Difficult Problems, Miller, G., Magical Number Seven, Processing Information, Psychological Review, Schmidhuber, J., Compositional Learning, Dynamic Neural Networks, Servan-Schreiber, D., Cleermans, A., McClelland, J., Encoding Sequential Structure, Simple Recurrent Networks, Reinforcement Learning.","Incremental Learning, Prediction, Temporal Differences, Credit Assignment, Past Experience, Future Behavior, Convergence, Optimality, Supervised Learning, Memory Efficiency, Computation Efficiency, Accurate Predictions, Reinforcement Learning.","Relevance: Relatively relevant

Reason: Both papers involve aspects of learning control, adaptive behavior, and reinforcement learning. Paper 1 focuses on optimal attitude control for satellites using artificial neural networks, while Paper 2 explores learning control problems and compositional learning with dynamic neural networks. While there is a commonality in terms of reinforcement learning, the specific applications (attitude control vs. compositional learning) and keywords indicate a relatively relevant connection between the two papers.","Relevance: Relatively relevant
Reason: While there is some overlap in keywords related to reinforcement learning, both papers focus on different aspects within the broader domain. Paper1 emphasizes optimal attitude control in satellites using artificial neural networks, while paper2 discusses incremental learning and prediction in the context of reinforcement learning. The relevance is not as strong as in highly relevant cases, but there is a connection through the shared topic of reinforcement learning."
145315,"Title: In: Machine Learning, Meta-reasoning and Logics, pp207-232,  Learning from Imperfect Data","Abstract: Systems interacting with real-world data must address the issues raised by the possible presence of errors in the observations it makes. In this paper we first present a framework for discussing imperfect data and the resulting problems it may cause. We distinguish between two categories of errors in data random errors or `noise', and systematic errors and examine their relationship to the task of describing observations in a way which is also useful for helping in future problem-solving and learning tasks. Secondly we proceed to examine some of the techniques currently used in AI research for recognising such errors.",Rule_Learning,649944,Title: Knowledge Integration and Learning,"Abstract: LIACC - Technical Report 91-1 Abstract. In this paper we address the problem of acquiring knowledge by integration . Our aim is to construct an integrated knowledge base from several separate sources. The objective of integration is to construct one system that exploits all the knowledge that is available and has good performance. The aim of this paper is to discuss the methodology of knowledge integration and present some concrete results. In our experiments the performance of the integrated theory exceeded the performance of the individual theories by quite a significant amount. Also, the performance did not fluctuate much when the experiments were repeated. These results indicate knowledge integration can complement other existing ML methods.",Rule_Learning,294126,Title: Knowledge Acquisition via Knowledge Integration,"Abstract: In this paper we are concerned with the problem of acquiring knowledge by integration. Our aim is to construct an integrated knowledge base from several separate sources. The need to merge knowledge bases can arise, for example, when knowledge bases are acquired independently from interactions with several domain experts. As opinions of different domain experts may differ, the knowledge bases constructed in this way will normally differ too. A similar problem can also arise whenever separate knowledge bases are generated by learning algorithms. The objective of integration is to construct one system that exploits all the knowledge that is available and has a good performance. The aim of this paper is to discuss the methodology of knowledge integration, describe the implemented system (INTEG.3), and present some concrete results which demonstrate the advantages of this method.",Rule_Learning,1,0,1,,"
Machine Learning, Meta-reasoning, Logics, Imperfect Data, Real-world Data, Errors, Observations, Random Errors, Systematic Errors, Framework, Problem-solving, Learning Tasks, AI Research, Error Recognition, Rule Learning","
Knowledge Integration, Learning, LIACC Technical Report, Acquiring Knowledge, Integration Problem, Integrated Knowledge Base, Separate Sources, System Construction, Performance Improvement, Methodology, Concrete Results, Experimentation, Integrated Theory, Significant Performance Enhancement, Rule Learning.","Knowledge Acquisition, Knowledge Integration, Integrated Knowledge Base, Merging Knowledge Bases, Domain Experts, Learning Algorithms, System Construction, Exploiting Knowledge, Performance, Methodology, INTEG.3, Concrete Results, Rule Learning.","Relevance: Highly relevant

Reason: Both papers share common keywords related to machine learning, rule learning, and the integration of knowledge. Paper 1 focuses on meta-reasoning, logics, and error recognition in the context of imperfect data, while Paper 2 emphasizes knowledge integration, system construction, and performance improvement. The overlap in keywords indicates a strong relevance between the two papers, as they address similar themes in the context of learning tasks and AI research.","Relevance: Relatively relevant
Reason: Both papers share keywords related to rule learning, machine learning, and knowledge acquisition, indicating a connection in the broader context of learning from data and integrating knowledge. However, the focus and emphasis within these topics might differ, making the relevance relatively strong but not as direct as in highly relevant cases."
112378,Title: Language-Independent Data-Oriented Grapheme-to-Phoneme Conversion,"Abstract: We describe an approach to grapheme-to-phoneme conversion which is both language-independent and data-oriented. Given a set of examples (spelling words with their associated phonetic representation) in a language, a grapheme-to-phoneme conversion system is automatically produced for that language which takes as its input the spelling of words, and produces as its output the phonetic transcription according to the rules implicit in the training data. We describe the design of the system, and compare its performance to knowledge-based and alternative data-oriented approaches.",Case_Based,141868,Title: ABSTRACTION CONSIDERED HARMFUL: LAZY LEARNING OF LANGUAGE PROCESSING,"Abstract: When m = 0 (no delays), we set A 0 (ffi) = f(j; k) ; j 6= kg, such that P m (*jffi) depends only on *. The estimated probabilities above become quite noisy when the number of elements in set A m and B m are small. For this reason, we estimate the standard deviation of P m (*jffi). Notice that this estimate is the empirical average of a binomial variable (either a given couple satisfied the conditions on ffi and *, or it does not). The standard deviation is then estimated easily by: Generally speaking, P m (*jffi) increases with * (laxer output test), and when ffi approaches 0 (stricter input condition). Let us now define by P m (*) the maximum over ffi of P m (*jffi): P m (*) = max ffi&gt;0 P m (*jffi). The dependability index is defined as: P 0 (*) represents how much data passes the continuity test when no input information is available. This dependability index measures how much of the remaining continuity information is associated with involving input i m . This index is then averaged over * with respect to the probability (1 P 0 (*)): m (*) (1 P 0 (*)) d* (4.8) It is clear that m (*), and therefore its average, should be positive quantities. Furthermore, if the system is deterministic, the dependability is zero after a certain number of inputs, so the sum of averages saturates. If the system is also noise-free, they sum up to 1. For any m greater than the embedding dimension: refers to results obtained using this method. 4.6 Statistical variable selection Statistical variable selection (or feature selection) encompasses a number of techniques aimed at choosing a relevant subset of input variables in a regression or a classification problem. As in the rest of this document, we will limit ourselves to considerations related to the regression problem, even though most methods discussed below apply to classification as well. Variable selection can be seen as a part of the data analysis problem: the selection (or discard) of a variable tells us about the relevance of the associated measurement to the modelled system. In a general setting, this is a purely combinatorial problem: given V possible variables, there is 2 V possible subsets (including the empty set and the full set) of these variables. Given a performance measure, such as prediction error, the only optimal scheme is to test all these subset and choose the one that gives the best performance. It is easy to see that such an extensive scheme is only viable when the number of variables is rather low. Identifying 2 V models when we have more than a few variables requires too much computation. A number of techniques have been devised to overcome this combinatorial limit. Some of them use an iterative, locally optimal technique to construct an estimate of the relevant subset in a number of steps. We will refer to them as stepwise selection methods, not to be con fused with stepwise regression, a subset of these methods that we will address below. In forward selection, we start with an empty set of variables. At each step, we select a candidate variable using a selection criteria, check whether this variable should be added to the set, and iterate until a given stop condition is reached. On the contrary, backward elimination methods start with the full set of all input variables. At each step, the least significant variable is selected according to a selection criteria. If this variable is irrelevant, it is removed and the process is iterated until a stop condition is reached. It is easy to devise examples where the inclusion of a variable causes a previously included variable to become irrelevant. It thus seems appropriate to consider running a backward elimination each time a new variable is added by forward selection. This combination of both ap proaches is known as stepwise regression in the linear regression con",Case_Based,145215,Title: Memory-Based Lexical Acquisition and Processing,"Abstract: Current approaches to computational lexicology in language technology are knowledge-based (competence-oriented) and try to abstract away from specific formalisms, domains, and applications. This results in severe complexity, acquisition and reusability bottlenecks. As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks. The consequences of the approach for computational lexicology are discussed, and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology, morphology and syntax is described.",Case_Based,1,0,1,"Summarization Title: [Case-Based]
Summarization information: Language-Independent Data-Oriented Grapheme-to-Phoneme Conversion presents a language-independent and data-oriented approach for grapheme-to-phoneme conversion, comparing its performance to knowledge-based and alternative data-oriented methods. ABSTRACTION CONSIDERED HARMFUL: LAZY LEARNING OF LANGUAGE PROCESSING discusses lazy learning in language processing, exploring statistical variable selection and highlighting issues related to backward elimination and forward selection. Memory-Based Lexical Acquisition and Processing proposes a performance-oriented approach to computational lexicology based on automatic memory-based learning, addressing complexity and reusability bottlenecks in knowledge-based approaches.
Summarization keywords: [Grapheme-to-Phoneme Conversion, Lazy Learning, Memory-Based Lexical Acquisition]
Domains difference: [The domains of the src paper and two hop paper are both in the Case-Based category.]","Language-Independent, Data-Oriented, Grapheme-to-Phoneme Conversion, Examples, Phonetic Representation, Conversion System, Spelling, Phonetic Transcription, Training Data, System Design, Performance Comparison, Knowledge-Based, Case-Based Approach","Abstraction, Lazy Learning, Language Processing, Delay Parameter, Probability Estimation, Noisy Data, Dependability Index, Continuity Test, Statistical Variable Selection, Feature Selection, Regression Problem, Stepwise Selection Methods, Forward Selection, Backward Elimination, Stepwise Regression, Combinatorial Problem, Performance Measure.","Memory-Based Learning, Lexical Acquisition, Natural Language Processing, Computational Lexicology, Linguistic Tasks, Lexical Disambiguation, Phonology, Morphology, Syntax, Performance-Oriented Approach, Automatic Learning, Case-Based.","Relevance: Not relevant

Reason: The keywords in Paper 1 focus on grapheme-to-phoneme conversion, language-independent systems, and phonetic representation, while Paper 2 deals with lazy learning, language processing, statistical variable selection, and regression problems. The topics and themes addressed in each paper are quite distinct, suggesting that there is not a significant overlap or relevance between them.","Relevance: Highly relevant
Reason: Both papers share keywords related to language-independent systems, case-based approaches, and natural language processing. The overlap in topics, such as lexical acquisition, phonetic representation, and case-based methods, indicates a strong thematic connection. The relevance is high as the papers seem to address similar challenges and approaches within the field of language processing and conversion systems."
1129629,Title: of a simulator for evolving morphology are: Universal the simulator should cover an infinite gen,"Abstract: Funes, P. and Pollack, J. (1997) Computer Evolution of Buildable Objects. Fourth European Conference on Artificial Life. P. Husbands and I. Harvey, eds., MIT Press. pp 358-367. knowledge into the program, which would result in familiar structures, we provided the algorithm with a model of the physical reality and a purely utilitarian fitness function, thus supplying measures of feasibility and functionality. In this way the evolutionary process runs in an environment that has not been unnecessarily constrained. We added, however, a requirement of computability to reject overly complex structures when they took too long for our simulations to evaluate. The results are encouraging. The evolved structures had a surprisingly alien look: they are not based in common knowledge on how to build with brick toys; instead, the computer found ways of its own through the evolutionary search process. We were able to assemble the final designs manually and confirm that they accomplish the objectives introduced with our fitness functions. After some background on related problems, we describe our physical simulation model for two-dimensional Lego structures, and the representation for encoding them and applying evolution. We demonstrate the feasibility of our work with photos of actual objects which were the result of particular optimizations. Finally, we discuss future work and draw some conclusions. In order to evolve both the morphology and behavior of autonomous mechanical devices which can be manufactured, one must have a simulator which operates under several constraints, and a resultant controller which is adaptive enough to cover the gap between simulated and real world. eral space of mechanisms. Conservative - because simulation is never perfect, it should preserve a margin of safety. Efficient - it should be quicker to test in simulation than through physical production and test. Buildable - results should be convertible from a simula tion to a real object Computer Evolution of Buildable Objects Abstract The idea of co-evolution of bodies and brains is becoming popular, but little work has been done in evolution of physical structure because of the lack of a general framework for doing it. Evolution of creatures in simulation has been constrained by the reality gap which implies that resultant objects are usually not buildable. The work we present takes a step in the problem of body evolution by applying evolutionary techniques to the design of structures assembled out of parts. Evolution takes place in a simulator we designed, which computes forces and stresses and predicts failure for 2-dimensional Lego structures. The final printout of our program is a schematic assembly, which can then be built physically. We demonstrate its functionality in several different evolved entities.",Genetic_Algorithms,568857,Title: Evolving Self-Supporting Structures Page 18 References Evolution of Visual Control Systems for Robots. To appear,"Abstract: In this paper we are concerned with the problem of acquiring knowledge by integration. Our aim is to construct an integrated knowledge base from several separate sources. The need to merge knowledge bases can arise, for example, when knowledge bases are acquired independently from interactions with several domain experts. As opinions of different domain experts may differ, the knowledge bases constructed in this way will normally differ too. A similar problem can also arise whenever separate knowledge bases are generated by learning algorithms. The objective of integration is to construct one system that exploits all the knowledge that is available and has a good performance. The aim of this paper is to discuss the methodology of knowledge integration, describe the implemented system (INTEG.3), and present some concrete results which demonstrate the advantages of this method.",Genetic_Algorithms,210871,Title: Coevolving High-Level Representations,"Abstract: Rules extracted from trained feedforward networks can be used for explanation, validation, and cross-referencing of network output decisions. This paper introduces a rule evaluation and ordering mechanism that orders rules extracted from feedforward networks based on three performance measures. Detailed experiments using three rule extraction techniques as applied to the Wisconsin breast cancer database, illustrate the power of the proposed methods. Moreover, a method of integrating the output decisions of both the extracted rule-based system and the corresponding trained network is proposed. The integrated system provides further improvements.",Genetic_Algorithms,0,0,1,"Summarization Title: [Genetic Algorithms]
Summarization information: The src paper presents the Computer Evolution of Buildable Objects, focusing on evolving physical structures using a simulator designed for two-dimensional Lego structures. Evolving Self-Supporting Structures extends this idea, exploring the evolution of structures assembled from parts, while Coevolving High-Level Representations introduces a rule evaluation mechanism for extracting rules from trained feedforward networks and proposes integrating rule-based systems with trained networks.
Summarization keywords: [Evolution, Buildable Objects, Rule Extraction]
Domains difference: [The domains of the src paper and two hop paper are both in the Genetic Algorithms category.]","Simulator, Evolving Morphology, Universal, Knowledge Incorporation, Fitness Function, Feasibility, Functionality, Evolutionary Process, Computability Requirement, Complex Structures, Alien-Looking Designs, Two-Dimensional Lego Structures, Physical Simulation Model, Evolutionary Search, Autonomous Mechanical Devices, Co-evolution, Bodies and Brains, Reality Gap, Evolutionary Techniques, Design of Structures, Genetic Algorithms","
Knowledge Integration, Integrated Knowledge Base, Merging Knowledge Bases, Domain Experts, Learning Algorithms, Evolving Structures, Self-Supporting Structures, Visual Control Systems, Robots, Genetic Algorithms, Methodology, Concrete Results.","Coevolution, High-Level Representations, Rule Extraction, Feedforward Networks, Performance Measures, Wisconsin Breast Cancer Database, Rule Evaluation, Rule Ordering, Explanation, Validation, Cross-Referencing, Integrated System.","Relevance: Relatively relevant

Reason: Both papers involve evolving structures and genetic algorithms. Paper 1 focuses on evolving morphology, simulator, and autonomous mechanical devices, while Paper 2 addresses knowledge integration, learning algorithms, and evolving structures in the context of self-supporting structures and robots. While there are commonalities in terms of evolving structures and genetic algorithms, the specific applications (morphology evolution vs. knowledge integration in robotics) suggest a relatively relevant connection between the two papers.","Relevance: Relatively relevant
Reason: While there is some overlap in keywords related to coevolution and evolutionary techniques, the focus of the two papers appears to be different. Paper1 emphasizes evolving morphology and design structures in a physical simulation model, while paper2 focuses on coevolution in the context of rule extraction and high-level representations with applications in the Wisconsin Breast Cancer Database. The relevance is not as strong as in highly relevant cases, but there is a connection through the shared theme of coevolution and evolutionary techniques."
645897,Title: A note on convergence rates of Gibbs sampling for nonparametric mixtures,"Abstract: We consider a mixture model where the mixing distribution is random and is given a Dirichlet process prior. We describe the general structure of two Gibbs sampling algorithms that are useful for approximating Bayesian inferences in this problem. When the kernel f(x j ) of the mixture is bounded, we show that the Markov chains resulting from the Gibbs sampling are uniformly ergodic, and we provide an explicit rate bound. Unfortunately, the bound is not sharp in general; improving sensibly the bound seems however quite difficult.",Probabilistic_Methods,643221,Title: A simulation approach to convergence rates for Markov chain Monte Carlo algorithms,"Abstract: Markov chain Monte Carlo (MCMC) methods, including the Gibbs sampler and the Metropolis-Hastings algorithm, are very commonly used in Bayesian statistics for sampling from complicated, high-dimensional posterior distributions. A continuing source of uncertainty is how long such a sampler must be run in order to converge approximately to its target stationary distribution. Rosenthal (1995b) presents a method to compute rigorous theoretical upper bounds on the number of iterations required to achieve a specified degree of convergence in total variation distance by verifying drift and minorization conditions. We propose the use of auxiliary simulations to estimate the numerical values needed in Rosenthal's theorem. Our simulation method makes it possible to compute quantitative convergence bounds for models for which the requisite analytical computations would be prohibitively difficult or impossible. On the other hand, although our method appears to perform well in our example problems, it can not provide the guarantees offered by analytical proof. Acknowledgements. We thank Brad Carlin for assistance and encouragement.",Probabilistic_Methods,126927,Title: Convergence properties of perturbed Markov chains,"Abstract: Acknowledgements. We thank Neal Madras, Radford Neal, Peter Rosenthal, and Richard Tweedie for helpful conversations. This work was partially supported by EPSRC of the U.K., and by NSERC of Canada.",Probabilistic_Methods,0,1,1,"
Summarization Title: [Probabilistic Methods]
Summarization information: The src paper discusses convergence rates of Gibbs sampling for nonparametric mixtures with a Dirichlet process prior, providing a uniform ergodicity proof. The one hop paper introduces a simulation approach to estimate convergence rates for Markov chain Monte Carlo algorithms, offering an alternative to analytical proofs. The two hop paper explores the convergence properties of perturbed Markov chains, with acknowledgments for support from EPSRC and NSERC.
Summarization keywords: [Gibbs sampling, Convergence rates, Markov chain Monte Carlo]
Domains difference: [The domains of the src paper and two hop paper are both in the Probabilistic Methods category.]","Convergence Rates, Gibbs Sampling, Nonparametric Mixtures, Mixture Model, Mixing Distribution, Dirichlet Process Prior, Bayesian Inferences, Kernel, Bounded Kernel, Markov Chains, Uniformly Ergodic, Rate Bound, Sharpness, Probabilistic Methods","
Markov Chain Monte Carlo, Convergence Rates, Gibbs Sampler, Metropolis-Hastings Algorithm, Bayesian Statistics, Posterior Distributions, Total Variation Distance, Rosenthal's Theorem, Drift Conditions, Minorization Conditions, Simulation Approach, Numerical Estimation, Analytical Proof, Probabilistic Methods.","Convergence Properties, Perturbed Markov Chains, Acknowledgements, Neal Madras, Radford Neal, Peter Rosenthal, Richard Tweedie, EPSRC, U.K., NSERC, Canada.","
Relevance: Highly relevant

Reason: Both papers focus on convergence rates, Gibbs sampling, Markov chains, and probabilistic methods in the context of Bayesian inferences and statistical modeling. The keywords indicate a strong overlap in topics, including mixture models, Dirichlet process prior, and simulation approaches. The similarity in the specific keywords and themes makes these papers highly relevant to each other.","
Relevance: Not relevant
Reason: The keywords in paper1 focus on convergence rates, Gibbs sampling, and probabilistic methods in the context of nonparametric mixtures and Bayesian inferences. On the other hand, the keywords in paper2 mention convergence properties, perturbed Markov chains, and acknowledgments related to specific individuals and funding sources. The topics seem unrelated, indicating a lack of relevance between the two papers."
69418,Title: Constructive Algorithms for Hierarchical Mixtures of Experts,"Abstract: We present two additions to the hierarchical mixture of experts (HME) architecture. We view the HME as a tree structured classifier. Firstly, by applying a likelihood splitting criteria to each expert in the HME we ""grow"" the tree adaptively during training. Secondly, by considering only the most probable path through the tree we may ""prune"" branches away, either temporarily, or permanently if they become redundant. We demonstrate results for the growing and pruning algorithms which show significant speed ups and more efficient use of parameters over the conventional algorithms in discriminating between two interlocking spirals and classifying 8-bit parity patterns.",Neural_Networks,4330,Title: Hierarchical Mixtures of Experts and the EM Algorithm,"Abstract: We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. This report describes research done at the Dept. of Brain and Cognitive Sciences, the Center for Biological and Computational Learning, and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for CBCL is provided in part by a grant from the NSF (ASC-9217041). Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Dept. of Defense. The authors were supported by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, by by grant IRI-9013991 from the National Science Foundation, by grant N00014-90-J-1942 from the Office of Naval Research, and by NSF grant ECS-9216531 to support an Initiative in Intelligent Control at MIT. Michael I. Jordan is a NSF Presidential Young Investigator.",Probabilistic_Methods,32688,Title: CLASSIFICATION USING HIERARCHICAL MIXTURES OF EXPERTS,"Abstract: There has recently been widespread interest in the use of multiple models for classification and regression in the statistics and neural networks communities. The Hierarchical Mixture of Experts (HME) [1] has been successful in a number of regression problems, yielding significantly faster training through the use of the Expectation Maximisation algorithm. In this paper we extend the HME to classification and results are reported for three common classification benchmark tests: Exclusive-Or, N-input Parity and Two Spirals.",Neural_Networks,0,1,1,"Summarization Title: [Neural Networks]
Summarization information: The src paper introduces constructive algorithms for hierarchical mixtures of experts (HME), incorporating adaptive tree growth and branch pruning for improved efficiency. The one hop paper focuses on the HME architecture and presents an Expectation-Maximization (EM) algorithm for supervised learning, with applications in the robot dynamics domain. The two hop paper extends HME to classification, demonstrating success in common benchmark tests.
Summarization keywords: [Hierarchical Mixtures of Experts, Constructive Algorithms, Expectation-Maximization, Classification]
Domains difference: [The domains of the src paper and two hop paper are both in the Neural Networks category.]","Constructive Algorithms, Hierarchical Mixtures of Experts, Tree Structured Classifier, Likelihood Splitting Criteria, Adaptive Tree Growth, Branch Pruning, Redundancy Removal, Growing Algorithms, Pruning Algorithms, Speed Ups, Efficient Parameter Use, Conventional Algorithms, Interlocking Spirals, 8-bit Parity Patterns, Neural Networks","Hierarchical Mixtures of Experts, EM Algorithm, Tree-Structured Architecture, Supervised Learning, Statistical Model, Generalized Linear Models (GLIM's), Maximum Likelihood, Expectation-Maximization, On-line Learning, Robot Dynamics, Comparative Simulation, Dept. of Brain and Cognitive Sciences, Center for Biological and Computational Learning, Artificial Intelligence Laboratory, Massachusetts Institute of Technology, NSF Support, Advanced Research Projects Agency, McDonnell-Pew Foundation, ATR Human Information Processing Research Laboratories, Siemens Corporation, National Science Foundation, Office of Naval Research, Intelligent Control Initiative, NSF Presidential Young Investigator, Probabilistic Methods.","Classification, Hierarchical Mixtures of Experts, Multiple Models, Regression, Statistics, Neural Networks, Expectation Maximisation algorithm, Exclusive-Or, N-input Parity, Two Spirals.","Relevance: Highly relevant

Reason: Both papers share common keywords such as hierarchical mixtures of experts, tree-structured classifier, EM algorithm, and probabilistic methods. Paper 1 focuses on constructive algorithms, tree growth, and pruning in the context of neural networks, while Paper 2 discusses a tree-structured architecture, supervised learning, and applications in robot dynamics. The overlap in keywords and themes indicates a high relevance between the two papers.","
Relevance: Highly relevant
Reason: Both papers share keywords such as hierarchical mixtures of experts, neural networks, and classification. The overlap in topics, including tree-structured classifiers and adaptive tree growth in paper1, and hierarchical mixtures of experts and regression in paper2, indicates a strong thematic connection. The relevance is high as the papers seem to address similar concepts and techniques within the broader context of constructive algorithms and hierarchical models in machine learning."
134199,Title: Using Markov Chains to Analyze GAFOs,"Abstract: Our theoretical understanding of the properties of genetic algorithms (GAs) being used for function optimization (GAFOs) is not as strong as we would like. Traditional schema analysis provides some first order insights, but doesn't capture the non-linear dynamics of the GA search process very well. Markov chain theory has been used primarily for steady state analysis of GAs. In this paper we explore the use of transient Markov chain analysis to model and understand the behavior of finite population GAFOs observed while in transition to steady states. This approach appears to provide new insights into the circumstances under which GAFOs will (will not) perform well. Some preliminary results are presented and an initial evaluation of the merits of this approach is provided.",Genetic_Algorithms,447224,Title: A COMPRESSION ALGORITHM FOR PROBABILITY TRANSITION MATRICES,"Abstract: This paper describes a compression algorithm for probability transition matrices. The compressed matrix is itself a probability transition matrix. In general the compression is not error-free, but the error appears to be small even for high levels of compression.",Genetic_Algorithms,164885,Title: Analyzing GAs Using Markov Models with Semantically Ordered and Lumped States,"Abstract: At the previous FOGA workshop, we presented some initial results on using Markov models to analyze the transient behavior of genetic algorithms (GAs) being used as function optimizers (GAFOs). In that paper, the states of the Markov model were ordered via a simple and mathematically convenient lexicographic ordering used initially by Nix and Vose. In this paper, we explore alternative orderings of states based on interesting semantic properties such as average fitness, degree of homogeneity, average attractive force, etc. We also explore lumping techniques for reducing the size of the state space. Analysis of these reordered and lumped Markov models provides new insights into the transient behavior of GAs in general and GAFOs in particular.",Genetic_Algorithms,1,1,1,"Summarization Title: [Genetic Algorithms]
Summarization information: The src paper explores the application of transient Markov chain analysis to model and understand the behavior of finite population genetic algorithms (GAs) during the transition to steady states. The one hop paper introduces a compression algorithm for probability transition matrices, preserving matrix properties even with high levels of compression. The two hop paper extends the analysis of GAs using Markov models with semantically ordered and lumped states, providing insights into transient behavior based on properties like average fitness and homogeneity.
Summarization keywords: [Markov Chains, GAFOs, Compression Algorithm, Probability Transition Matrices]
Domains difference: [The domains of the src paper and two hop paper are both in the Genetic Algorithms category.]","Markov Chains, Genetic Algorithms, Function Optimization, GAFOs, Theoretical Understanding, Schema Analysis, Non-linear Dynamics, GA Search Process, Steady State Analysis, Transient Markov Chain Analysis, Finite Population, Transition to Steady States, Performance Evaluation, Insights, Circumstances, Preliminary Results","Compression Algorithm, Probability Transition Matrices, Error-free Compression, Genetic Algorithms.","Genetic Algorithms, Markov Models, Semantically Ordered States, Lumped States, Transient Behavior, Function Optimizers, FOGA Workshop, Average Fitness, Degree of Homogeneity, Average Attractive Force.","Relevance: Relatively relevant

Reason: Both papers involve genetic algorithms, but Paper 1 focuses on Markov chains, function optimization, and theoretical understanding, while Paper 2 emphasizes compression algorithms and probability transition matrices. While there is a commonality in the use of genetic algorithms, the specific topics and themes covered in each paper suggest a relatively relevant connection rather than high relevance.","Relevance: Highly relevant
Reason: Both papers share keywords related to genetic algorithms, Markov chains, and function optimization. The overlap in topics, such as schema analysis, GA search process, and transient behavior, indicates a strong thematic connection. The relevance is high as the papers appear to address similar concepts and techniques within the broader context of genetic algorithms and their application to function optimization, providing insights and analysis of the GA search process."
411005,Title: GUESSING CAN OUTPERFORM MANY LONG TIME LAG ALGORITHMS,"Abstract: Numerous recent papers focus on standard recurrent nets' problems with long time lags between relevant signals. Some propose rather sophisticated, alternative methods. We show: many problems used to test previous methods can be solved more quickly by random weight guessing.",Neural_Networks,131315,Title: Hierarchical Recurrent Neural Networks for Long-Term Dependencies,"Abstract: We have already shown that extracting long-term dependencies from sequential data is difficult, both for deterministic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.",Neural_Networks,411005,Title: GUESSING CAN OUTPERFORM MANY LONG TIME LAG ALGORITHMS,"Abstract: Numerous recent papers focus on standard recurrent nets' problems with long time lags between relevant signals. Some propose rather sophisticated, alternative methods. We show: many problems used to test previous methods can be solved more quickly by random weight guessing.",Neural_Networks,0,1,0,,,,,,
108962,Title: BUCKET ELIMINATION: A UNIFYING FRAMEWORK FOR PROBABILISTIC INFERENCE,"Abstract: Probabilistic inference algorithms for belief updating, finding the most probable explanation, the maximum a posteriori hypothesis, and the maximum expected utility are reformulated within the bucket elimination framework. This emphasizes the principles common to many of the algorithms appearing in the probabilistic inference literature and clarifies the relationship of such algorithms to nonserial dynamic programming algorithms. A general method for combining conditioning and bucket elimination is also presented. For all the algorithms, bounds on complexity are given as a function of the problem's structure.",Probabilistic_Methods,108963,Title: Topological Parameters for time-space tradeoff,Abstract: In this paper we propose a family of algorithms combining tree-clustering with conditioning that trade space for time. Such algorithms are useful for reasoning in probabilistic and deterministic networks as well as for accomplishing optimization tasks. By analyzing the problem structure it will be possible to select from a spectrum the algorithm that best meets a given time-space specifica tion.,Probabilistic_Methods,1095507,Title: Generalized Queries on Probabilistic Context-Free Grammars  on Pattern Analysis and Machine Intelligence,"Abstract: In this paper, we describe methods for efficiently computing better solutions to control problems in continuous state spaces. We provide algorithms that exploit online search to boost the power of very approximate value functions discovered by traditional reinforcement learning techniques. We examine local searches, where the agent performs a finite-depth lookahead search, and global searches, where the agent performs a search for a trajectory all the way from the current state to a goal state. The key to the success of the global methods lies in using aggressive state-space search techniques such as uniform-cost search and A fl , tamed into a tractable form by exploiting neighborhood relations and trajectory constraints that arise from continuous-space dynamic control.",Probabilistic_Methods,1,1,0,,"Bucket Elimination, Probabilistic Inference, Belief Updating, Most Probable Explanation, Maximum a Posteriori Hypothesis, Maximum Expected Utility, Unifying Framework, Algorithms, Nonserial Dynamic Programming, Conditioning, Complexity Bounds, Probabilistic Methods","Topological Parameters, Time-Space Tradeoff, Tree-Clustering, Conditioning, Probabilistic Networks, Deterministic Networks, Optimization Tasks.","Probabilistic Context-Free Grammars, Generalized Queries, Pattern Analysis, Machine Intelligence, Control Problems, Continuous State Spaces, Online Search, Approximate Value Functions, Reinforcement Learning, Local Searches, Global Searches, State-Space Search, Uniform-Cost Search, A*, Trajectory Constraints.","
Relevance: Highly relevant

Reason: Both papers focus on probabilistic inference, belief updating, and conditioning. Paper 1 discusses bucket elimination, nonserial dynamic programming, and probabilistic methods, while Paper 2 explores topological parameters, tree-clustering, and optimization tasks in the context of probabilistic and deterministic networks. The commonality in specific keywords and the overarching theme of probabilistic inference makes these papers highly relevant to each other.","Relevance: Relatively relevant
Reason: While there is some overlap in the broader context of probabilistic methods and algorithms, the focus of the two papers appears to be different. Paper1 emphasizes probabilistic inference and belief updating, while paper2 discusses probabilistic context-free grammars, pattern analysis, and reinforcement learning in the context of machine intelligence and control problems. The relevance is not as strong as in highly relevant cases, but there is a connection through the shared theme of probabilistic methods and algorithms."
235678,Title: Stage Scheduling: A Technique to Reduce the Register Requirements of a Modulo Schedule,"Abstract: Modulo scheduling is an efficient technique for exploiting instruction level parallelism in a variety of loops, resulting in high performance code but increased register requirements. We present a set of low computational complexity stage-scheduling heuristics that reduce the register requirements of a given modulo schedule by shifting operations by multiples of II cycles. Measurements on a benchmark suite of 1289 loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels shows that our best heuristic achieves on average 99% of the decrease in register requirements obtained by an optimal stage scheduler.",Rule_Learning,235679,Title: Minimum Register Requirements for a Modulo Schedule,"Abstract: Modulo scheduling is an efficient technique for exploiting instruction level parallelism in a variety of loops, resulting in high performance code but increased register requirements. We present a combined approach that schedules the loop operations for minimum register requirements, given a modulo reservation table. Our method determines optimal register requirements for machines with finite resources and for general dependence graphs. This method demonstrates the potential of lifetime-sensitive modulo scheduling and is useful in evaluating the performance of lifetime-sensitive modulo scheduling heuristics.",Rule_Learning,689439,Title: A Reduced Multipipeline Machine Description that Preserves Scheduling Constraints,"Abstract: High performance compilers increasingly rely on accurate modeling of the machine resources to efficiently exploit the instruction level parallelism of an application. In this paper, we propose a reduced machine description that results in faster detection of resource contentions while preserving the scheduling constraints present in the original machine description. The proposed approach reduces a machine description in an automated, error-free, and efficient fashion. Moreover, it fully supports schedulers that backtrack and process operations in arbitrary order. Reduced descriptions for the DEC Alpha 21064, MIPS R3000/R3010, and Cydra 5 result in 4 to 7 times faster detection of resource contentions and require 22 to 90% of the memory storage used by the original machine descriptions.",Rule_Learning,1,1,0,,"Stage Scheduling, Modulo Scheduling, Register Requirements, Instruction Level Parallelism, Computational Complexity, Heuristics, Shift Operations, II Cycles, Benchmark Suite, Perfect Club, SPEC-89, Livermore Fortran Kernels, Optimal Scheduler, Rule Learning","
Minimum Register Requirements, Modulo Schedule, Instruction Level Parallelism, High-Performance Code, Register Allocation, Dependence Graphs, Lifetime-Sensitive Modulo Scheduling, Heuristics.","Multipipeline Machine, Machine Description, Scheduling Constraints, High-Performance Compilers, Instruction Level Parallelism, Resource Contentions, Reduced Description, Automated Reduction, Error-Free, Efficient, Backtracking Schedulers, DEC Alpha 21064, MIPS R3000/R3010, Cydra 5, Memory Storage.","Relevance: Highly relevant

Reason: Both papers share common keywords related to stage scheduling, modulo scheduling, register requirements, and instruction level parallelism. Additionally, keywords such as computational complexity, heuristics, and optimal scheduler appear in both papers. The overlap in specific keywords and the shared focus on scheduling and instruction level parallelism makes these papers highly relevant to each other.","Relevance: Highly relevant
Reason: Both papers share keywords related to stage scheduling, modulo scheduling, register requirements, instruction level parallelism, and optimal scheduling. The overlap in topics, including heuristics and computational complexity, indicates a strong thematic connection. The relevance is high as the papers appear to address similar concepts and techniques within the broader context of high-performance compilers and scheduling optimization for instruction-level parallelism."
628815,Title: Optimal Attitude Control of Satellites by Artificial Neural Networks: a Pilot Study,"Abstract: A pilot study is described on the practical application of artificial neural networks. The limit cycle of the attitude control of a satellite is selected as the test case. One of the sources of the limit cycle is a position dependent error in the observed attitude. A Reinforcement Learning method is selected, which is able to adapt a controller such that a cost function is optimised. An estimate of the cost function is learned by a neural `critic'. In our approach, the estimated cost function is directly represented as a function of the parameters of a linear controller. The critic is implemented as a CMAC network. Results from simulations show that the method is able to find optimal parameters without unstable behaviour. In particular in the case of large discontinuities in the attitude measurements, the method shows a clear improvement compared to the conventional approach: the RMS attitude error decreases approximately 30%.",Reinforcement_Learning,4584,"Title: References elements that can solve difficult learning control problems. on Simulation of Adaptive Behavior, pages","Abstract: Miller, G. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. The Psychological Review, 63(2):81-97. Schmidhuber, J. (1990b). Towards compositional learning with dynamic neural networks. Technical Report FKI-129-90, Technische Universitat Munchen, Institut fu Informatik. Servan-Schreiber, D., Cleermans, A., and McClelland, J. (1988). Encoding sequential structure in simple recurrent networks. Technical Report CMU-CS-88-183, Carnegie Mellon University, Computer Science Department.",Reinforcement_Learning,6213,"Title: Machine Learning Learning to Predict by the Methods of Temporal Differences Keywords: Incremental learning, prediction,","Abstract: This article introduces a class of incremental learning procedures specialized for prediction|that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods; and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",Reinforcement_Learning,0,1,0,,"Optimal Attitude Control, Satellites, Artificial Neural Networks, Pilot Study, Limit Cycle, Reinforcement Learning, Controller Adaptation, Cost Function Optimization, Neural Critic, CMAC Network, Simulations, Attitude Error, Discontinuities, Conventional Approach, RMS Attitude Error Reduction","Learning Control Problems, Simulation of Adaptive Behavior, References, Sequential Structure, Dynamic Neural Networks, Compositional Learning, Processing Information, Psychological Review, Encoding, Simple Recurrent Networks, Reinforcement Learning.","Machine Learning, Temporal Differences, Incremental Learning, Prediction, Credit Assignment, Conventional Prediction-Learning Methods, Temporally Successive Predictions, Convergence, Optimality, Supervised Learning, Real-world Prediction Problems, Memory Efficiency, Peak Computation Efficiency, Accurate Predictions, Reinforcement Learning.","
Relevance: Relatively relevant

Reason: Both papers involve aspects of attitude control, artificial neural networks, and reinforcement learning. Paper 1 focuses on optimal attitude control for satellites using neural networks, while Paper 2 explores learning control problems, compositional learning, and reinforcement learning with dynamic neural networks. While there is a commonality in terms of reinforcement learning and neural networks, the specific applications (attitude control vs. learning control problems) suggest a relatively relevant connection between the two papers.","Relevance: Relatively relevant
Reason: While there is some overlap in keywords related to reinforcement learning and machine learning, the focus of the two papers appears to be different. Paper1 emphasizes optimal attitude control in satellites using artificial neural networks, while paper2 focuses on temporal differences, incremental learning, and prediction in the context of machine learning. The relevance is not as strong as in highly relevant cases, but there is a connection through the shared theme of learning methods."
145315,"Title: In: Machine Learning, Meta-reasoning and Logics, pp207-232,  Learning from Imperfect Data","Abstract: Systems interacting with real-world data must address the issues raised by the possible presence of errors in the observations it makes. In this paper we first present a framework for discussing imperfect data and the resulting problems it may cause. We distinguish between two categories of errors in data random errors or `noise', and systematic errors and examine their relationship to the task of describing observations in a way which is also useful for helping in future problem-solving and learning tasks. Secondly we proceed to examine some of the techniques currently used in AI research for recognising such errors.",Rule_Learning,649944,Title: Knowledge Integration and Learning,"Abstract: LIACC - Technical Report 91-1 Abstract. In this paper we address the problem of acquiring knowledge by integration . Our aim is to construct an integrated knowledge base from several separate sources. The objective of integration is to construct one system that exploits all the knowledge that is available and has good performance. The aim of this paper is to discuss the methodology of knowledge integration and present some concrete results. In our experiments the performance of the integrated theory exceeded the performance of the individual theories by quite a significant amount. Also, the performance did not fluctuate much when the experiments were repeated. These results indicate knowledge integration can complement other existing ML methods.",Rule_Learning,294126,Title: Knowledge Acquisition via Knowledge Integration,"Abstract: In this paper we are concerned with the problem of acquiring knowledge by integration. Our aim is to construct an integrated knowledge base from several separate sources. The need to merge knowledge bases can arise, for example, when knowledge bases are acquired independently from interactions with several domain experts. As opinions of different domain experts may differ, the knowledge bases constructed in this way will normally differ too. A similar problem can also arise whenever separate knowledge bases are generated by learning algorithms. The objective of integration is to construct one system that exploits all the knowledge that is available and has a good performance. The aim of this paper is to discuss the methodology of knowledge integration, describe the implemented system (INTEG.3), and present some concrete results which demonstrate the advantages of this method.",Rule_Learning,1,0,0,,"Machine Learning, Meta-reasoning, Logics, Imperfect Data, Real-world Data, Errors, Observations, Random Errors, Systematic Errors, Framework, Problem-solving, Learning Tasks, AI Research, Error Recognition, Rule Learning","Knowledge Integration, Learning, Integrated Knowledge Base, Methodology, Performance, Experiments, ML Methods, Rule Learning.","Knowledge Acquisition, Knowledge Integration, Integrated Knowledge Base, Merge Knowledge Bases, Domain Experts, Learning Algorithms, Opinion Differences, System Construction, Knowledge Utilization, Performance, Methodology, INTEG.3, Concrete Results, Advantages, Rule Learning.","Relevance: Highly relevant

Reason: Both papers involve machine learning, meta-reasoning, logics, and rule learning. Paper 1 focuses on the application of these concepts in the context of imperfect data, real-world data, and error recognition, while Paper 2 emphasizes knowledge integration, learning, and the use of machine learning methods for rule learning. The overlap in keywords and the shared emphasis on rule learning and machine learning make these papers highly relevant to each other.","Relevance: Highly relevant
Reason: Both papers share keywords related to machine learning, knowledge acquisition, knowledge integration, and rule learning. The overlap in topics, including learning algorithms and system construction, indicates a strong thematic connection. The relevance is high as the papers appear to address similar concepts and techniques within the broader context of machine learning, meta-reasoning, and knowledge utilization."
112378,Title: Language-Independent Data-Oriented Grapheme-to-Phoneme Conversion,"Abstract: We describe an approach to grapheme-to-phoneme conversion which is both language-independent and data-oriented. Given a set of examples (spelling words with their associated phonetic representation) in a language, a grapheme-to-phoneme conversion system is automatically produced for that language which takes as its input the spelling of words, and produces as its output the phonetic transcription according to the rules implicit in the training data. We describe the design of the system, and compare its performance to knowledge-based and alternative data-oriented approaches.",Case_Based,145215,Title: Memory-Based Lexical Acquisition and Processing,"Abstract: Current approaches to computational lexicology in language technology are knowledge-based (competence-oriented) and try to abstract away from specific formalisms, domains, and applications. This results in severe complexity, acquisition and reusability bottlenecks. As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks. The consequences of the approach for computational lexicology are discussed, and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology, morphology and syntax is described.",Case_Based,654177,Title: RAPID DEVELOPMENT OF NLP MODULES WITH MEMORY-BASED LEARNING,"Abstract: The need for software modules performing natural language processing (NLP) tasks is growing. These modules should perform efficiently and accurately, while at the same time rapid development is often mandatory. Recent work has indicated that machine learning techniques in general, and memory-based learning (MBL) in particular, offer the tools to meet both ends. We present examples of modules trained with MBL on three NLP tasks: (i) text-to-speech conversion, (ii) part-of-speech tagging, and (iii) phrase chunking. We demonstrate that the three modules display high generalization accuracy, and argue why MBL is applicable similarly well to a large class of other NLP tasks.",Case_Based,1,1,0,,"Language-Independent, Data-Oriented, Grapheme-to-Phoneme Conversion, Examples, Phonetic Representation, Conversion System, Spelling, Phonetic Transcription, Training Data, System Design, Performance Comparison, Knowledge-Based, Case-Based Approach","Memory-Based Learning, Lexical Acquisition, Processing, Computational Lexicology, Language Technology, Performance-Oriented Approach, Linguistic Tasks, Lexical Disambiguation, Phonology, Morphology, Syntax, Case-Based.","Rapid Development, NLP Modules, Memory-Based Learning, Software Modules, Natural Language Processing, Efficiency, Accuracy, Machine Learning Techniques, Text-to-Speech Conversion, Part-of-Speech Tagging, Phrase Chunking, Generalization Accuracy, Applicability, Case-Based.","
Relevance: Highly relevant

Reason: Both papers involve language technology, grapheme-to-phoneme conversion, and case-based approaches. Paper 1 focuses on language-independent systems and phonetic representation, while Paper 2 emphasizes memory-based learning, lexical acquisition, and performance-oriented approaches in computational lexicology. The overlap in keywords and the shared emphasis on language technology and case-based approaches make these papers highly relevant to each other.","Relevance: Highly relevant
Reason: Both papers share keywords related to language-independent systems, data-oriented approaches, and case-based methods in the context of natural language processing. The overlap in topics, such as grapheme-to-phoneme conversion, knowledge-based approaches, and memory-based learning, indicates a strong thematic connection. The relevance is high as the papers seem to address similar challenges and techniques within the broader context of language processing and conversion systems."
1129629,Title: of a simulator for evolving morphology are: Universal the simulator should cover an infinite gen,"Abstract: Funes, P. and Pollack, J. (1997) Computer Evolution of Buildable Objects. Fourth European Conference on Artificial Life. P. Husbands and I. Harvey, eds., MIT Press. pp 358-367. knowledge into the program, which would result in familiar structures, we provided the algorithm with a model of the physical reality and a purely utilitarian fitness function, thus supplying measures of feasibility and functionality. In this way the evolutionary process runs in an environment that has not been unnecessarily constrained. We added, however, a requirement of computability to reject overly complex structures when they took too long for our simulations to evaluate. The results are encouraging. The evolved structures had a surprisingly alien look: they are not based in common knowledge on how to build with brick toys; instead, the computer found ways of its own through the evolutionary search process. We were able to assemble the final designs manually and confirm that they accomplish the objectives introduced with our fitness functions. After some background on related problems, we describe our physical simulation model for two-dimensional Lego structures, and the representation for encoding them and applying evolution. We demonstrate the feasibility of our work with photos of actual objects which were the result of particular optimizations. Finally, we discuss future work and draw some conclusions. In order to evolve both the morphology and behavior of autonomous mechanical devices which can be manufactured, one must have a simulator which operates under several constraints, and a resultant controller which is adaptive enough to cover the gap between simulated and real world. eral space of mechanisms. Conservative - because simulation is never perfect, it should preserve a margin of safety. Efficient - it should be quicker to test in simulation than through physical production and test. Buildable - results should be convertible from a simula tion to a real object Computer Evolution of Buildable Objects Abstract The idea of co-evolution of bodies and brains is becoming popular, but little work has been done in evolution of physical structure because of the lack of a general framework for doing it. Evolution of creatures in simulation has been constrained by the reality gap which implies that resultant objects are usually not buildable. The work we present takes a step in the problem of body evolution by applying evolutionary techniques to the design of structures assembled out of parts. Evolution takes place in a simulator we designed, which computes forces and stresses and predicts failure for 2-dimensional Lego structures. The final printout of our program is a schematic assembly, which can then be built physically. We demonstrate its functionality in several different evolved entities.",Genetic_Algorithms,594047,Title: A Hybrid GP/GA Approach for Co-evolving Controllers and Robot Bodies to Achieve Fitness-Specified Tasks,"Abstract: Evolutionary approaches have been advocated to automate robot design. Some research work has shown the success of evolving controllers for the robots by genetic approaches. As we can observe, however, not only the controller but also the robot body itself can affect the behavior of the robot in a robot system. In this paper, we develop a hybrid GP/GA approach to evolve both controllers and robot bodies to achieve behavior-specified tasks. In order to assess the performance of the developed approach, it is used to evolve a simulated agent, with its own controller and body, to do obstacle avoidance in the simulated environment. Experimental results show the promise of this work. In addition, the importance of co-evolving controllers and robot bodies is analyzed and discussed in this paper.",Genetic_Algorithms,650834,Title: Neural Networks in an Artificial Life Perspective,"Abstract: In the last few years several researchers within the Artificial Life and Mobile Robotics community used Artificial Neural Networks. Explicitly viewing Neural Networks in an Artificial Life perspective has a number of consequences that make research on what we will call Artificial Life Neural Networks ( ALNNs) rather different from traditional connectionist research. The aim of the paper is to make the differences between ALNNs and ""classical"" neural networks explicit.",Genetic_Algorithms,0,1,0,,"Simulator, Evolving Morphology, Universal, Knowledge Incorporation, Fitness Function, Feasibility, Functionality, Evolutionary Process, Computability Requirement, Complex Structures, Alien-Looking Designs, Two-Dimensional Lego Structures, Physical Simulation Model, Evolutionary Search, Autonomous Mechanical Devices, Co-evolution, Bodies and Brains, Reality Gap, Evolutionary Techniques, Design of Structures, Genetic Algorithms","
Hybrid Approach, Genetic Programming, Genetic Algorithms, Co-evolution, Controllers, Robot Bodies, Fitness-Specified Tasks, Robot Design Automation, Evolutionary Robotics, Behavior-Specified Tasks, Obstacle Avoidance, Simulated Environment, Experimental Results.","
Neural Networks, Artificial Life Perspective, Artificial Life Neural Networks (ALNNs), Mobile Robotics, Connectionist Research, Research Differences, Genetic Algorithms.","Relevance: Highly relevant

Reason: Both papers share common keywords such as simulator, evolving morphology, universal, knowledge incorporation, and genetic algorithms. Paper 1 focuses on the design of structures and autonomous mechanical devices using evolutionary techniques, while Paper 2 addresses a hybrid approach in evolutionary robotics with a focus on robot design automation, obstacle avoidance, and fitness-specified tasks. The overlap in specific keywords and the shared emphasis on evolving structures using genetic algorithms make these papers highly relevant to each other.","Relevance: Relatively relevant
Reason: Both papers share keywords related to evolving morphology, genetic algorithms, and evolutionary techniques. While paper1 focuses on simulating the evolution of structures and autonomous mechanical devices, paper2 emphasizes neural networks and artificial life from a connectionist perspective, especially in the context of mobile robotics. The relevance is not as strong as in highly relevant cases, but there is a connection through the shared theme of evolutionary techniques and genetic algorithms.





"
645897,Title: A note on convergence rates of Gibbs sampling for nonparametric mixtures,"Abstract: We consider a mixture model where the mixing distribution is random and is given a Dirichlet process prior. We describe the general structure of two Gibbs sampling algorithms that are useful for approximating Bayesian inferences in this problem. When the kernel f(x j ) of the mixture is bounded, we show that the Markov chains resulting from the Gibbs sampling are uniformly ergodic, and we provide an explicit rate bound. Unfortunately, the bound is not sharp in general; improving sensibly the bound seems however quite difficult.",Probabilistic_Methods,126920,Title: Auxiliary Variable Methods for Markov Chain Monte Carlo with Applications,"Abstract: Suppose one wishes to sample from the density (x) using Markov chain Monte Carlo (MCMC). An auxiliary variable u and its conditional distribution (ujx) can be defined, giving the joint distribution (x; u) = (x)(ujx). A MCMC scheme which samples over this joint distribution can lead to substantial gains in efficiency compared to standard approaches. The revolutionary algorithm of Swendsen and Wang (1987) is one such example. In addition to reviewing the Swendsen-Wang algorithm and its generalizations, this paper introduces a new auxiliary variable method called partial decoupling. Two applications in Bayesian image analysis are considered. The first is a binary classification problem in which partial decoupling out performs SW and single site Metropolis. The second is a PET reconstruction which uses the gray level prior of Geman and McClure (1987). A generalized Swendsen-Wang algorithm is developed for this problem, which reduces the computing time to the point that MCMC is a viable method of posterior exploration.",Probabilistic_Methods,645897,Title: A note on convergence rates of Gibbs sampling for nonparametric mixtures,"Abstract: We consider a mixture model where the mixing distribution is random and is given a Dirichlet process prior. We describe the general structure of two Gibbs sampling algorithms that are useful for approximating Bayesian inferences in this problem. When the kernel f(x j ) of the mixture is bounded, we show that the Markov chains resulting from the Gibbs sampling are uniformly ergodic, and we provide an explicit rate bound. Unfortunately, the bound is not sharp in general; improving sensibly the bound seems however quite difficult.",Probabilistic_Methods,0,1,0,,,,,,
69418,Title: Constructive Algorithms for Hierarchical Mixtures of Experts,"Abstract: We present two additions to the hierarchical mixture of experts (HME) architecture. We view the HME as a tree structured classifier. Firstly, by applying a likelihood splitting criteria to each expert in the HME we ""grow"" the tree adaptively during training. Secondly, by considering only the most probable path through the tree we may ""prune"" branches away, either temporarily, or permanently if they become redundant. We demonstrate results for the growing and pruning algorithms which show significant speed ups and more efficient use of parameters over the conventional algorithms in discriminating between two interlocking spirals and classifying 8-bit parity patterns.",Neural_Networks,4330,Title: Hierarchical Mixtures of Experts and the EM Algorithm,"Abstract: We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. This report describes research done at the Dept. of Brain and Cognitive Sciences, the Center for Biological and Computational Learning, and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for CBCL is provided in part by a grant from the NSF (ASC-9217041). Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Dept. of Defense. The authors were supported by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, by by grant IRI-9013991 from the National Science Foundation, by grant N00014-90-J-1942 from the Office of Naval Research, and by NSF grant ECS-9216531 to support an Initiative in Intelligent Control at MIT. Michael I. Jordan is a NSF Presidential Young Investigator.",Probabilistic_Methods,1132459,Title: First experiments using a mixture of nonlinear experts for time series prediction,"Abstract: This paper investigates the advantages and disadvantages of the mixture of experts (ME) model (introduced to the connectionist community in [JJNH91] and applied to time series analysis in [WM95]) on two time series where the dynamics is well understood. The first series is a computer-generated series, consisting of a mixture between a noise-free process (the quadratic map) and a noisy process (a composition of a noisy linear autoregressive and a hyperbolic tangent). There are three main results: (1) the ME model produces significantly better results than single networks; (2) it discovers the regimes correctly and also allows us to characterize the sub-processes through their variances. (3) due to the correct matching of the noise level of the model to that of the data it avoids overfitting. The second series is the laser series used in the Santa Fe competition; the ME model also obtains excellent out-of-sample predictions, allows for analysis and shows no overfitting.",Neural_Networks,0,1,0,,"
Constructive Algorithms, Hierarchical Mixtures of Experts, Tree Structured Classifier, Likelihood Splitting Criteria, Adaptive Tree Growth, Branch Pruning, Redundancy Removal, Growing Algorithms, Pruning Algorithms, Speed Ups, Efficient Parameter Use, Conventional Algorithms, Interlocking Spirals, 8-bit Parity Patterns, Neural Networks","Hierarchical Mixtures of Experts, EM Algorithm, Supervised Learning, Tree-Structured Architecture, Generalized Linear Models, Maximum Likelihood, On-line Learning, Expectation-Maximization, Robot Dynamics, Comparative Simulation, Brain and Cognitive Sciences, Computational Learning, Artificial Intelligence Laboratory, Massachusetts Institute of Technology, NSF, Advanced Research Projects Agency, McDonnell-Pew Foundation, ATR Human Information Processing Research Laboratories, Siemens Corporation, National Science Foundation, Office of Naval Research, Initiative in Intelligent Control, Michael I. Jordan, Probabilistic Methods.","Mixture of Experts, Time Series Prediction, Nonlinear Experts, Dynamics Analysis, Computer-Generated Series, Noise-Free Process, Noisy Process, Quadratic Map, Autoregressive, Hyperbolic Tangent, Regime Discovery, Sub-process Characterization, Variances, Laser Series, Santa Fe Competition, Out-of-sample Predictions, Overfitting Avoidance.","
2 / 2

Relevance: Highly relevant

Reason: Both papers involve hierarchical mixtures of experts, tree-structured architecture, and probabilistic methods. Paper 1 focuses on constructive algorithms, tree growth, and pruning in the context of neural networks, while Paper 2 discusses EM algorithm, supervised learning, and applications in robot dynamics. The overlap in keywords and the shared emphasis on hierarchical mixtures of experts and tree-structured architecture make these papers highly relevant to each other.","Relevance: Highly relevant
Reason: Both papers share keywords related to hierarchical mixtures of experts, constructive algorithms, and neural networks. The overlap in topics, including tree-structured classifiers, adaptive tree growth, and pruning algorithms in paper1, and mixture of experts, time series prediction, and nonlinear experts in paper2, indicates a strong thematic connection. The relevance is high as the papers appear to address similar concepts and techniques within the broader context of constructive algorithms and hierarchical models in machine learning, with a focus on expert models and predictive analysis."
134199,Title: Using Markov Chains to Analyze GAFOs,"Abstract: Our theoretical understanding of the properties of genetic algorithms (GAs) being used for function optimization (GAFOs) is not as strong as we would like. Traditional schema analysis provides some first order insights, but doesn't capture the non-linear dynamics of the GA search process very well. Markov chain theory has been used primarily for steady state analysis of GAs. In this paper we explore the use of transient Markov chain analysis to model and understand the behavior of finite population GAFOs observed while in transition to steady states. This approach appears to provide new insights into the circumstances under which GAFOs will (will not) perform well. Some preliminary results are presented and an initial evaluation of the merits of this approach is provided.",Genetic_Algorithms,447224,Title: A COMPRESSION ALGORITHM FOR PROBABILITY TRANSITION MATRICES,"Abstract: This paper describes a compression algorithm for probability transition matrices. The compressed matrix is itself a probability transition matrix. In general the compression is not error-free, but the error appears to be small even for high levels of compression.",Genetic_Algorithms,134219,Title: An Overview of Evolutionary Computation,"Abstract: Evolutionary computation uses computational models of evolution - ary processes as key elements in the design and implementation of computer-based problem solving systems. In this paper we provide an overview of evolutionary computation, and describe several evolutionary algorithms that are currently of interest. Important similarities and di fferences are noted, which lead to a discussion of important issues that need to be resolved, and items for future research.",Genetic_Algorithms,1,0,0,,"Markov Chains, Genetic Algorithms, Function Optimization, GAFOs, Theoretical Understanding, Schema Analysis, Non-linear Dynamics, GA Search Process, Steady State Analysis, Transient Markov Chain Analysis, Finite Population, Transition to Steady States, Performance Evaluation, Insights, Circumstances, Preliminary Results","Compression Algorithm, Probability Transition Matrices, Error-Free Compression, Genetic Algorithms.","Evolutionary Computation, Computational Models, Evolutionary Processes, Evolutionary Algorithms, Problem Solving Systems, Overview, Similarities, Differences, Research, Genetic Algorithms.","Relevance: Relatively relevant

Reason: Both papers involve genetic algorithms, but Paper 1 focuses on Markov chains, function optimization, and theoretical understanding, while Paper 2 emphasizes compression algorithms and probability transition matrices. While there is a commonality in the use of genetic algorithms, the specific topics and themes covered in each paper suggest a relatively relevant connection rather than high relevance.","
Relevance: Relatively relevant
Reason: Both papers share keywords related to genetic algorithms and evolutionary computation, indicating a connection in the broader context of evolutionary processes and algorithms. However, while paper1 emphasizes Markov chains, function optimization, and schema analysis, paper2 provides an overview of evolutionary computation and computational models with a focus on similarities and differences. The relevance is not as strong as in highly relevant cases, but there is a connection through the shared theme of evolutionary computation and genetic algorithms."
