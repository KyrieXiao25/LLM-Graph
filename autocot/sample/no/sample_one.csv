cite_id,cite_title,cite_abs,cite_label,cited_id,cited_title,cited_abs,cited_label,link,autocot,citation_rules
35,"Title: 4 Implementing Application Specific Routines  Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.","Abstract: To implement a specific application, you should only have to change the file app.c. Section 2 describes the routines in app.c in detail. If you use additional variables for your specific problem, the easiest method of making them available to other program units is to declare them in sga.h and external.h. However, take care that you do not redeclare existing variables. Two example applications files are included in the SGA-C distribution. The file app1.c performs the simple example problem included with the Pascal version; finding the maximum of x 10 , where x is an integer interpretation of a chromosome. A slightly more complex application is include in app2.c. This application illustrates two features that have been added to SGA-C. The first of these is the ithruj2int function, which converts bits i through j in a chromosome to an integer. The second new feature is the utility pointer that is associated with each population member. The example application interprets each chromosome as a set of concatenated integers in binary form. The lengths of these integer fields is determined by the user-specified value of field size, which is read in by the function app data(). The field size must be less than the smallest of the chromosome length and the length of an unsigned integer. An integer array for storing the interpreted form of each chromosome is dynamically allocated and assigned to the chromosome's utility pointer in app malloc(). The ithruj2int routine (see utility.c) is used to translate each chromosome into its associated vector. The fitness for each chromosome is simply the sum of the squares of these integers. This example application will function for any chromosome length. SGA-C is intended to be a simple program for first-time GA experimentation. It is not intended to be definitive in terms of its efficiency or the grace of its implementation. The authors are interested in the comments, criticisms, and bug reports from SGA-C users, so that the code can be refined for easier use in subsequent versions. Please email your comments to rob@galab2.mh.ua.edu, or write to TCGA: The authors gratefully acknowledge support provided by NASA under Grant NGT-50224 and support provided by the National Science Foundation under Grant CTS-8451610. We also thank Hillol Kargupta for donating his tournament selection implementation. Booker, L. B. (1982). Intelligent behavior as an adaptation to the task environment (Doctoral dissertation, Technical Report No. 243. Ann Arbor: University of Michigan, Logic of Computers Group). Dissertations Abstracts International, 43(2), 469B. (University Microfilms No. 8214966)",Genetic_Algorithms,263498,Title: Automatic Design of Cellular Neural Networks by means of Genetic Algorithms: Finding a Feature Detector,"Abstract: This paper aims to examine the use of genetic algorithms to optimize subsystems of cellular neural network architectures. The application at hand is character recognition: the aim is to evolve an optimal feature detector in order to aid a conventional classifier network to generalize across different fonts. To this end, a performance function and a genetic encoding for a feature detector are presented. An experiment is described where an optimal feature detector is indeed found by the genetic algorithm. We are interested in the application of cellular neural networks in computer vision. Genetic algorithms (GA's) [1-3] can serve to optimize the design of cellular neural networks. Although the design of the global architecture of the system could still be done by human insight, we propose that specific sub-modules of the system are best optimized using one or other optimization method. GAs are a good candidate to fulfill this optimization role, as they are well suited to problems where the objective function is a complex function of many parameters. The specific problem we want to investigate is one of character recognition. More specifically, we would like to use the GA to find optimal feature detectors to be used in the recognition of digits .",Genetic_Algorithms,1,"Sure, let's break this down step by step.

Firstly, in Paper 1:

There is no explicit citation provided in the text.
The authors mention using two example applications included in the SGA-C distribution, but they don't attribute or cite Paper 2 directly.
In Paper 2:

There is no direct mention or citation of Paper 1.
Paper 2 talks about the use of genetic algorithms in optimizing cellular neural network architectures for character recognition.
Given the provided information from both papers, there doesn't appear to be a direct citation of Paper 2 within Paper 1, nor vice versa. Both papers discuss the application of genetic algorithms in different contexts without referencing each other.","Title/Source Citation: If one paper explicitly mentions the title or source of another paper along with contextual relevance or discussion, it indicates a potential citation relationship.
Acknowledgment of Contribution: When a paper acknowledges or references the work, findings, or methodology of another paper, it suggests a possible citation relationship.
Shared Authors or Collaborators: If there are shared authors or collaborators between two papers, it often indicates a citation relationship, especially if the topics are related.
Similar Research Focus or Context: Papers addressing similar topics, methodologies, or problems might mutually reference each other, implying a citation relationship."
45605,Title: Learning and evolution in neural networks,Abstract: DIMACS Technical Report 96-56 December 1996,Genetic_Algorithms,503871,Title: Growing neural networks,"Abstract: Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency.",Genetic_Algorithms,1,"Based on the provided information:

Paper 1:

Title: ""Learning and evolution in neural networks""
Abstract: DIMACS Technical Report 96-56 December 1996
Category: Genetic Algorithms
Paper 2:

Title: ""Growing neural networks""
Abstract: Discusses the use of genetic algorithms for feature selection in machine learning contexts.
Category: Genetic Algorithms
There isn't direct evidence in the information provided to confirm whether Paper 1 cites Paper 2 or vice versa. The details available don't explicitly mention citations or references between the two papers. Both papers seem to explore neural networks and their relationship with evolutionary processes but without specific references to each other.","Shared Conceptual Alignment: When one paper shares a conceptual alignment or discusses similar methodologies, techniques, or problem-solving approaches introduced or detailed in another paper, it hints at a potential citation relationship between the two."
9581,Title: DE-NOISING BY reconstruction f n is defined in the wavelet domain by translating all the,"Abstract: p n. We prove two results about that estimator. [Smooth]: With high probability ^ f fl n is at least as smooth as f , in any of a wide variety of smoothness measures. [Adapt]: The estimator comes nearly as close in mean square to f as any measurable estimator can come, uniformly over balls in each of two broad scales of smoothness classes. These two properties are unprecedented in several ways. Our proof of these results develops new facts about abstract statistical inference and its connection with Acknowledgements. These results were described at the Symposium on Wavelet Theory, held in connection with the Shanks Lectures at Van-derbilt University, April 3-4 1992. The author would like to thank Professor L.L. Schumaker for hospitality at the conference, and R.A. DeVore, Iain Johnstone, Gerard Kerkyacharian, Bradley Lucier, A.S. Nemirovskii, Ingram Olkin, and Dominique Picard for interesting discussions and correspondence on related topics. The author is also at the University of California, Berkeley",Probabilistic_Methods,1130780,Title: Wavelet Shrinkage: Asymptopia?,"Abstract: Considerable effort has been directed recently to develop asymptotically minimax methods in problems of recovering infinite-dimensional objects (curves, densities, spectral densities, images) from noisy data. A rich and complex body of work has evolved, with nearly- or exactly- minimax estimators being obtained for a variety of interesting problems. Unfortunately, the results have often not been translated into practice, for a variety of reasons sometimes, similarity to known methods, sometimes, computational intractability, and sometimes, lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data; one translates the empirical wavelet coefficients towards the origin by an amount method is different from methods in common use today, is computationally practical, and is spatially adaptive; thus it avoids a number of previous objections to minimax estimators. At the same time, the method is nearly minimax for a wide variety of loss functions - e.g. pointwise error, global error measured in L p norms, pointwise and global error in estimation of derivatives and for a wide range of smoothness classes, including standard Holder classes, Sobolev classes, and Bounded Variation. This is a much broader near-optimality than anything previously proposed in the minimax literature. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity. Acknowledgements: These results have been described at the Oberwolfach meeting `Mathematische Stochastik' December, 1992 and at the AMS Annual meeting, January 1993. This work was supported by NSF DMS 92-09130. The authors would like to thank Paul-Louis Hennequin, who organized the Ecole d' Ete de Probabilites at Saint Flour 1990, where this collaboration began, and to Universite de Paris VII (Jussieu) and Universite de Paris-sud (Orsay) for supporting visits of DLD and IMJ. The authors would like to thank Ildar Ibragimov and Arkady Nemirovskii for personal correspondence cited below. p",Probabilistic_Methods,1,"From the information provided:

Paper 1:

Title: ""DE-NOISING BY reconstruction f n is defined in the wavelet domain by translating all the""
Abstract: Discusses estimators in the wavelet domain and their smoothness properties. Mentions results presented at a symposium and acknowledges individuals for discussions and correspondence.
Category: Probabilistic Methods
Paper 2:

Title: ""Wavelet Shrinkage: Asymptopia?""
Abstract: Describes a method for curve estimation using empirical wavelet coefficients, emphasizing computational practicality and spatial adaptivity. Acknowledges meetings and support received for the work.
Category: Probabilistic Methods
There isn't explicit evidence within the provided information to suggest that Paper 1 cites Paper 2 or vice versa. Both papers seem to explore aspects of wavelet-based methods in signal processing or curve estimation within the domain of probabilistic methods, but without direct references to each other. The acknowledgments in each paper mention various individuals, conferences, or meetings, but they do not specifically reference each other's work.","1. **Methodology Reference:** If one paper describes or discusses the methodology, techniques, or concepts introduced in another paper, indicating a clear connection or relevance between the content, it implies a potential citation relationship.
2. **Acknowledgment of Findings or Results:** When a paper acknowledges or refers to the specific findings, results, or theories presented in another paper, demonstrating a relationship or influence, it signifies a potential citation relationship."
40,Title: Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques,"Abstract: This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.",Genetic_Algorithms,1114442,Title: Soft Computing: the Convergence of Emerging Reasoning Technologies,"Abstract: The term Soft Computing (SC) represents the combination of emerging problem-solving technologies such as Fuzzy Logic (FL), Probabilistic Reasoning (PR), Neural Networks (NNs), and Genetic Algorithms (GAs). Each of these technologies provide us with complementary reasoning and searching methods to solve complex, real-world problems. After a brief description of each of these technologies, we will analyze some of their most useful combinations, such as the use of FL to control GAs and NNs parameters; the application of GAs to evolve NNs (topologies or weights) or to tune FL controllers; and the implementation of FL controllers as NNs tuned by backpropagation-type algorithms.",Genetic_Algorithms,1,"From the provided information:

Paper 1:

Title: ""Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques""
Abstract: Discusses the utilization of fuzzy logic techniques to dynamically control parameter settings in genetic algorithms (GAs). It introduces a method for automatically designing and tuning the fuzzy knowledge-base system using GAs. Shows performance improvement over static GAs.
Category: Genetic Algorithms
Paper 2:

Title: ""Soft Computing: the Convergence of Emerging Reasoning Technologies""
Abstract: Discusses Soft Computing (SC), which encompasses technologies like Fuzzy Logic (FL), Probabilistic Reasoning (PR), Neural Networks (NNs), and Genetic Algorithms (GAs). Examines combinations and applications of these technologies, such as using FL to control GAs' parameters.
Category: Genetic Algorithms
Paper 1 proposes using fuzzy logic to control GA parameters, which aligns with the discussion in Paper 2 about the use of FL to control GAs. However, based on the provided information, there's no explicit mention or citation of Paper 2 within Paper 1.

While both papers discuss related topics concerning the use of fuzzy logic in conjunction with genetic algorithms, there's no direct reference from Paper 1 to Paper 2 or vice versa.","Conceptual Integration: When one paper discusses the methodology, techniques, or concepts introduced in another paper, specifically focusing on their integration or application within its content, it suggests a potential citation relationship between the two papers."
1365,Title: Cholinergic suppression of transmission may allow combined associative memory function and self-organization in the neocortex.,"Abstract: Selective suppression of transmission at feedback synapses during learning is proposed as a mechanism for combining associative feedback with self-organization of feedforward synapses. Experimental data demonstrates cholinergic suppression of synaptic transmission in layer I (feedback synapses), and a lack of suppression in layer IV (feed-forward synapses). A network with this feature uses local rules to learn mappings which are not linearly separable. During learning, sensory stimuli and desired response are simultaneously presented as input. Feedforward connections form self-organized representations of input, while suppressed feedback connections learn the transpose of feedfor-ward connectivity. During recall, suppression is removed, sensory input activates the self-organized representation, and activity generates the learned response.",Neural_Networks,22835,Title: A Scalable Performance Prediction Method for Parallel Neural Network Simulations,"Abstract: A performance prediction method is presented for indicating the performance range of MIMD parallel processor systems for neural network simulations. The total execution time of a parallel application is modeled as the sum of its calculation and communication times. The method is scalable because based on the times measured on one processor and one communication link, the performance, speedup, and efficiency can be predicted for a larger processor system. It is validated quantitatively by applying it to two popular neural networks, backpropagation and the Kohonen self-organizing feature map, decomposed on a GCel-512, a 512 transputer system. Agreement of the model with the measurements is within 9%.",Neural_Networks,1,"From the provided information:

Paper 1:

Title: ""Cholinergic suppression of transmission may allow combined associative memory function and self-organization in the neocortex.""
Abstract: Discusses the selective suppression of transmission at feedback synapses during learning and proposes this as a mechanism to combine associative feedback with self-organization of feedforward synapses in the neocortex.
Category: Neural Networks
Paper 2:

Title: ""A Scalable Performance Prediction Method for Parallel Neural Network Simulations""
Abstract: Presents a performance prediction method for indicating the performance range of MIMD parallel processor systems for neural network simulations. The method models the total execution time of a parallel application as the sum of calculation and communication times and is validated quantitatively.
Category: Neural Networks
There is no explicit mention or citation of Paper 2 within the information provided for Paper 1, and vice versa. Paper 1 delves into the mechanisms of synaptic transmission and learning in neural networks, while Paper 2 focuses on a performance prediction method for parallel neural network simulations. Based on the information provided, there doesn't appear to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Conceptual Relevance: When one paper discusses findings, concepts, or methodologies that align or complement the themes, mechanisms, or techniques introduced in another paper, it suggests a potential citation relationship between the two."
27531,Title: Using Sampling and Queries to Extract Rules from Trained Neural Networks,"Abstract: Concepts learned by neural networks are difficult to understand because they are represented using large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract symbolic rules that describe their classification behavior. There are several existing rule-extraction approaches that operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a learning problem. In addition to learning from training examples, our method exploits the property that networks can be efficiently queried. We describe algorithms for extracting both conjunctive and M -of-N rules, and present experiments that show that our method is more efficient than conventional search-based approaches.",Neural_Networks,1106370,Title: The Effective Size of a Neural Network: A Principal Component Approach,"Abstract: Often when learning from data, one attaches a penalty term to a standard error term in an attempt to prefer simple models and prevent overfitting. Current penalty terms for neural networks, however, often do not take into account weight interaction. This is a critical drawback since the effective number of parameters in a network usually differs dramatically from the total number of possible parameters. In this paper, we present a penalty term that uses Principal Component Analysis to help detect functional redundancy in a neural network. Results show that our new algorithm gives a much more accurate estimate of network complexity than do standard approaches. As a result, our new term should be able to improve techniques that make use of a penalty term, such as weight decay, weight pruning, feature selection, Bayesian, and prediction-risk tech niques.",Neural_Networks,1,"Based on the provided information:

Paper 1:

Title: ""Using Sampling and Queries to Extract Rules from Trained Neural Networks""
Abstract: Introduces a novel method for extracting symbolic rules from trained neural networks by approaching rule extraction as a learning problem rather than a search problem. The method leverages the ability to efficiently query networks and describes algorithms for extracting different types of rules.
Category: Neural Networks
Paper 2:

Title: ""The Effective Size of a Neural Network: A Principal Component Approach""
Abstract: Discusses the issue of incorporating penalty terms in neural networks to prevent overfitting and introduces a penalty term utilizing Principal Component Analysis to detect functional redundancy in neural networks. Demonstrates that this new algorithm provides a more accurate estimate of network complexity compared to standard approaches.
Category: Neural Networks
There's no explicit mention or citation of Paper 2 within the information provided for Paper 1, and vice versa. Paper 1 focuses on rule extraction from trained neural networks, while Paper 2 deals with estimating network complexity and incorporating penalty terms to prevent overfitting in neural networks. Based on the given information, there doesn't seem to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Methodological Contribution: When one paper presents methodologies, techniques, or novel approaches that directly complement or improve upon concepts introduced in another paper, it indicates a potential citation relationship between the two."
6814,Title: Factorial Hidden Markov Models,"Abstract: One of the basic probabilistic tools used for time series modeling is the hidden Markov model (HMM). In an HMM, information about the past of the time series is conveyed through a single discrete variable|the hidden state. We present a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. Both inference and learning in this model depend critically on computing the posterior probabilities of the hidden state variables given the observations. We present an exact algorithm for inference in this model, and relate it to the Forward-Backward algorithm for HMMs and to algorithms for more general belief networks. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or mean field theory. We also present a structured approximation in which the the state variables are decoupled, based on which we derive a tractable learning algorithm. Empirical comparisons suggest that these approximations are efficient and accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that it outperforms HMMs in capturing the complex temporal patterns in this dataset.",Probabilistic_Methods,293974,Title: Tractable Inference for Complex Stochastic Processes,"Abstract: The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief statea probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs), where the process itself can be represented compactly, the representation of the belief state is intractable. We investigate the idea of maintaining a compact approximation to the true belief state, and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant. We show that the error in a belief state contracts exponentially as the process evolves. Thus, even with multiple approximations, the error in our process remains bounded indefinitely. We show how the additional structure of a DBN can be used to design our approximation scheme, improving its performance significantly. We demonstrate the applicability of our ideas in the context of a monitoring task, showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy.",Probabilistic_Methods,1,"From the provided information:

Paper 1:

Title: ""Factorial Hidden Markov Models""
Abstract: Discusses a generalization of hidden Markov models (HMMs) by factoring the hidden state into multiple state variables, presenting algorithms for inference and learning in this model, including both exact and approximate methods. Empirically compares these approximations to exact methods and applies the model to model Bach's chorales.
Category: Probabilistic Methods
Paper 2:

Title: ""Tractable Inference for Complex Stochastic Processes""
Abstract: Investigates maintaining a compact approximation to the belief state of a stochastic system, especially in cases where explicit representation of the belief state is intractable due to large state spaces. Analyzes the error in the approximation scheme over the lifetime of the process and shows that the error remains bounded even with multiple approximations. Demonstrates the applicability of these ideas in a monitoring task, achieving faster inference with a minimal decrease in accuracy.
Category: Probabilistic Methods
There's no explicit mention or citation of Paper 2 within the information provided for Paper 1, and vice versa. Paper 1 introduces factorial hidden Markov models and discusses inference and learning methods for these models, while Paper 2 delves into maintaining compact approximations for belief states in stochastic systems. Based on the provided information, there doesn't seem to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Methodological Comparison or Enhancement: When one paper introduces advancements, methodologies, or approaches that directly relate to or improve upon concepts presented in another paper within a similar domain or field, it suggests a potential citation relationship between the two."
34257,Title: A Self-Adjusting Dynamic Logic Module,"Abstract: This paper presents an ASOCS (Adaptive Self-Organizing Concurrent System) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic, robotics, logical inference, and dynamic control. An ASOCS is an adaptive network composed of many simple computing elements operating asynchronously and in parallel. This paper focuses on Adaptive Algorithm 2 (AA2) and details its architecture and learning algorithm. AA2 has significant memory and knowledge maintenance advantages over previous ASOCS models. An ASOCS can operate in either a data processing mode or a learning mode. During learning mode, the ASOCS is given a new rule expressed as a boolean conjunction. The AA2 learning algorithm incorporates the new rule in a distributed fashion in a short, bounded time. During data processing mode, the ASOCS acts as a parallel hardware circuit.",Neural_Networks,87482,Title: A Multi-Chip Module Implementation of a Neural Network,"Abstract: The requirement for dense interconnect in artificial neural network systems has led researchers to seek high-density interconnect technologies. This paper reports an implementation using multi-chip modules (MCMs) as the interconnect medium. The specific system described is a self-organizing, parallel, and dynamic learning model which requires a dense interconnect technology for effective implementation; this requirement is fulfilled by exploiting MCM technology. The ideas presented in this paper regarding an MCM implementation of artificial neural networks are versatile and can be adapted to apply to other neural network and connectionist models.",Neural_Networks,1,"From the information provided:

Paper 1:

Title: ""A Self-Adjusting Dynamic Logic Module""
Abstract: Introduces the ASOCS (Adaptive Self-Organizing Concurrent System) model for massively parallel processing of incrementally defined rule systems in areas like adaptive logic, robotics, logical inference, and dynamic control. Focuses on Adaptive Algorithm 2 (AA2) and details its architecture and learning algorithm. Discusses the advantages of AA2 in memory and knowledge maintenance.
Category: Neural Networks
Paper 2:

Title: ""A Multi-Chip Module Implementation of a Neural Network""
Abstract: Discusses the use of multi-chip modules (MCMs) as a high-density interconnect technology for implementing self-organizing, parallel, and dynamic learning models in neural networks. Describes the specific system implemented using MCMs and its adaptability to other neural network models.
Category: Neural Networks
There is no explicit mention or citation of Paper 2 within the provided information for Paper 1, and vice versa. Paper 1 discusses an ASOCS model for massively parallel processing of rule systems, while Paper 2 focuses on implementing neural network models using multi-chip modules (MCMs) as interconnect technology. Based on the provided information, there doesn't seem to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Technological Implementation Utilization: When one paper introduces or utilizes a specific technological methodology, technique, or implementation approach detailed in another paper, especially within a related field or domain, it suggests a potential citation relationship between the two."
164885,Title: Analyzing GAs Using Markov Models with Semantically Ordered and Lumped States,"Abstract: At the previous FOGA workshop, we presented some initial results on using Markov models to analyze the transient behavior of genetic algorithms (GAs) being used as function optimizers (GAFOs). In that paper, the states of the Markov model were ordered via a simple and mathematically convenient lexicographic ordering used initially by Nix and Vose. In this paper, we explore alternative orderings of states based on interesting semantic properties such as average fitness, degree of homogeneity, average attractive force, etc. We also explore lumping techniques for reducing the size of the state space. Analysis of these reordered and lumped Markov models provides new insights into the transient behavior of GAs in general and GAFOs in particular.",Genetic_Algorithms,447224,Title: A COMPRESSION ALGORITHM FOR PROBABILITY TRANSITION MATRICES,"Abstract: This paper describes a compression algorithm for probability transition matrices. The compressed matrix is itself a probability transition matrix. In general the compression is not error-free, but the error appears to be small even for high levels of compression.",Genetic_Algorithms,1,"From the information provided:

Paper 1:

Title: ""Analyzing GAs Using Markov Models with Semantically Ordered and Lumped States""
Abstract: Discusses the use of Markov models to analyze the transient behavior of genetic algorithms (GAs) used as function optimizers (GAFOs). Explores different orderings of states based on semantic properties and lumping techniques to reduce the state space size. Provides new insights into the transient behavior of GAs.
Category: Genetic Algorithms
Paper 2:

Title: ""A COMPRESSION ALGORITHM FOR PROBABILITY TRANSITION MATRICES""
Abstract: Describes a compression algorithm specifically for probability transition matrices, producing a compressed matrix that remains a probability transition matrix. Acknowledges that the compression is not error-free but indicates that the error tends to be small even with high levels of compression.
Category: Genetic Algorithms
There is no explicit mention or citation of Paper 2 within the information provided for Paper 1, and vice versa. Paper 1 focuses on analyzing GAs using Markov models with reordered and lumped states, whereas Paper 2 discusses a compression algorithm for probability transition matrices. Based on the information provided, there doesn't appear to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Methodological Alignment: When one paper adopts or builds upon a specific methodology, technique, or algorithm detailed in another paper, particularly within a similar domain or field, it implies a potential citation relationship between the two."
40,Title: Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques,"Abstract: This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.",Genetic_Algorithms,116552,Title: Empirical studies of the genetic algorithm with non-coding segments,"Abstract: The genetic algorithm (GA) is a problem solving method that is modelled after the process of natural selection. We are interested in studying a specific aspect of the GA: the effect of non-coding segments on GA performance. Non-coding segments are segments of bits in an individual that provide no contribution, positive or negative, to the fitness of that individual. Previous research on non-coding segments suggests that including these structures in the GA may improve GA performance. Understanding when and why this improvement occurs will help us to use the GA to its full potential. In this article, we discuss our hypotheses on non-coding segments and describe the results of our experiments. The experiments may be separated into two categories: testing our program on problems from previous related studies, and testing new hypotheses on the effect of non-coding segments.",Genetic_Algorithms,1,"From the information provided:

Paper 1:

Title: ""Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques""
Abstract: Proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). Introduces the Dynamic Parametric GA that uses a fuzzy knowledge-based system to control GA parameters. Describes a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs and reports performance improvements over a simple static GA.
Category: Genetic Algorithms
Paper 2:

Title: ""Empirical studies of the genetic algorithm with non-coding segments""
Abstract: Investigates the impact of non-coding segments in genetic algorithms (GA), exploring how these segments, which do not contribute to an individual's fitness, might affect GA performance. Discusses hypotheses and experimental results related to the effects of non-coding segments on GA performance.
Category: Genetic Algorithms
There isn't explicit mention or citation of Paper 2 within the provided information for Paper 1, and vice versa. Paper 1 discusses using fuzzy logic techniques to control GA parameters dynamically, while Paper 2 focuses on empirical studies regarding the impact of non-coding segments on GA performance. Based on the information given, there doesn't appear to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Methodological Influence: When one paper presents findings or techniques that directly impact or align with specific aspects, methodologies, or areas of research introduced or explored in another paper, it suggests a potential citation relationship between the two."
1122642,Title: Word Perfect Corp. LIA: A Location-Independent Transformation for ASOCS Adaptive Algorithm 2,"Abstract: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and often suffer from a number of shortcomings as a result. ANNs that use dynamic topologies have shown ability to overcome many of these problems. Adaptive Self Organizing Concurrent Systems (ASOCS) are a class of learning models with inherently dynamic topologies. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing learning models that use dynamic topologies efficiently in parallel hardware. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. In particular, this paper presents the Location - Independent ASOCS (LIA) model as a LIT for ASOCS Adaptive Algorithm 2. The description of LIA gives formal definitions for LIA algorithms. Because LIA implements basic ASOCS mechanisms, these definitions provide a formal description of basic ASOCS mechanisms in general, in addition to LIA.",Neural_Networks,1153097,Title: An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes,"Abstract: A General Result on the Stabilization of Linear Systems Using Bounded Controls 1 ABSTRACT We present two constructions of controllers that globally stabilize linear systems subject to control saturation. We allow essentially arbitrary saturation functions. The only conditions imposed on the system are the obvious necessary ones, namely that no eigenvalues of the uncontrolled system have positive real part and that the standard stabilizability rank condition hold. One of the constructions is in terms of a ""neural-network type"" one-hidden layer architecture, while the other one is in terms of cascades of linear maps and saturations.",Neural_Networks,0,"From the provided information:

Paper 1:

Title: ""Word Perfect Corp. LIA: A Location-Independent Transformation for ASOCS Adaptive Algorithm 2""
Abstract: Introduces Location-Independent Transformations (LITs) as a strategy for implementing learning models with dynamic topologies efficiently in parallel hardware. Specifically, it presents the Location-Independent ASOCS (LIA) model as a LIT for ASOCS Adaptive Algorithm 2. Describes LIA algorithms and formal definitions for ASOCS mechanisms.
Category: Neural Networks
Paper 2:

Title: ""An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes""
Abstract: Discusses stabilization of linear systems using controllers subject to control saturation. Presents two constructions of controllers for global stabilization of linear systems with arbitrary saturation functions, ensuring conditions for stabilization.
Category: Neural Networks
Based on the provided information, there is no explicit mention or citation of Paper 2 within Paper 1, and vice versa. Paper 1 focuses on introducing Location-Independent Transformations (LITs) and describes the Location-Independent ASOCS (LIA) model, while Paper 2 discusses the stabilization of linear systems using controllers subject to control saturation. There doesn't appear to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Independent Topic Focus: When one paper focuses on presenting or exploring a distinct topic, methodology, or concept that does not directly align or overlap with the content or area of research introduced or detailed in another paper, it implies a lack of a citation relationship between the two."
78994,Title: Adaptive tuning of numerical weather prediction models: Randomized GCV in three and four dimensional data assimilation,"Abstract: This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.",Neural_Networks,72101,Title: Constructive Learning of Recurrent Neural Networks: Limitations of Recurrent Casade Correlation and a Simple Solution,"Abstract: It is often difficult to predict the optimal neural network size for a particular application. Constructive or destructive methods that add or subtract neurons, layers, connections, etc. might offer a solution to this problem. We prove that one method, Recurrent Cascade Correlation, due to its topology, has fundamental limitations in representation and thus in its learning capabilities. It cannot represent with monotone (i.e. sigmoid) and hard-threshold activation functions certain finite state automata. We give a ""preliminary"" approach on how to get around these limitations by devising a simple constructive training method that adds neurons during training while still preserving the powerful fully-recurrent structure. We illustrate this approach by simulations which learn many examples of regular grammars that the",Neural_Networks,0,"Based on the provided information:

Paper 1:

Title: ""Adaptive tuning of numerical weather prediction models: Randomized GCV in three and four dimensional data assimilation""
Abstract: Describes a system for the induction of oblique decision trees named OC1. This system combines deterministic hill-climbing with randomization to find a good oblique split at each node of a decision tree. It's specifically tuned for domains with numeric attributes and demonstrates the ability to construct oblique trees that are smaller and more accurate than axis-parallel counterparts. Empirical studies using real and artificial data are presented to analyze OC1's performance.
Category: Neural Networks
Paper 2:

Title: ""Constructive Learning of Recurrent Neural Networks: Limitations of Recurrent Cascade Correlation and a Simple Solution""
Abstract: Discusses difficulties in predicting the optimal size of neural networks for specific applications. Explores constructive methods that add or subtract neurons, layers, connections, etc., to address this problem. Highlights limitations of Recurrent Cascade Correlation in representing certain finite state automata and proposes a simple constructive training method to overcome these limitations while maintaining a fully-recurrent structure.
Category: Neural Networks
There isn't explicit mention or citation of Paper 2 within the provided information for Paper 1, and vice versa. Paper 1 focuses on the development and analysis of OC1, a system for constructing oblique decision trees, while Paper 2 discusses limitations of Recurrent Cascade Correlation and proposes a solution for constructing recurrent neural networks. Based on the information given, there doesn't appear to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Distinct Topic Focus: When one paper focuses on presenting methodologies, approaches, or findings that do not directly intersect or address the concepts, techniques, or domain discussed in another paper, it suggests a lack of a citation relationship between the two."
141347,Title: Adapting Crossover in a Genetic Algorithm,"Abstract: Traditionally, genetic algorithms have relied upon 1 and 2-point crossover operators. Many recent empirical studies, however, have shown the benefits of higher numbers of crossover points. Some of the most intriguing recent work has focused on uniform crossover, which involves on the average L/2 crossover points for strings of length L. Despite theoretical analysis, however, it appears difficult to predict when a particular crossover form will be optimal for a given problem. This paper describes an adaptive genetic algorithm that decides, as it runs, which form is optimal.",Genetic_Algorithms,1103031,"Title: Confidence as Higher Order Uncertainty proposed for handling higher order uncertainty, including the Bayesian approach,",Abstract:,Probabilistic_Methods,0,"From the information provided, Paper 1 focuses on adapting crossover in genetic algorithms, exploring the benefits of different crossover operators and proposing an adaptive genetic algorithm that dynamically determines the optimal form of crossover during execution.

Paper 2 discusses confidence as higher-order uncertainty and its application in handling higher-order uncertainty, including the Bayesian approach.

There isn't an explicit mention or citation of Paper 2 within the provided information for Paper 1, and vice versa. Paper 1 concentrates on adaptive genetic algorithms and crossover operators, while Paper 2 discusses higher-order uncertainty and confidence. Based on the provided information, there doesn't seem to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Disparate Research Areas: When two papers explore distinct topics, methodologies, or domains without overlapping concepts or direct relevance to each other's subject matter, it suggests a lack of a citation relationship between them."
30901,Title: Lookahead and Pathology in Decision Tree Induction,"Abstract: The standard approach to decision tree induction is a top-down, greedy algorithm that makes locally optimal, irrevocable decisions at each node of a tree. In this paper, we empirically study an alternative approach, in which the algorithms use one-level lookahead to decide what test to use at a node. We systematically compare, using a very large number of real and artificial data sets, the quality of decision trees induced by the greedy approach to that of trees induced using lookahead. The main observations from our experiments are: (i) the greedy approach consistently produced trees that were just as accurate as trees produced with the much more expensive lookahead step; and (ii) we observed many instances of pathology, i.e., lookahead produced trees that were both larger and less accurate than trees produced without it.",Theory,1109439,Title: Classifying Seismic Signals by Integrating Ensembles of Neural Networks,"Abstract: This paper proposes a classification scheme based on integration of multiple Ensembles of ANNs. It is demonstrated on a classification problem, in which seismic signals of Natural Earthquakes must be distinguished from seismic signals of Artificial Explosions. A Redundant Classification Environment consists of several Ensembles of Neural Networks is created and trained on Bootstrap Sample Sets, using various data representations and architectures. The ANNs within the Ensembles are aggregated (as in Bagging) while the Ensembles are integrated non-linearly, in a signal adaptive manner, using a posterior confidence measure based on the agreement (variance) within the Ensembles. The proposed Integrated Classification Machine achieved 92.1% correct classifications on the seismic test data. Cross Validation evaluations and comparisons indicate that such integration of a collection of ANN's Ensembles is a robust way for handling high dimensional problems with a complex non-stationary signal space as in the current Seismic Classification problem.",Neural_Networks,0,"From the information provided:

Paper 1:

Title: ""Lookahead and Pathology in Decision Tree Induction""
Abstract: Explores decision tree induction, comparing a standard greedy algorithm to an alternative approach using one-level lookahead. Empirical studies on numerous real and artificial datasets show that the greedy approach consistently produces trees as accurate as those produced with the more computationally expensive lookahead step. Additionally, it observes instances where lookahead produces larger and less accurate trees.
Category: Theory
Paper 2:

Title: ""Classifying Seismic Signals by Integrating Ensembles of Neural Networks""
Abstract: Proposes a classification scheme based on integrating multiple ensembles of artificial neural networks (ANNs) for distinguishing seismic signals of natural earthquakes from those of artificial explosions. Utilizes a redundant classification environment with various data representations and architectures, aggregating ANNs within ensembles (similar to Bagging) and integrating ensembles non-linearly using a posterior confidence measure based on ensemble agreement. Achieves 92.1% correct classifications on seismic test data.
Category: Neural Networks
Based on the provided information, there's no explicit mention or citation of Paper 2 within the abstract or content of Paper 1, and vice versa. Paper 1 focuses on decision tree induction methods, comparing greedy approaches with lookahead, while Paper 2 revolves around classifying seismic signals using ensemble integration of neural networks. There doesn't seem to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Unrelated Methodologies/Subjects: When two papers explore distinct methodologies or unrelated subjects without direct overlap in concepts or relevance to each other's research domain, it suggests a lack of a citation relationship between them."
578650,Title: Collective Memory Search 1 Collective Memory Search: Exploiting an Information Center for Exploration,"Abstract: The results reported here empirically show the benefit of decision tree size biases as a function of concept distribution. First, it is shown how concept distribution complexity (the number of internal nodes in the smallest decision tree consistent with the example space) affects the benefit of minimum size and maximum size decision tree biases. Second, a policy is described that defines what a learner should do given knowledge of the complexity of the distribution of concepts. Third, explanations for why the distribution of concepts seen in practice is amenable to the minimum size decision tree bias are given and evaluated empirically.",Genetic_Algorithms,8832,Title: The Possible Contribution of AI to the Avoidance of Crises and Wars: Using CBR Methods,"Abstract: This paper presents the application of Case-Based Reasoning methods to the KOSIMO data base of international conflicts. A Case-Based Reasoning tool - VIE-CBR has been deveolped and used for the classification of various outcome variables, like political, military, and territorial outcome, solution modalities, and conflict intensity. In addition, the case retrieval algorithms are presented as an interactive, user-modifiable tool for intelli gently searching the conflict data base for precedent cases.",Case_Based,0,"From the provided information:

Paper 1:

Title: ""Collective Memory Search: Exploiting an Information Center for Exploration""
Abstract: Discusses the benefit of decision tree size biases concerning concept distribution complexity. It explores how the complexity of concept distribution affects the advantages of biases in decision tree sizes. It describes a policy guiding learners based on the knowledge of concept distribution complexity and provides explanations for why the distribution of concepts often aligns with the minimum size decision tree bias.
Category: Genetic Algorithms
Paper 2:

Title: ""The Possible Contribution of AI to the Avoidance of Crises and Wars: Using CBR Methods""
Abstract: Introduces the application of Case-Based Reasoning (CBR) methods to the KOSIMO database of international conflicts. It presents the VIE-CBR tool for classifying various outcome variables in conflicts, such as political, military, and territorial outcomes, solution modalities, and conflict intensity. Additionally, it outlines case retrieval algorithms as an interactive tool for intelligently searching the conflict database for precedent cases.
Category: Case-Based
Based on the information provided, there is no explicit mention or citation of Paper 2 within the abstract or content of Paper 1, and vice versa. Paper 1 focuses on decision tree biases and concept distribution complexity, whereas Paper 2 delves into applying Case-Based Reasoning methods to international conflicts. There doesn't appear to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Divergent Research Areas: When two papers explore entirely different research domains or methodologies with no evident conceptual or topical overlap, it often suggests a lack of a citation relationship between them."
10798,"Title: Learning physical descriptions from functional definitions, examples, Learning from examples: The effect of different conceptual","Abstract: Technical Report 94-07 February 7, 1994 (updated April 25, 1994) This paper will appear in Proceedings of the Eleventh International Conference on Machine Learning. Abstract This paper presents an algorithm for incremental induction of decision trees that is able to handle both numeric and symbolic variables. In order to handle numeric variables, a new tree revision operator called `slewing' is introduced. Finally, a non-incremental method is given for finding a decision tree based on a direct metric of a candidate tree.",Case_Based,80491,"Title: References Linear Controller Design, Limits of Performance, ""The parallel projection operators of a nonlinear feedback","Abstract: 13] Yang, Y., H.J. Sussmann, and E.D. Sontag, ""Stabilization of linear systems with bounded controls,"" in Proc. Nonlinear Control Systems Design Symp., Bordeaux, June 1992 (M. Fliess, Ed.), IFAC Publications, pp. 15-20. Journal version to appear in IEEE Trans. Autom. Control .",Neural_Networks,0,"From the information provided, there's no explicit mention or citation of Paper 2 within the abstract or content of Paper 1, and vice versa. Paper 1 focuses on incremental induction of decision trees handling both numeric and symbolic variables, while Paper 2 appears to reference linear controller design and stabilization of linear systems with bounded controls in the context of nonlinear control systems.

Based on the information available, there doesn't seem to be a direct reference from Paper 1 to Paper 2 or from Paper 2 to Paper 1.","Distinct Research Focus: When two papers delve into vastly different research domains or fields, lacking thematic or conceptual alignment, it often indicates no citation relationship between them."
22886,Title: Continuous Case-Based Reasoning,"Abstract: Case-based reasoning systems have traditionally been used to perform high-level reasoning in problem domains that can be adequately described using discrete, symbolic representations. However, many real-world problem domains, such as autonomous robotic navigation, are better characterized using continuous representations. Such problem domains also require continuous performance, such as online sensorimotor interaction with the environment, and continuous adaptation and learning during the performance task. This article introduces a new method for continuous case-based reasoning, and discusses its application to the dynamic selection, modification, and acquisition of robot behaviors in an autonomous navigation system, SINS (Self-Improving Navigation System). The computer program and the underlying method are systematically evaluated through statistical analysis of results from several empirical studies. The article concludes with a general discussion of case-based reasoning issues addressed by this research.",Case_Based,1103031,"Title: Confidence as Higher Order Uncertainty proposed for handling higher order uncertainty, including the Bayesian approach,",Abstract:,Probabilistic_Methods,0,"Based on the provided information, Paper 1 discusses continuous case-based reasoning in problem domains that require continuous representations, specifically focusing on autonomous robotic navigation. There's no explicit mention or citation of Paper 2 within the abstract or content of Paper 1.

Therefore, based on the information provided, Paper 1 does not cite Paper 2.","Divergent Research Domains: When two papers explore distinct research domains, one focusing on continuous case-based reasoning and the other on handling higher order uncertainty using probabilistic methods, the lack of thematic alignment suggests no direct citation relationship."
101662,Title: Learning from positive data,"Abstract: Gold showed in 1967 that not even regular grammars can be exactly identified from positive examples alone. Since it is known that children learn natural grammars almost exclusively from positives examples, Gold's result has been used as a theoretical support for Chomsky's theory of innate human linguistic abilities. In this paper new results are presented which show that within a Bayesian framework not only grammars, but also logic programs are learnable with arbitrarily low expected error from positive examples only. In addition, we show that the upper bound for expected error of a learner which maximises the Bayes' posterior probability when learning from positive examples is within a small additive term of one which does the same from a mixture of positive and negative examples. An Inductive Logic Programming implementation is described which avoids the pitfalls of greedy search by global optimisation of this function during the local construction of individual clauses of the hypothesis. Results of testing this implementation on artificially-generated data-sets are reported. These results are in agreement with the theoretical predictions.",Theory,97645,Title: Genetic Algorithms for Combinatorial Optimization: The Assembly Line Balancing Problem,"Abstract: Genetic algorithms are one example of the use of a random element within an algorithm for combinatorial optimization. We consider the application of the genetic algorithm to a particular problem, the Assembly Line Balancing Problem. A general description of genetic algorithms is given, and their specialized use on our test-bed problems is discussed. We carry out extensive computational testing to find appropriate values for the various parameters associated with this genetic algorithm. These experiments underscore the importance of the correct choice of a scaling parameter and mutation rate to ensure the good performance of a genetic algorithm. We also describe a parallel implementation of the genetic algorithm and give some comparisons between the parallel and serial implementations. Both versions of the algorithm are shown to be effective in producing good solutions for problems of this type (with appropriately chosen parameters).",Genetic_Algorithms,0,"Based on the provided information, there's no explicit mention or citation of Paper 2 within the abstract or content of Paper 1.

Therefore, based on the information provided, Paper 1 does not cite Paper 2.","Disparate Research Focus: When papers belong to distinctly different research domains, like theoretical learning from positive examples and application of genetic algorithms to combinatorial optimization problems, the lack of thematic overlap might indicate no direct citation relationship."
387795,Title: University of Nevada Reno Design Strategies for Evolutionary Robotics,"Abstract: CuPit-2 is a special-purpose programming language designed for expressing dynamic neural network learning algorithms. It provides most of the flexibility of general-purpose languages such as C or C ++ , but is more expressive. It allows writing much clearer and more elegant programs, in particular for algorithms that change the network topology dynamically (constructive algorithms, pruning algorithms). In contrast to other languages, CuPit-2 programs can be compiled into efficient code for parallel machines without any changes in the source program, thus providing an easy start for using parallel platforms. This article analyzes the circumstances under which the CuPit-2 approach is the most useful one, presents a description of most language constructs and reports performance results for CuPit-2 on symmetric multiprocessors (SMPs). It concludes that in many cases CuPit-2 is a good basis for neural learning algorithm research on small-scale parallel machines.",Genetic_Algorithms,1125469,Title: Structured Representation of Complex Stochastic Systems,"Abstract: This paper considers the problem of representing complex systems that evolve stochastically over time. Dynamic Bayesian networks provide a compact representation for stochastic processes. Unfortunately, they are often unwieldy since they cannot explicitly model the complex organizational structure of many real life systems: the fact that processes are typically composed of several interacting subprocesses, each of which can, in turn, be further decomposed. We propose a hierarchically structured representation language which extends both dynamic Bayesian networks and the object-oriented Bayesian network framework of [9], and show that our language allows us to describe such systems in a natural and modular way. Our language supports a natural representation for certain system characteristics that are hard to capture using more traditional frameworks. For example, it allows us to represent systems where some processes evolve at a different rate than others, or systems where the processes interact only intermittently. We provide a simple inference mechanism for our representation via translation to Bayesian networks, and suggest ways in which the inference algorithm can exploit the additional structure encoded in our representation.",Probabilistic_Methods,0,"Based on the information provided, there doesn't seem to be any direct indication or explicit citation of Paper 2 within the abstract or content of Paper 1.

Therefore, based on the information provided, Paper 1 does not appear to cite Paper 2.",Divergent Focus Areas: When papers belong to distinctly different research domainslike evolutionary robotics design strategies and structured representation of stochastic systemslacking a thematic intersection can indicate no direct citation relationship.
6767,Title: Generalizing from Case Studies: A Case Study,"Abstract: Most empirical evaluations of machine learning algorithms are case studies evaluations of multiple algorithms on multiple databases. Authors of case studies implicitly or explicitly hypothesize that the pattern of their results, which often suggests that one algorithm performs significantly better than others, is not limited to the small number of databases investigated, but instead holds for some general class of learning problems. However, these hypotheses are rarely supported with additional evidence, which leaves them suspect. This paper describes an empirical method for generalizing results from case studies and an example application. This method yields rules describing when some algorithms significantly outperform others on some dependent measures. Advantages for generalizing from case studies and limitations of this particular approach are also described.",Case_Based,20178,Title: Discovery of Physical Principles from Design Experiences,"Abstract: One method for making analogies is to access and instantiate abstract domain principles, and one method for acquiring knowledge of abstract principles is to discover them from experience. We view generalization over experiences in the absence of any prior knowledge of the target principle as the task of hypothesis formation, a subtask of discovery. Also, we view the use of the hypothesized principles for analogical design as the task of hypothesis testing, another subtask of discovery. In this paper, we focus on discovery of physical principles by generalization over design experiences in the domain of physical devices. Some important issues in generalization from experiences are what to generalize from an experience, how far to generalize, and what methods to use. We represent a reasoner's comprehension of specific designs in the form of structure-behavior-function (SBF) models. An SBF model provides a functional and causal explanation of the working of a device. We represent domain principles as device-independent behavior-function (BF) models. We show that (i) the function of a device determines what to generalize from its SBF model, (ii) the SBF model itself suggests how far to generalize, and (iii) the typology of functions indicates what method to use.",Case_Based,0,"From the provided information, there's no explicit indication that Paper 1 cites Paper 2 within the abstract or content of Paper 1. Therefore, based on the information provided, it doesn't seem like Paper 1 cites Paper 2.","Divergent Methodologies: When papers explore distinct methodologies or frameworksfor instance, generalizing from case studies versus discovering principles from design experienceslack of citation could indicate a divergence in research approaches, potentially leading to no direct citation relationship."
