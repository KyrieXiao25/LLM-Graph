{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no path information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: 4 Implementing Application Specific Routines  Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.\n",
      "abstract:Abstract: To implement a specific application, you should only have to change the file app.c. Section 2 describes the routines in app.c in detail. If you use additional variables for your specific problem, the easiest method of making them available to other program units is to declare them in sga.h and external.h. However, take care that you do not redeclare existing variables. Two example applications files are included in the SGA-C distribution. The file app1.c performs the simple example problem included with the Pascal version; finding the maximum of x 10 , where x is an integer interpretation of a chromosome. A slightly more complex application is include in app2.c. This application illustrates two features that have been added to SGA-C. The first of these is the ithruj2int function, which converts bits i through j in a chromosome to an integer. The second new feature is the utility pointer that is associated with each population member. The example application interprets each chromosome as a set of concatenated integers in binary form. The lengths of these integer fields is determined by the user-specified value of field size, which is read in by the function app data(). The field size must be less than the smallest of the chromosome length and the length of an unsigned integer. An integer array for storing the interpreted form of each chromosome is dynamically allocated and assigned to the chromosome's utility pointer in app malloc(). The ithruj2int routine (see utility.c) is used to translate each chromosome into its associated vector. The fitness for each chromosome is simply the sum of the squares of these integers. This example application will function for any chromosome length. SGA-C is intended to be a simple program for first-time GA experimentation. It is not intended to be definitive in terms of its efficiency or the grace of its implementation. The authors are interested in the comments, criticisms, and bug reports from SGA-C users, so that the code can be refined for easier use in subsequent versions. Please email your comments to rob@galab2.mh.ua.edu, or write to TCGA: The authors gratefully acknowledge support provided by NASA under Grant NGT-50224 and support provided by the National Science Foundation under Grant CTS-8451610. We also thank Hillol Kargupta for donating his tournament selection implementation. Booker, L. B. (1982). Intelligent behavior as an adaptation to the task environment (Doctoral dissertation, Technical Report No. 243. Ann Arbor: University of Michigan, Logic of Computers Group). Dissertations Abstracts International, 43(2), 469B. (University Microfilms No. 8214966)\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Automatic Design of Cellular Neural Networks by means of Genetic Algorithms: Finding a Feature Detector\n",
      "abstract:Abstract: This paper aims to examine the use of genetic algorithms to optimize subsystems of cellular neural network architectures. The application at hand is character recognition: the aim is to evolve an optimal feature detector in order to aid a conventional classifier network to generalize across different fonts. To this end, a performance function and a genetic encoding for a feature detector are presented. An experiment is described where an optimal feature detector is indeed found by the genetic algorithm. We are interested in the application of cellular neural networks in computer vision. Genetic algorithms (GA's) [1-3] can serve to optimize the design of cellular neural networks. Although the design of the global architecture of the system could still be done by human insight, we propose that specific sub-modules of the system are best optimized using one or other optimization method. GAs are a good candidate to fulfill this optimization role, as they are well suited to problems where the objective function is a complex function of many parameters. The specific problem we want to investigate is one of character recognition. More specifically, we would like to use the GA to find optimal feature detectors to be used in the recognition of digits .\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "1\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Learning and evolution in neural networks\n",
      "abstract:Abstract: DIMACS Technical Report 96-56 December 1996\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Growing neural networks\n",
      "abstract:Abstract: Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "2\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: DE-NOISING BY reconstruction f n is defined in the wavelet domain by translating all the\n",
      "abstract:Abstract: p n. We prove two results about that estimator. [Smooth]: With high probability ^ f fl n is at least as smooth as f , in any of a wide variety of smoothness measures. [Adapt]: The estimator comes nearly as close in mean square to f as any measurable estimator can come, uniformly over balls in each of two broad scales of smoothness classes. These two properties are unprecedented in several ways. Our proof of these results develops new facts about abstract statistical inference and its connection with Acknowledgements. These results were described at the Symposium on Wavelet Theory, held in connection with the Shanks Lectures at Van-derbilt University, April 3-4 1992. The author would like to thank Professor L.L. Schumaker for hospitality at the conference, and R.A. DeVore, Iain Johnstone, Gerard Kerkyacharian, Bradley Lucier, A.S. Nemirovskii, Ingram Olkin, and Dominique Picard for interesting discussions and correspondence on related topics. The author is also at the University of California, Berkeley\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Wavelet Shrinkage: Asymptopia?\n",
      "abstract:Abstract: Considerable effort has been directed recently to develop asymptotically minimax methods in problems of recovering infinite-dimensional objects (curves, densities, spectral densities, images) from noisy data. A rich and complex body of work has evolved, with nearly- or exactly- minimax estimators being obtained for a variety of interesting problems. Unfortunately, the results have often not been translated into practice, for a variety of reasons sometimes, similarity to known methods, sometimes, computational intractability, and sometimes, lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data; one translates the empirical wavelet coefficients towards the origin by an amount method is different from methods in common use today, is computationally practical, and is spatially adaptive; thus it avoids a number of previous objections to minimax estimators. At the same time, the method is nearly minimax for a wide variety of loss functions - e.g. pointwise error, global error measured in L p norms, pointwise and global error in estimation of derivatives and for a wide range of smoothness classes, including standard Holder classes, Sobolev classes, and Bounded Variation. This is a much broader near-optimality than anything previously proposed in the minimax literature. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity. Acknowledgements: These results have been described at the Oberwolfach meeting `Mathematische Stochastik' December, 1992 and at the AMS Annual meeting, January 1993. This work was supported by NSF DMS 92-09130. The authors would like to thank Paul-Louis Hennequin, who organized the Ecole d' Ete de Probabilites at Saint Flour 1990, where this collaboration began, and to Universite de Paris VII (Jussieu) and Universite de Paris-sud (Orsay) for supporting visits of DLD and IMJ. The authors would like to thank Ildar Ibragimov and Arkady Nemirovskii for personal correspondence cited below. p\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "3\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques\n",
      "abstract:Abstract: This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Soft Computing: the Convergence of Emerging Reasoning Technologies\n",
      "abstract:Abstract: The term Soft Computing (SC) represents the combination of emerging problem-solving technologies such as Fuzzy Logic (FL), Probabilistic Reasoning (PR), Neural Networks (NNs), and Genetic Algorithms (GAs). Each of these technologies provide us with complementary reasoning and searching methods to solve complex, real-world problems. After a brief description of each of these technologies, we will analyze some of their most useful combinations, such as the use of FL to control GAs and NNs parameters; the application of GAs to evolve NNs (topologies or weights) or to tune FL controllers; and the implementation of FL controllers as NNs tuned by backpropagation-type algorithms.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "4\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Cholinergic suppression of transmission may allow combined associative memory function and self-organization in the neocortex.\n",
      "abstract:Abstract: Selective suppression of transmission at feedback synapses during learning is proposed as a mechanism for combining associative feedback with self-organization of feedforward synapses. Experimental data demonstrates cholinergic suppression of synaptic transmission in layer I (feedback synapses), and a lack of suppression in layer IV (feed-forward synapses). A network with this feature uses local rules to learn mappings which are not linearly separable. During learning, sensory stimuli and desired response are simultaneously presented as input. Feedforward connections form self-organized representations of input, while suppressed feedback connections learn the transpose of feedfor-ward connectivity. During recall, suppression is removed, sensory input activates the self-organized representation, and activity generates the learned response.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: A Scalable Performance Prediction Method for Parallel Neural Network Simulations\n",
      "abstract:Abstract: A performance prediction method is presented for indicating the performance range of MIMD parallel processor systems for neural network simulations. The total execution time of a parallel application is modeled as the sum of its calculation and communication times. The method is scalable because based on the times measured on one processor and one communication link, the performance, speedup, and efficiency can be predicted for a larger processor system. It is validated quantitatively by applying it to two popular neural networks, backpropagation and the Kohonen self-organizing feature map, decomposed on a GCel-512, a 512 transputer system. Agreement of the model with the measurements is within 9%.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "5\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Using Sampling and Queries to Extract Rules from Trained Neural Networks\n",
      "abstract:Abstract: Concepts learned by neural networks are difficult to understand because they are represented using large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract symbolic rules that describe their classification behavior. There are several existing rule-extraction approaches that operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a learning problem. In addition to learning from training examples, our method exploits the property that networks can be efficiently queried. We describe algorithms for extracting both conjunctive and M -of-N rules, and present experiments that show that our method is more efficient than conventional search-based approaches.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: The Effective Size of a Neural Network: A Principal Component Approach\n",
      "abstract:Abstract: Often when learning from data, one attaches a penalty term to a standard error term in an attempt to prefer simple models and prevent overfitting. Current penalty terms for neural networks, however, often do not take into account weight interaction. This is a critical drawback since the effective number of parameters in a network usually differs dramatically from the total number of possible parameters. In this paper, we present a penalty term that uses Principal Component Analysis to help detect functional redundancy in a neural network. Results show that our new algorithm gives a much more accurate estimate of network complexity than do standard approaches. As a result, our new term should be able to improve techniques that make use of a penalty term, such as weight decay, weight pruning, feature selection, Bayesian, and prediction-risk tech niques.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "6\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Factorial Hidden Markov Models\n",
      "abstract:Abstract: One of the basic probabilistic tools used for time series modeling is the hidden Markov model (HMM). In an HMM, information about the past of the time series is conveyed through a single discrete variable|the hidden state. We present a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. Both inference and learning in this model depend critically on computing the posterior probabilities of the hidden state variables given the observations. We present an exact algorithm for inference in this model, and relate it to the Forward-Backward algorithm for HMMs and to algorithms for more general belief networks. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or mean field theory. We also present a structured approximation in which the the state variables are decoupled, based on which we derive a tractable learning algorithm. Empirical comparisons suggest that these approximations are efficient and accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that it outperforms HMMs in capturing the complex temporal patterns in this dataset.\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Tractable Inference for Complex Stochastic Processes\n",
      "abstract:Abstract: The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief statea probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs), where the process itself can be represented compactly, the representation of the belief state is intractable. We investigate the idea of maintaining a compact approximation to the true belief state, and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant. We show that the error in a belief state contracts exponentially as the process evolves. Thus, even with multiple approximations, the error in our process remains bounded indefinitely. We show how the additional structure of a DBN can be used to design our approximation scheme, improving its performance significantly. We demonstrate the applicability of our ideas in the context of a monitoring task, showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy.\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "7\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: A Self-Adjusting Dynamic Logic Module\n",
      "abstract:Abstract: This paper presents an ASOCS (Adaptive Self-Organizing Concurrent System) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic, robotics, logical inference, and dynamic control. An ASOCS is an adaptive network composed of many simple computing elements operating asynchronously and in parallel. This paper focuses on Adaptive Algorithm 2 (AA2) and details its architecture and learning algorithm. AA2 has significant memory and knowledge maintenance advantages over previous ASOCS models. An ASOCS can operate in either a data processing mode or a learning mode. During learning mode, the ASOCS is given a new rule expressed as a boolean conjunction. The AA2 learning algorithm incorporates the new rule in a distributed fashion in a short, bounded time. During data processing mode, the ASOCS acts as a parallel hardware circuit.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: A Multi-Chip Module Implementation of a Neural Network\n",
      "abstract:Abstract: The requirement for dense interconnect in artificial neural network systems has led researchers to seek high-density interconnect technologies. This paper reports an implementation using multi-chip modules (MCMs) as the interconnect medium. The specific system described is a self-organizing, parallel, and dynamic learning model which requires a dense interconnect technology for effective implementation; this requirement is fulfilled by exploiting MCM technology. The ideas presented in this paper regarding an MCM implementation of artificial neural networks are versatile and can be adapted to apply to other neural network and connectionist models.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "8\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Analyzing GAs Using Markov Models with Semantically Ordered and Lumped States\n",
      "abstract:Abstract: At the previous FOGA workshop, we presented some initial results on using Markov models to analyze the transient behavior of genetic algorithms (GAs) being used as function optimizers (GAFOs). In that paper, the states of the Markov model were ordered via a simple and mathematically convenient lexicographic ordering used initially by Nix and Vose. In this paper, we explore alternative orderings of states based on interesting semantic properties such as average fitness, degree of homogeneity, average attractive force, etc. We also explore lumping techniques for reducing the size of the state space. Analysis of these reordered and lumped Markov models provides new insights into the transient behavior of GAs in general and GAFOs in particular.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: A COMPRESSION ALGORITHM FOR PROBABILITY TRANSITION MATRICES\n",
      "abstract:Abstract: This paper describes a compression algorithm for probability transition matrices. The compressed matrix is itself a probability transition matrix. In general the compression is not error-free, but the error appears to be small even for high levels of compression.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "9\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques\n",
      "abstract:Abstract: This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Empirical studies of the genetic algorithm with non-coding segments\n",
      "abstract:Abstract: The genetic algorithm (GA) is a problem solving method that is modelled after the process of natural selection. We are interested in studying a specific aspect of the GA: the effect of non-coding segments on GA performance. Non-coding segments are segments of bits in an individual that provide no contribution, positive or negative, to the fitness of that individual. Previous research on non-coding segments suggests that including these structures in the GA may improve GA performance. Understanding when and why this improvement occurs will help us to use the GA to its full potential. In this article, we discuss our hypotheses on non-coding segments and describe the results of our experiments. The experiments may be separated into two categories: testing our program on problems from previous related studies, and testing new hypotheses on the effect of non-coding segments.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "10\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Word Perfect Corp. LIA: A Location-Independent Transformation for ASOCS Adaptive Algorithm 2\n",
      "abstract:Abstract: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and often suffer from a number of shortcomings as a result. ANNs that use dynamic topologies have shown ability to overcome many of these problems. Adaptive Self Organizing Concurrent Systems (ASOCS) are a class of learning models with inherently dynamic topologies. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing learning models that use dynamic topologies efficiently in parallel hardware. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. In particular, this paper presents the Location - Independent ASOCS (LIA) model as a LIT for ASOCS Adaptive Algorithm 2. The description of LIA gives formal definitions for LIA algorithms. Because LIA implements basic ASOCS mechanisms, these definitions provide a formal description of basic ASOCS mechanisms in general, in addition to LIA.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes\n",
      "abstract:Abstract: A General Result on the Stabilization of Linear Systems Using Bounded Controls 1 ABSTRACT We present two constructions of controllers that globally stabilize linear systems subject to control saturation. We allow essentially arbitrary saturation functions. The only conditions imposed on the system are the obvious necessary ones, namely that no eigenvalues of the uncontrolled system have positive real part and that the standard stabilizability rank condition hold. One of the constructions is in terms of a \"neural-network type\" one-hidden layer architecture, while the other one is in terms of cascades of linear maps and saturations.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "11\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Adaptive tuning of numerical weather prediction models: Randomized GCV in three and four dimensional data assimilation\n",
      "abstract:Abstract: This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Constructive Learning of Recurrent Neural Networks: Limitations of Recurrent Casade Correlation and a Simple Solution\n",
      "abstract:Abstract: It is often difficult to predict the optimal neural network size for a particular application. Constructive or destructive methods that add or subtract neurons, layers, connections, etc. might offer a solution to this problem. We prove that one method, Recurrent Cascade Correlation, due to its topology, has fundamental limitations in representation and thus in its learning capabilities. It cannot represent with monotone (i.e. sigmoid) and hard-threshold activation functions certain finite state automata. We give a \"preliminary\" approach on how to get around these limitations by devising a simple constructive training method that adds neurons during training while still preserving the powerful fully-recurrent structure. We illustrate this approach by simulations which learn many examples of regular grammars that the\n",
      "category:Neural_Networks\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "12\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Adapting Crossover in a Genetic Algorithm\n",
      "abstract:Abstract: Traditionally, genetic algorithms have relied upon 1 and 2-point crossover operators. Many recent empirical studies, however, have shown the benefits of higher numbers of crossover points. Some of the most intriguing recent work has focused on uniform crossover, which involves on the average L/2 crossover points for strings of length L. Despite theoretical analysis, however, it appears difficult to predict when a particular crossover form will be optimal for a given problem. This paper describes an adaptive genetic algorithm that decides, as it runs, which form is optimal.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Confidence as Higher Order Uncertainty proposed for handling higher order uncertainty, including the Bayesian approach,\n",
      "abstract:Abstract:\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "13\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Lookahead and Pathology in Decision Tree Induction\n",
      "abstract:Abstract: The standard approach to decision tree induction is a top-down, greedy algorithm that makes locally optimal, irrevocable decisions at each node of a tree. In this paper, we empirically study an alternative approach, in which the algorithms use one-level lookahead to decide what test to use at a node. We systematically compare, using a very large number of real and artificial data sets, the quality of decision trees induced by the greedy approach to that of trees induced using lookahead. The main observations from our experiments are: (i) the greedy approach consistently produced trees that were just as accurate as trees produced with the much more expensive lookahead step; and (ii) we observed many instances of pathology, i.e., lookahead produced trees that were both larger and less accurate than trees produced without it.\n",
      "category:Theory\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Classifying Seismic Signals by Integrating Ensembles of Neural Networks\n",
      "abstract:Abstract: This paper proposes a classification scheme based on integration of multiple Ensembles of ANNs. It is demonstrated on a classification problem, in which seismic signals of Natural Earthquakes must be distinguished from seismic signals of Artificial Explosions. A Redundant Classification Environment consists of several Ensembles of Neural Networks is created and trained on Bootstrap Sample Sets, using various data representations and architectures. The ANNs within the Ensembles are aggregated (as in Bagging) while the Ensembles are integrated non-linearly, in a signal adaptive manner, using a posterior confidence measure based on the agreement (variance) within the Ensembles. The proposed Integrated Classification Machine achieved 92.1% correct classifications on the seismic test data. Cross Validation evaluations and comparisons indicate that such integration of a collection of ANN's Ensembles is a robust way for handling high dimensional problems with a complex non-stationary signal space as in the current Seismic Classification problem.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "14\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Collective Memory Search 1 Collective Memory Search: Exploiting an Information Center for Exploration\n",
      "abstract:Abstract: The results reported here empirically show the benefit of decision tree size biases as a function of concept distribution. First, it is shown how concept distribution complexity (the number of internal nodes in the smallest decision tree consistent with the example space) affects the benefit of minimum size and maximum size decision tree biases. Second, a policy is described that defines what a learner should do given knowledge of the complexity of the distribution of concepts. Third, explanations for why the distribution of concepts seen in practice is amenable to the minimum size decision tree bias are given and evaluated empirically.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: The Possible Contribution of AI to the Avoidance of Crises and Wars: Using CBR Methods\n",
      "abstract:Abstract: This paper presents the application of Case-Based Reasoning methods to the KOSIMO data base of international conflicts. A Case-Based Reasoning tool - VIE-CBR has been deveolped and used for the classification of various outcome variables, like political, military, and territorial outcome, solution modalities, and conflict intensity. In addition, the case retrieval algorithms are presented as an interactive, user-modifiable tool for intelli gently searching the conflict data base for precedent cases.\n",
      "category:Case_Based\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "15\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Learning physical descriptions from functional definitions, examples, Learning from examples: The effect of different conceptual\n",
      "abstract:Abstract: Technical Report 94-07 February 7, 1994 (updated April 25, 1994) This paper will appear in Proceedings of the Eleventh International Conference on Machine Learning. Abstract This paper presents an algorithm for incremental induction of decision trees that is able to handle both numeric and symbolic variables. In order to handle numeric variables, a new tree revision operator called `slewing' is introduced. Finally, a non-incremental method is given for finding a decision tree based on a direct metric of a candidate tree.\n",
      "category:Case_Based\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: References Linear Controller Design, Limits of Performance, \"The parallel projection operators of a nonlinear feedback\n",
      "abstract:Abstract: 13] Yang, Y., H.J. Sussmann, and E.D. Sontag, \"Stabilization of linear systems with bounded controls,\" in Proc. Nonlinear Control Systems Design Symp., Bordeaux, June 1992 (M. Fliess, Ed.), IFAC Publications, pp. 15-20. Journal version to appear in IEEE Trans. Autom. Control .\n",
      "category:Neural_Networks\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "16\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Continuous Case-Based Reasoning\n",
      "abstract:Abstract: Case-based reasoning systems have traditionally been used to perform high-level reasoning in problem domains that can be adequately described using discrete, symbolic representations. However, many real-world problem domains, such as autonomous robotic navigation, are better characterized using continuous representations. Such problem domains also require continuous performance, such as online sensorimotor interaction with the environment, and continuous adaptation and learning during the performance task. This article introduces a new method for continuous case-based reasoning, and discusses its application to the dynamic selection, modification, and acquisition of robot behaviors in an autonomous navigation system, SINS (Self-Improving Navigation System). The computer program and the underlying method are systematically evaluated through statistical analysis of results from several empirical studies. The article concludes with a general discussion of case-based reasoning issues addressed by this research.\n",
      "category:Case_Based\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Confidence as Higher Order Uncertainty proposed for handling higher order uncertainty, including the Bayesian approach,\n",
      "abstract:Abstract:\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "17\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Learning from positive data\n",
      "abstract:Abstract: Gold showed in 1967 that not even regular grammars can be exactly identified from positive examples alone. Since it is known that children learn natural grammars almost exclusively from positives examples, Gold's result has been used as a theoretical support for Chomsky's theory of innate human linguistic abilities. In this paper new results are presented which show that within a Bayesian framework not only grammars, but also logic programs are learnable with arbitrarily low expected error from positive examples only. In addition, we show that the upper bound for expected error of a learner which maximises the Bayes' posterior probability when learning from positive examples is within a small additive term of one which does the same from a mixture of positive and negative examples. An Inductive Logic Programming implementation is described which avoids the pitfalls of greedy search by global optimisation of this function during the local construction of individual clauses of the hypothesis. Results of testing this implementation on artificially-generated data-sets are reported. These results are in agreement with the theoretical predictions.\n",
      "category:Theory\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Genetic Algorithms for Combinatorial Optimization: The Assembly Line Balancing Problem\n",
      "abstract:Abstract: Genetic algorithms are one example of the use of a random element within an algorithm for combinatorial optimization. We consider the application of the genetic algorithm to a particular problem, the Assembly Line Balancing Problem. A general description of genetic algorithms is given, and their specialized use on our test-bed problems is discussed. We carry out extensive computational testing to find appropriate values for the various parameters associated with this genetic algorithm. These experiments underscore the importance of the correct choice of a scaling parameter and mutation rate to ensure the good performance of a genetic algorithm. We also describe a parallel implementation of the genetic algorithm and give some comparisons between the parallel and serial implementations. Both versions of the algorithm are shown to be effective in producing good solutions for problems of this type (with appropriately chosen parameters).\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "18\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: University of Nevada Reno Design Strategies for Evolutionary Robotics\n",
      "abstract:Abstract: CuPit-2 is a special-purpose programming language designed for expressing dynamic neural network learning algorithms. It provides most of the flexibility of general-purpose languages such as C or C ++ , but is more expressive. It allows writing much clearer and more elegant programs, in particular for algorithms that change the network topology dynamically (constructive algorithms, pruning algorithms). In contrast to other languages, CuPit-2 programs can be compiled into efficient code for parallel machines without any changes in the source program, thus providing an easy start for using parallel platforms. This article analyzes the circumstances under which the CuPit-2 approach is the most useful one, presents a description of most language constructs and reports performance results for CuPit-2 on symmetric multiprocessors (SMPs). It concludes that in many cases CuPit-2 is a good basis for neural learning algorithm research on small-scale parallel machines.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Structured Representation of Complex Stochastic Systems\n",
      "abstract:Abstract: This paper considers the problem of representing complex systems that evolve stochastically over time. Dynamic Bayesian networks provide a compact representation for stochastic processes. Unfortunately, they are often unwieldy since they cannot explicitly model the complex organizational structure of many real life systems: the fact that processes are typically composed of several interacting subprocesses, each of which can, in turn, be further decomposed. We propose a hierarchically structured representation language which extends both dynamic Bayesian networks and the object-oriented Bayesian network framework of [9], and show that our language allows us to describe such systems in a natural and modular way. Our language supports a natural representation for certain system characteristics that are hard to capture using more traditional frameworks. For example, it allows us to represent systems where some processes evolve at a different rate than others, or systems where the processes interact only intermittently. We provide a simple inference mechanism for our representation via translation to Bayesian networks, and suggest ways in which the inference algorithm can exploit the additional structure encoded in our representation.\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n",
      "19\n",
      "\n",
      "Given information of two paper shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Generalizing from Case Studies: A Case Study\n",
      "abstract:Abstract: Most empirical evaluations of machine learning algorithms are case studies evaluations of multiple algorithms on multiple databases. Authors of case studies implicitly or explicitly hypothesize that the pattern of their results, which often suggests that one algorithm performs significantly better than others, is not limited to the small number of databases investigated, but instead holds for some general class of learning problems. However, these hypotheses are rarely supported with additional evidence, which leaves them suspect. This paper describes an empirical method for generalizing results from case studies and an example application. This method yields rules describing when some algorithms significantly outperform others on some dependent measures. Advantages for generalizing from case studies and limitations of this particular approach are also described.\n",
      "category:Case_Based\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Discovery of Physical Principles from Design Experiences\n",
      "abstract:Abstract: One method for making analogies is to access and instantiate abstract domain principles, and one method for acquiring knowledge of abstract principles is to discover them from experience. We view generalization over experiences in the absence of any prior knowledge of the target principle as the task of hypothesis formation, a subtask of discovery. Also, we view the use of the hypothesized principles for analogical design as the task of hypothesis testing, another subtask of discovery. In this paper, we focus on discovery of physical principles by generalization over design experiences in the domain of physical devices. Some important issues in generalization from experiences are what to generalize from an experience, how far to generalize, and what methods to use. We represent a reasoner's comprehension of specific designs in the form of structure-behavior-function (SBF) models. An SBF model provides a functional and causal explanation of the working of a device. We represent domain principles as device-independent behavior-function (BF) models. We show that (i) the function of a device determines what to generalize from its SBF model, (ii) the SBF model itself suggests how far to generalize, and (iii) the typology of functions indicates what method to use.\n",
      "category:Case_Based\n",
      "\\\n",
      "Question: Does paper1 cite paper2. Let's think step by step.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# auto_cot\n",
    "sample = pd.read_csv('sample/no/sample_one.csv')\n",
    "\n",
    "for i in range(len(sample)):\n",
    "    cite_title, cite_abs, cite_label, cited_title, cited_abs, cited_label = sample.loc[i, 'cite_title'], sample.loc[i, 'cite_abs'], sample.loc[i, 'cite_label'], sample.loc[i, 'cited_title'], sample.loc[i, 'cited_abs'], sample.loc[i, 'cited_label']\n",
    "    prompt = f\"\"\"\n",
    "Given information of two paper shown as follow:\n",
    "\\\\\n",
    "paper1 informations:\n",
    "title:{cite_title}\n",
    "abstract:{cite_abs}\n",
    "category:{cite_label}\n",
    "\\\\\n",
    "\\\\\n",
    "paper2 informations: \n",
    "title:{cited_title}\n",
    "abstract:{cited_abs}\n",
    "category:{cited_label}\n",
    "\\\\\n",
    "Question: Does paper1 cite paper2. Let's think step by step.\n",
    "    \"\"\"\n",
    "    print(i)\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: 4 Implementing Application Specific Routines  Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.\n",
      "abstract:Abstract: To implement a specific application, you should only have to change the file app.c. Section 2 describes the routines in app.c in detail. If you use additional variables for your specific problem, the easiest method of making them available to other program units is to declare them in sga.h and external.h. However, take care that you do not redeclare existing variables. Two example applications files are included in the SGA-C distribution. The file app1.c performs the simple example problem included with the Pascal version; finding the maximum of x 10 , where x is an integer interpretation of a chromosome. A slightly more complex application is include in app2.c. This application illustrates two features that have been added to SGA-C. The first of these is the ithruj2int function, which converts bits i through j in a chromosome to an integer. The second new feature is the utility pointer that is associated with each population member. The example application interprets each chromosome as a set of concatenated integers in binary form. The lengths of these integer fields is determined by the user-specified value of field size, which is read in by the function app data(). The field size must be less than the smallest of the chromosome length and the length of an unsigned integer. An integer array for storing the interpreted form of each chromosome is dynamically allocated and assigned to the chromosome's utility pointer in app malloc(). The ithruj2int routine (see utility.c) is used to translate each chromosome into its associated vector. The fitness for each chromosome is simply the sum of the squares of these integers. This example application will function for any chromosome length. SGA-C is intended to be a simple program for first-time GA experimentation. It is not intended to be definitive in terms of its efficiency or the grace of its implementation. The authors are interested in the comments, criticisms, and bug reports from SGA-C users, so that the code can be refined for easier use in subsequent versions. Please email your comments to rob@galab2.mh.ua.edu, or write to TCGA: The authors gratefully acknowledge support provided by NASA under Grant NGT-50224 and support provided by the National Science Foundation under Grant CTS-8451610. We also thank Hillol Kargupta for donating his tournament selection implementation. Booker, L. B. (1982). Intelligent behavior as an adaptation to the task environment (Doctoral dissertation, Technical Report No. 243. Ann Arbor: University of Michigan, Logic of Computers Group). Dissertations Abstracts International, 43(2), 469B. (University Microfilms No. 8214966)\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Automatic Design of Cellular Neural Networks by means of Genetic Algorithms: Finding a Feature Detector\n",
      "abstract:Abstract: This paper aims to examine the use of genetic algorithms to optimize subsystems of cellular neural network architectures. The application at hand is character recognition: the aim is to evolve an optimal feature detector in order to aid a conventional classifier network to generalize across different fonts. To this end, a performance function and a genetic encoding for a feature detector are presented. An experiment is described where an optimal feature detector is indeed found by the genetic algorithm. We are interested in the application of cellular neural networks in computer vision. Genetic algorithms (GA's) [1-3] can serve to optimize the design of cellular neural networks. Although the design of the global architecture of the system could still be done by human insight, we propose that specific sub-modules of the system are best optimized using one or other optimization method. GAs are a good candidate to fulfill this optimization role, as they are well suited to problems where the objective function is a complex function of many parameters. The specific problem we want to investigate is one of character recognition. More specifically, we would like to use the GA to find optimal feature detectors to be used in the recognition of digits .\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\n",
      "Question: Title: 4 Implementing Application Specific Routines  Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley. cites Title: Automatic Design of Cellular Neural Networks by means of Genetic Algorithms: Finding a Feature Detector. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "1\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Learning and evolution in neural networks\n",
      "abstract:Abstract: DIMACS Technical Report 96-56 December 1996\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Growing neural networks\n",
      "abstract:Abstract: Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\n",
      "Question: Title: Learning and evolution in neural networks cites Title: Growing neural networks. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "2\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: DE-NOISING BY reconstruction f n is defined in the wavelet domain by translating all the\n",
      "abstract:Abstract: p n. We prove two results about that estimator. [Smooth]: With high probability ^ f fl n is at least as smooth as f , in any of a wide variety of smoothness measures. [Adapt]: The estimator comes nearly as close in mean square to f as any measurable estimator can come, uniformly over balls in each of two broad scales of smoothness classes. These two properties are unprecedented in several ways. Our proof of these results develops new facts about abstract statistical inference and its connection with Acknowledgements. These results were described at the Symposium on Wavelet Theory, held in connection with the Shanks Lectures at Van-derbilt University, April 3-4 1992. The author would like to thank Professor L.L. Schumaker for hospitality at the conference, and R.A. DeVore, Iain Johnstone, Gerard Kerkyacharian, Bradley Lucier, A.S. Nemirovskii, Ingram Olkin, and Dominique Picard for interesting discussions and correspondence on related topics. The author is also at the University of California, Berkeley\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Wavelet Shrinkage: Asymptopia?\n",
      "abstract:Abstract: Considerable effort has been directed recently to develop asymptotically minimax methods in problems of recovering infinite-dimensional objects (curves, densities, spectral densities, images) from noisy data. A rich and complex body of work has evolved, with nearly- or exactly- minimax estimators being obtained for a variety of interesting problems. Unfortunately, the results have often not been translated into practice, for a variety of reasons sometimes, similarity to known methods, sometimes, computational intractability, and sometimes, lack of spatial adaptivity. We discuss a method for curve estimation based on n noisy data; one translates the empirical wavelet coefficients towards the origin by an amount method is different from methods in common use today, is computationally practical, and is spatially adaptive; thus it avoids a number of previous objections to minimax estimators. At the same time, the method is nearly minimax for a wide variety of loss functions - e.g. pointwise error, global error measured in L p norms, pointwise and global error in estimation of derivatives and for a wide range of smoothness classes, including standard Holder classes, Sobolev classes, and Bounded Variation. This is a much broader near-optimality than anything previously proposed in the minimax literature. Finally, the theory underlying the method is interesting, as it exploits a correspondence between statistical questions and questions of optimal recovery and information-based complexity. Acknowledgements: These results have been described at the Oberwolfach meeting `Mathematische Stochastik' December, 1992 and at the AMS Annual meeting, January 1993. This work was supported by NSF DMS 92-09130. The authors would like to thank Paul-Louis Hennequin, who organized the Ecole d' Ete de Probabilites at Saint Flour 1990, where this collaboration began, and to Universite de Paris VII (Jussieu) and Universite de Paris-sud (Orsay) for supporting visits of DLD and IMJ. The authors would like to thank Ildar Ibragimov and Arkady Nemirovskii for personal correspondence cited below. p\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\n",
      "Question: Title: DE-NOISING BY reconstruction f n is defined in the wavelet domain by translating all the cites Title: Wavelet Shrinkage: Asymptopia?. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "3\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques\n",
      "abstract:Abstract: This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Soft Computing: the Convergence of Emerging Reasoning Technologies\n",
      "abstract:Abstract: The term Soft Computing (SC) represents the combination of emerging problem-solving technologies such as Fuzzy Logic (FL), Probabilistic Reasoning (PR), Neural Networks (NNs), and Genetic Algorithms (GAs). Each of these technologies provide us with complementary reasoning and searching methods to solve complex, real-world problems. After a brief description of each of these technologies, we will analyze some of their most useful combinations, such as the use of FL to control GAs and NNs parameters; the application of GAs to evolve NNs (topologies or weights) or to tune FL controllers; and the implementation of FL controllers as NNs tuned by backpropagation-type algorithms.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\n",
      "Question: Title: Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques cites Title: Soft Computing: the Convergence of Emerging Reasoning Technologies. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "4\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Cholinergic suppression of transmission may allow combined associative memory function and self-organization in the neocortex.\n",
      "abstract:Abstract: Selective suppression of transmission at feedback synapses during learning is proposed as a mechanism for combining associative feedback with self-organization of feedforward synapses. Experimental data demonstrates cholinergic suppression of synaptic transmission in layer I (feedback synapses), and a lack of suppression in layer IV (feed-forward synapses). A network with this feature uses local rules to learn mappings which are not linearly separable. During learning, sensory stimuli and desired response are simultaneously presented as input. Feedforward connections form self-organized representations of input, while suppressed feedback connections learn the transpose of feedfor-ward connectivity. During recall, suppression is removed, sensory input activates the self-organized representation, and activity generates the learned response.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: A Scalable Performance Prediction Method for Parallel Neural Network Simulations\n",
      "abstract:Abstract: A performance prediction method is presented for indicating the performance range of MIMD parallel processor systems for neural network simulations. The total execution time of a parallel application is modeled as the sum of its calculation and communication times. The method is scalable because based on the times measured on one processor and one communication link, the performance, speedup, and efficiency can be predicted for a larger processor system. It is validated quantitatively by applying it to two popular neural networks, backpropagation and the Kohonen self-organizing feature map, decomposed on a GCel-512, a 512 transputer system. Agreement of the model with the measurements is within 9%.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\n",
      "Question: Title: Cholinergic suppression of transmission may allow combined associative memory function and self-organization in the neocortex. cites Title: A Scalable Performance Prediction Method for Parallel Neural Network Simulations. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "5\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Using Sampling and Queries to Extract Rules from Trained Neural Networks\n",
      "abstract:Abstract: Concepts learned by neural networks are difficult to understand because they are represented using large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract symbolic rules that describe their classification behavior. There are several existing rule-extraction approaches that operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a learning problem. In addition to learning from training examples, our method exploits the property that networks can be efficiently queried. We describe algorithms for extracting both conjunctive and M -of-N rules, and present experiments that show that our method is more efficient than conventional search-based approaches.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: The Effective Size of a Neural Network: A Principal Component Approach\n",
      "abstract:Abstract: Often when learning from data, one attaches a penalty term to a standard error term in an attempt to prefer simple models and prevent overfitting. Current penalty terms for neural networks, however, often do not take into account weight interaction. This is a critical drawback since the effective number of parameters in a network usually differs dramatically from the total number of possible parameters. In this paper, we present a penalty term that uses Principal Component Analysis to help detect functional redundancy in a neural network. Results show that our new algorithm gives a much more accurate estimate of network complexity than do standard approaches. As a result, our new term should be able to improve techniques that make use of a penalty term, such as weight decay, weight pruning, feature selection, Bayesian, and prediction-risk tech niques.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\n",
      "Question: Title: Using Sampling and Queries to Extract Rules from Trained Neural Networks cites Title: The Effective Size of a Neural Network: A Principal Component Approach. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "6\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Factorial Hidden Markov Models\n",
      "abstract:Abstract: One of the basic probabilistic tools used for time series modeling is the hidden Markov model (HMM). In an HMM, information about the past of the time series is conveyed through a single discrete variable|the hidden state. We present a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. Both inference and learning in this model depend critically on computing the posterior probabilities of the hidden state variables given the observations. We present an exact algorithm for inference in this model, and relate it to the Forward-Backward algorithm for HMMs and to algorithms for more general belief networks. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or mean field theory. We also present a structured approximation in which the the state variables are decoupled, based on which we derive a tractable learning algorithm. Empirical comparisons suggest that these approximations are efficient and accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that it outperforms HMMs in capturing the complex temporal patterns in this dataset.\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Tractable Inference for Complex Stochastic Processes\n",
      "abstract:Abstract: The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief statea probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs), where the process itself can be represented compactly, the representation of the belief state is intractable. We investigate the idea of maintaining a compact approximation to the true belief state, and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant. We show that the error in a belief state contracts exponentially as the process evolves. Thus, even with multiple approximations, the error in our process remains bounded indefinitely. We show how the additional structure of a DBN can be used to design our approximation scheme, improving its performance significantly. We demonstrate the applicability of our ideas in the context of a monitoring task, showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy.\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\n",
      "Question: Title: Factorial Hidden Markov Models cites Title: Tractable Inference for Complex Stochastic Processes. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "7\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: A Self-Adjusting Dynamic Logic Module\n",
      "abstract:Abstract: This paper presents an ASOCS (Adaptive Self-Organizing Concurrent System) model for massively parallel processing of incrementally defined rule systems in such areas as adaptive logic, robotics, logical inference, and dynamic control. An ASOCS is an adaptive network composed of many simple computing elements operating asynchronously and in parallel. This paper focuses on Adaptive Algorithm 2 (AA2) and details its architecture and learning algorithm. AA2 has significant memory and knowledge maintenance advantages over previous ASOCS models. An ASOCS can operate in either a data processing mode or a learning mode. During learning mode, the ASOCS is given a new rule expressed as a boolean conjunction. The AA2 learning algorithm incorporates the new rule in a distributed fashion in a short, bounded time. During data processing mode, the ASOCS acts as a parallel hardware circuit.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: A Multi-Chip Module Implementation of a Neural Network\n",
      "abstract:Abstract: The requirement for dense interconnect in artificial neural network systems has led researchers to seek high-density interconnect technologies. This paper reports an implementation using multi-chip modules (MCMs) as the interconnect medium. The specific system described is a self-organizing, parallel, and dynamic learning model which requires a dense interconnect technology for effective implementation; this requirement is fulfilled by exploiting MCM technology. The ideas presented in this paper regarding an MCM implementation of artificial neural networks are versatile and can be adapted to apply to other neural network and connectionist models.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\n",
      "Question: Title: A Self-Adjusting Dynamic Logic Module cites Title: A Multi-Chip Module Implementation of a Neural Network. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "8\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Analyzing GAs Using Markov Models with Semantically Ordered and Lumped States\n",
      "abstract:Abstract: At the previous FOGA workshop, we presented some initial results on using Markov models to analyze the transient behavior of genetic algorithms (GAs) being used as function optimizers (GAFOs). In that paper, the states of the Markov model were ordered via a simple and mathematically convenient lexicographic ordering used initially by Nix and Vose. In this paper, we explore alternative orderings of states based on interesting semantic properties such as average fitness, degree of homogeneity, average attractive force, etc. We also explore lumping techniques for reducing the size of the state space. Analysis of these reordered and lumped Markov models provides new insights into the transient behavior of GAs in general and GAFOs in particular.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: A COMPRESSION ALGORITHM FOR PROBABILITY TRANSITION MATRICES\n",
      "abstract:Abstract: This paper describes a compression algorithm for probability transition matrices. The compressed matrix is itself a probability transition matrix. In general the compression is not error-free, but the error appears to be small even for high levels of compression.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\n",
      "Question: Title: Analyzing GAs Using Markov Models with Semantically Ordered and Lumped States cites Title: A COMPRESSION ALGORITHM FOR PROBABILITY TRANSITION MATRICES. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "9\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques\n",
      "abstract:Abstract: This paper proposes using fuzzy logic techniques to dynamically control parameter settings of genetic algorithms (GAs). We describe the Dynamic Parametric GA: a GA that uses a fuzzy knowledge-based system to control GA parameters. We then introduce a technique for automatically designing and tuning the fuzzy knowledge-base system using GAs. Results from initial experiments show a performance improvement over a simple static GA. One Dynamic Parametric GA system designed by our automatic method demonstrated improvement on an application not included in the design phase, which may indicate the general applicability of the Dynamic Parametric GA to a wide range of ap plications.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Empirical studies of the genetic algorithm with non-coding segments\n",
      "abstract:Abstract: The genetic algorithm (GA) is a problem solving method that is modelled after the process of natural selection. We are interested in studying a specific aspect of the GA: the effect of non-coding segments on GA performance. Non-coding segments are segments of bits in an individual that provide no contribution, positive or negative, to the fitness of that individual. Previous research on non-coding segments suggests that including these structures in the GA may improve GA performance. Understanding when and why this improvement occurs will help us to use the GA to its full potential. In this article, we discuss our hypotheses on non-coding segments and describe the results of our experiments. The experiments may be separated into two categories: testing our program on problems from previous related studies, and testing new hypotheses on the effect of non-coding segments.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\n",
      "Question: Title: Dynamic Control of Genetic Algorithms using Fuzzy Logic Techniques cites Title: Empirical studies of the genetic algorithm with non-coding segments. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "10\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Word Perfect Corp. LIA: A Location-Independent Transformation for ASOCS Adaptive Algorithm 2\n",
      "abstract:Abstract: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and often suffer from a number of shortcomings as a result. ANNs that use dynamic topologies have shown ability to overcome many of these problems. Adaptive Self Organizing Concurrent Systems (ASOCS) are a class of learning models with inherently dynamic topologies. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing learning models that use dynamic topologies efficiently in parallel hardware. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. In particular, this paper presents the Location - Independent ASOCS (LIA) model as a LIT for ASOCS Adaptive Algorithm 2. The description of LIA gives formal definitions for LIA algorithms. Because LIA implements basic ASOCS mechanisms, these definitions provide a formal description of basic ASOCS mechanisms in general, in addition to LIA.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes\n",
      "abstract:Abstract: A General Result on the Stabilization of Linear Systems Using Bounded Controls 1 ABSTRACT We present two constructions of controllers that globally stabilize linear systems subject to control saturation. We allow essentially arbitrary saturation functions. The only conditions imposed on the system are the obvious necessary ones, namely that no eigenvalues of the uncontrolled system have positive real part and that the standard stabilizability rank condition hold. One of the constructions is in terms of a \"neural-network type\" one-hidden layer architecture, while the other one is in terms of cascades of linear maps and saturations.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\n",
      "Question: Title: Word Perfect Corp. LIA: A Location-Independent Transformation for ASOCS Adaptive Algorithm 2 does not cite Title: An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "11\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Adaptive tuning of numerical weather prediction models: Randomized GCV in three and four dimensional data assimilation\n",
      "abstract:Abstract: This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Constructive Learning of Recurrent Neural Networks: Limitations of Recurrent Casade Correlation and a Simple Solution\n",
      "abstract:Abstract: It is often difficult to predict the optimal neural network size for a particular application. Constructive or destructive methods that add or subtract neurons, layers, connections, etc. might offer a solution to this problem. We prove that one method, Recurrent Cascade Correlation, due to its topology, has fundamental limitations in representation and thus in its learning capabilities. It cannot represent with monotone (i.e. sigmoid) and hard-threshold activation functions certain finite state automata. We give a \"preliminary\" approach on how to get around these limitations by devising a simple constructive training method that adds neurons during training while still preserving the powerful fully-recurrent structure. We illustrate this approach by simulations which learn many examples of regular grammars that the\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\n",
      "Question: Title: Adaptive tuning of numerical weather prediction models: Randomized GCV in three and four dimensional data assimilation does not cite Title: Constructive Learning of Recurrent Neural Networks: Limitations of Recurrent Casade Correlation and a Simple Solution. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "12\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Adapting Crossover in a Genetic Algorithm\n",
      "abstract:Abstract: Traditionally, genetic algorithms have relied upon 1 and 2-point crossover operators. Many recent empirical studies, however, have shown the benefits of higher numbers of crossover points. Some of the most intriguing recent work has focused on uniform crossover, which involves on the average L/2 crossover points for strings of length L. Despite theoretical analysis, however, it appears difficult to predict when a particular crossover form will be optimal for a given problem. This paper describes an adaptive genetic algorithm that decides, as it runs, which form is optimal.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Confidence as Higher Order Uncertainty proposed for handling higher order uncertainty, including the Bayesian approach,\n",
      "abstract:Abstract:\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\n",
      "Question: Title: Adapting Crossover in a Genetic Algorithm does not cite Title: Confidence as Higher Order Uncertainty proposed for handling higher order uncertainty, including the Bayesian approach,. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "13\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Lookahead and Pathology in Decision Tree Induction\n",
      "abstract:Abstract: The standard approach to decision tree induction is a top-down, greedy algorithm that makes locally optimal, irrevocable decisions at each node of a tree. In this paper, we empirically study an alternative approach, in which the algorithms use one-level lookahead to decide what test to use at a node. We systematically compare, using a very large number of real and artificial data sets, the quality of decision trees induced by the greedy approach to that of trees induced using lookahead. The main observations from our experiments are: (i) the greedy approach consistently produced trees that were just as accurate as trees produced with the much more expensive lookahead step; and (ii) we observed many instances of pathology, i.e., lookahead produced trees that were both larger and less accurate than trees produced without it.\n",
      "category:Theory\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Classifying Seismic Signals by Integrating Ensembles of Neural Networks\n",
      "abstract:Abstract: This paper proposes a classification scheme based on integration of multiple Ensembles of ANNs. It is demonstrated on a classification problem, in which seismic signals of Natural Earthquakes must be distinguished from seismic signals of Artificial Explosions. A Redundant Classification Environment consists of several Ensembles of Neural Networks is created and trained on Bootstrap Sample Sets, using various data representations and architectures. The ANNs within the Ensembles are aggregated (as in Bagging) while the Ensembles are integrated non-linearly, in a signal adaptive manner, using a posterior confidence measure based on the agreement (variance) within the Ensembles. The proposed Integrated Classification Machine achieved 92.1% correct classifications on the seismic test data. Cross Validation evaluations and comparisons indicate that such integration of a collection of ANN's Ensembles is a robust way for handling high dimensional problems with a complex non-stationary signal space as in the current Seismic Classification problem.\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\n",
      "Question: Title: Lookahead and Pathology in Decision Tree Induction does not cite Title: Classifying Seismic Signals by Integrating Ensembles of Neural Networks. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "14\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Collective Memory Search 1 Collective Memory Search: Exploiting an Information Center for Exploration\n",
      "abstract:Abstract: The results reported here empirically show the benefit of decision tree size biases as a function of concept distribution. First, it is shown how concept distribution complexity (the number of internal nodes in the smallest decision tree consistent with the example space) affects the benefit of minimum size and maximum size decision tree biases. Second, a policy is described that defines what a learner should do given knowledge of the complexity of the distribution of concepts. Third, explanations for why the distribution of concepts seen in practice is amenable to the minimum size decision tree bias are given and evaluated empirically.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: The Possible Contribution of AI to the Avoidance of Crises and Wars: Using CBR Methods\n",
      "abstract:Abstract: This paper presents the application of Case-Based Reasoning methods to the KOSIMO data base of international conflicts. A Case-Based Reasoning tool - VIE-CBR has been deveolped and used for the classification of various outcome variables, like political, military, and territorial outcome, solution modalities, and conflict intensity. In addition, the case retrieval algorithms are presented as an interactive, user-modifiable tool for intelli gently searching the conflict data base for precedent cases.\n",
      "category:Case_Based\n",
      "\\\n",
      "\n",
      "Question: Title: Collective Memory Search 1 Collective Memory Search: Exploiting an Information Center for Exploration does not cite Title: The Possible Contribution of AI to the Avoidance of Crises and Wars: Using CBR Methods. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "15\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Learning physical descriptions from functional definitions, examples, Learning from examples: The effect of different conceptual\n",
      "abstract:Abstract: Technical Report 94-07 February 7, 1994 (updated April 25, 1994) This paper will appear in Proceedings of the Eleventh International Conference on Machine Learning. Abstract This paper presents an algorithm for incremental induction of decision trees that is able to handle both numeric and symbolic variables. In order to handle numeric variables, a new tree revision operator called `slewing' is introduced. Finally, a non-incremental method is given for finding a decision tree based on a direct metric of a candidate tree.\n",
      "category:Case_Based\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: References Linear Controller Design, Limits of Performance, \"The parallel projection operators of a nonlinear feedback\n",
      "abstract:Abstract: 13] Yang, Y., H.J. Sussmann, and E.D. Sontag, \"Stabilization of linear systems with bounded controls,\" in Proc. Nonlinear Control Systems Design Symp., Bordeaux, June 1992 (M. Fliess, Ed.), IFAC Publications, pp. 15-20. Journal version to appear in IEEE Trans. Autom. Control .\n",
      "category:Neural_Networks\n",
      "\\\n",
      "\n",
      "Question: Title: Learning physical descriptions from functional definitions, examples, Learning from examples: The effect of different conceptual does not cite Title: References Linear Controller Design, Limits of Performance, \"The parallel projection operators of a nonlinear feedback. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "16\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Continuous Case-Based Reasoning\n",
      "abstract:Abstract: Case-based reasoning systems have traditionally been used to perform high-level reasoning in problem domains that can be adequately described using discrete, symbolic representations. However, many real-world problem domains, such as autonomous robotic navigation, are better characterized using continuous representations. Such problem domains also require continuous performance, such as online sensorimotor interaction with the environment, and continuous adaptation and learning during the performance task. This article introduces a new method for continuous case-based reasoning, and discusses its application to the dynamic selection, modification, and acquisition of robot behaviors in an autonomous navigation system, SINS (Self-Improving Navigation System). The computer program and the underlying method are systematically evaluated through statistical analysis of results from several empirical studies. The article concludes with a general discussion of case-based reasoning issues addressed by this research.\n",
      "category:Case_Based\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Confidence as Higher Order Uncertainty proposed for handling higher order uncertainty, including the Bayesian approach,\n",
      "abstract:Abstract:\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\n",
      "Question: Title: Continuous Case-Based Reasoning does not cite Title: Confidence as Higher Order Uncertainty proposed for handling higher order uncertainty, including the Bayesian approach,. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "17\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Learning from positive data\n",
      "abstract:Abstract: Gold showed in 1967 that not even regular grammars can be exactly identified from positive examples alone. Since it is known that children learn natural grammars almost exclusively from positives examples, Gold's result has been used as a theoretical support for Chomsky's theory of innate human linguistic abilities. In this paper new results are presented which show that within a Bayesian framework not only grammars, but also logic programs are learnable with arbitrarily low expected error from positive examples only. In addition, we show that the upper bound for expected error of a learner which maximises the Bayes' posterior probability when learning from positive examples is within a small additive term of one which does the same from a mixture of positive and negative examples. An Inductive Logic Programming implementation is described which avoids the pitfalls of greedy search by global optimisation of this function during the local construction of individual clauses of the hypothesis. Results of testing this implementation on artificially-generated data-sets are reported. These results are in agreement with the theoretical predictions.\n",
      "category:Theory\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Genetic Algorithms for Combinatorial Optimization: The Assembly Line Balancing Problem\n",
      "abstract:Abstract: Genetic algorithms are one example of the use of a random element within an algorithm for combinatorial optimization. We consider the application of the genetic algorithm to a particular problem, the Assembly Line Balancing Problem. A general description of genetic algorithms is given, and their specialized use on our test-bed problems is discussed. We carry out extensive computational testing to find appropriate values for the various parameters associated with this genetic algorithm. These experiments underscore the importance of the correct choice of a scaling parameter and mutation rate to ensure the good performance of a genetic algorithm. We also describe a parallel implementation of the genetic algorithm and give some comparisons between the parallel and serial implementations. Both versions of the algorithm are shown to be effective in producing good solutions for problems of this type (with appropriately chosen parameters).\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\n",
      "Question: Title: Learning from positive data does not cite Title: Genetic Algorithms for Combinatorial Optimization: The Assembly Line Balancing Problem. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "18\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: University of Nevada Reno Design Strategies for Evolutionary Robotics\n",
      "abstract:Abstract: CuPit-2 is a special-purpose programming language designed for expressing dynamic neural network learning algorithms. It provides most of the flexibility of general-purpose languages such as C or C ++ , but is more expressive. It allows writing much clearer and more elegant programs, in particular for algorithms that change the network topology dynamically (constructive algorithms, pruning algorithms). In contrast to other languages, CuPit-2 programs can be compiled into efficient code for parallel machines without any changes in the source program, thus providing an easy start for using parallel platforms. This article analyzes the circumstances under which the CuPit-2 approach is the most useful one, presents a description of most language constructs and reports performance results for CuPit-2 on symmetric multiprocessors (SMPs). It concludes that in many cases CuPit-2 is a good basis for neural learning algorithm research on small-scale parallel machines.\n",
      "category:Genetic_Algorithms\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Structured Representation of Complex Stochastic Systems\n",
      "abstract:Abstract: This paper considers the problem of representing complex systems that evolve stochastically over time. Dynamic Bayesian networks provide a compact representation for stochastic processes. Unfortunately, they are often unwieldy since they cannot explicitly model the complex organizational structure of many real life systems: the fact that processes are typically composed of several interacting subprocesses, each of which can, in turn, be further decomposed. We propose a hierarchically structured representation language which extends both dynamic Bayesian networks and the object-oriented Bayesian network framework of [9], and show that our language allows us to describe such systems in a natural and modular way. Our language supports a natural representation for certain system characteristics that are hard to capture using more traditional frameworks. For example, it allows us to represent systems where some processes evolve at a different rate than others, or systems where the processes interact only intermittently. We provide a simple inference mechanism for our representation via translation to Bayesian networks, and suggest ways in which the inference algorithm can exploit the additional structure encoded in our representation.\n",
      "category:Probabilistic_Methods\n",
      "\\\n",
      "\n",
      "Question: Title: University of Nevada Reno Design Strategies for Evolutionary Robotics does not cite Title: Structured Representation of Complex Stochastic Systems. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n",
      "19\n",
      "\n",
      "Given information of two paper and their citation relationship shown as follow:\n",
      "\\\n",
      "paper1 informations:\n",
      "title:Title: Generalizing from Case Studies: A Case Study\n",
      "abstract:Abstract: Most empirical evaluations of machine learning algorithms are case studies evaluations of multiple algorithms on multiple databases. Authors of case studies implicitly or explicitly hypothesize that the pattern of their results, which often suggests that one algorithm performs significantly better than others, is not limited to the small number of databases investigated, but instead holds for some general class of learning problems. However, these hypotheses are rarely supported with additional evidence, which leaves them suspect. This paper describes an empirical method for generalizing results from case studies and an example application. This method yields rules describing when some algorithms significantly outperform others on some dependent measures. Advantages for generalizing from case studies and limitations of this particular approach are also described.\n",
      "category:Case_Based\n",
      "\\\n",
      "\\\n",
      "paper2 informations: \n",
      "title:Title: Discovery of Physical Principles from Design Experiences\n",
      "abstract:Abstract: One method for making analogies is to access and instantiate abstract domain principles, and one method for acquiring knowledge of abstract principles is to discover them from experience. We view generalization over experiences in the absence of any prior knowledge of the target principle as the task of hypothesis formation, a subtask of discovery. Also, we view the use of the hypothesized principles for analogical design as the task of hypothesis testing, another subtask of discovery. In this paper, we focus on discovery of physical principles by generalization over design experiences in the domain of physical devices. Some important issues in generalization from experiences are what to generalize from an experience, how far to generalize, and what methods to use. We represent a reasoner's comprehension of specific designs in the form of structure-behavior-function (SBF) models. An SBF model provides a functional and causal explanation of the working of a device. We represent domain principles as device-independent behavior-function (BF) models. We show that (i) the function of a device determines what to generalize from its SBF model, (ii) the SBF model itself suggests how far to generalize, and (iii) the typology of functions indicates what method to use.\n",
      "category:Case_Based\n",
      "\\\n",
      "\n",
      "Question: Title: Generalizing from Case Studies: A Case Study does not cite Title: Discovery of Physical Principles from Design Experiences. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
      "your answer format:\n",
      "\\\n",
      "citation rules: \n",
      "[rule1]\n",
      "[rule2]\n",
      "...\n",
      "\\\n",
      "Note that every rule should be limited in no more than two sentences.\n",
      "\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# generate rules\n",
    "sample = pd.read_csv('sample/no/sample_one.csv')\n",
    "\n",
    "for i in range(len(sample)):\n",
    "    cite_title, cite_abs, cite_label, cited_title, cited_abs, cited_label, link = sample.loc[i, 'cite_title'], sample.loc[i, 'cite_abs'], sample.loc[i, 'cite_label'], sample.loc[i, 'cited_title'], sample.loc[i, 'cited_abs'], sample.loc[i, 'cited_label'], sample.loc[i, 'link']\n",
    "    prompt = f\"\"\"\n",
    "Given information of two paper and their citation relationship shown as follow:\n",
    "\\\\\n",
    "paper1 informations:\n",
    "title:{cite_title}\n",
    "abstract:{cite_abs}\n",
    "category:{cite_label}\n",
    "\\\\\n",
    "\\\\\n",
    "paper2 informations: \n",
    "title:{cited_title}\n",
    "abstract:{cited_abs}\n",
    "category:{cited_label}\n",
    "\\\\\n",
    "\"\"\"\n",
    "\n",
    "    if link:\n",
    "        question = f\"\"\"\n",
    "Question: {cite_title} cites {cited_title}. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not\n",
    "your answer format:\n",
    "\\\\\n",
    "citation rules: \n",
    "[rule1]\n",
    "[rule2]\n",
    "...\n",
    "\\\\\n",
    "Note that every rule should be limited in no more than two sentences.\n",
    "\"\"\"\n",
    "    else:\n",
    "        question = f\"\"\"\n",
    "Question: {cite_title} does not cite {cited_title}. Please extract from this example the general rule for determining whether there is a citation relationship between two papers or not.\n",
    "your answer format:\n",
    "\\\\\n",
    "citation rules: \n",
    "[rule1]\n",
    "[rule2]\n",
    "...\n",
    "\\\\\n",
    "Note that every rule should be limited in no more than two sentences.\n",
    "\"\"\"\n",
    "\n",
    "    print(i)\n",
    "    print(prompt + question)\n",
    "    print('==============================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "given some rules for judging whether there is a citation relationship between two papers:\n",
      "\n",
      "1.Title/Source Citation: If one paper explicitly mentions the title or source of another paper along with contextual relevance or discussion, it indicates a potential citation relationship.\n",
      "Acknowledgment of Contribution: When a paper acknowledges or references the work, findings, or methodology of another paper, it suggests a possible citation relationship.\n",
      "Shared Authors or Collaborators: If there are shared authors or collaborators between two papers, it often indicates a citation relationship, especially if the topics are related.\n",
      "Similar Research Focus or Context: Papers addressing similar topics, methodologies, or problems might mutually reference each other, implying a citation relationship. \n",
      "2.Shared Conceptual Alignment: When one paper shares a conceptual alignment or discusses similar methodologies, techniques, or problem-solving approaches introduced or detailed in another paper, it hints at a potential citation relationship between the two. \n",
      "3.1. **Methodology Reference:** If one paper describes or discusses the methodology, techniques, or concepts introduced in another paper, indicating a clear connection or relevance between the content, it implies a potential citation relationship.\n",
      "2. **Acknowledgment of Findings or Results:** When a paper acknowledges or refers to the specific findings, results, or theories presented in another paper, demonstrating a relationship or influence, it signifies a potential citation relationship. \n",
      "4.Conceptual Integration: When one paper discusses the methodology, techniques, or concepts introduced in another paper, specifically focusing on their integration or application within its content, it suggests a potential citation relationship between the two papers. \n",
      "5.Conceptual Relevance: When one paper discusses findings, concepts, or methodologies that align or complement the themes, mechanisms, or techniques introduced in another paper, it suggests a potential citation relationship between the two. \n",
      "6.Methodological Contribution: When one paper presents methodologies, techniques, or novel approaches that directly complement or improve upon concepts introduced in another paper, it indicates a potential citation relationship between the two. \n",
      "7.Methodological Comparison or Enhancement: When one paper introduces advancements, methodologies, or approaches that directly relate to or improve upon concepts presented in another paper within a similar domain or field, it suggests a potential citation relationship between the two. \n",
      "8.Technological Implementation Utilization: When one paper introduces or utilizes a specific technological methodology, technique, or implementation approach detailed in another paper, especially within a related field or domain, it suggests a potential citation relationship between the two. \n",
      "9.Methodological Alignment: When one paper adopts or builds upon a specific methodology, technique, or algorithm detailed in another paper, particularly within a similar domain or field, it implies a potential citation relationship between the two. \n",
      "10.Methodological Influence: When one paper presents findings or techniques that directly impact or align with specific aspects, methodologies, or areas of research introduced or explored in another paper, it suggests a potential citation relationship between the two. \n",
      "\n",
      "Please help me summarise the above rules and merge similar ones\n",
      "your answer format:\n",
      "\\\n",
      "[rule keywords]: [description in no more than one sentence]\n",
      "\\\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv('sample/no/sample_one.csv')\n",
    "\n",
    "pos_sample = sample[sample.link == 1].reset_index(drop=True)\n",
    "\n",
    "prompt = \"\"\"\n",
    "given some rules for judging whether there is a citation relationship between two papers:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i in range(len(pos_sample)):\n",
    "    pos_rule = pos_sample.loc[i, 'citation_rules']\n",
    "    prompt += f'{i+1}.{pos_rule} \\n'\n",
    "\n",
    "question = \"\"\"\n",
    "Please help me summarise the above rules and merge similar ones\n",
    "your answer format:\n",
    "\\\\\n",
    "[rule keywords]: [description in no more than one sentence]\n",
    "\\\\\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt+question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "given some rules for judging whether there is a citation relationship between two papers:\n",
      "\n",
      "1.Independent Topic Focus: When one paper focuses on presenting or exploring a distinct topic, methodology, or concept that does not directly align or overlap with the content or area of research introduced or detailed in another paper, it implies a lack of a citation relationship between the two. \n",
      "2.Distinct Topic Focus: When one paper focuses on presenting methodologies, approaches, or findings that do not directly intersect or address the concepts, techniques, or domain discussed in another paper, it suggests a lack of a citation relationship between the two. \n",
      "3.Disparate Research Areas: When two papers explore distinct topics, methodologies, or domains without overlapping concepts or direct relevance to each other's subject matter, it suggests a lack of a citation relationship between them. \n",
      "4.Unrelated Methodologies/Subjects: When two papers explore distinct methodologies or unrelated subjects without direct overlap in concepts or relevance to each other's research domain, it suggests a lack of a citation relationship between them. \n",
      "5.Divergent Research Areas: When two papers explore entirely different research domains or methodologies with no evident conceptual or topical overlap, it often suggests a lack of a citation relationship between them. \n",
      "6.Distinct Research Focus: When two papers delve into vastly different research domains or fields, lacking thematic or conceptual alignment, it often indicates no citation relationship between them. \n",
      "7.Divergent Research Domains: When two papers explore distinct research domains, one focusing on continuous case-based reasoning and the other on handling higher order uncertainty using probabilistic methods, the lack of thematic alignment suggests no direct citation relationship. \n",
      "8.Disparate Research Focus: When papers belong to distinctly different research domains, like theoretical learning from positive examples and application of genetic algorithms to combinatorial optimization problems, the lack of thematic overlap might indicate no direct citation relationship. \n",
      "9.Divergent Focus Areas: When papers belong to distinctly different research domainslike evolutionary robotics design strategies and structured representation of stochastic systemslacking a thematic intersection can indicate no direct citation relationship. \n",
      "10.Divergent Methodologies: When papers explore distinct methodologies or frameworksfor instance, generalizing from case studies versus discovering principles from design experienceslack of citation could indicate a divergence in research approaches, potentially leading to no direct citation relationship. \n",
      "\n",
      "Please help me summarise the above rules and merge similar ones\n",
      "your answer format:\n",
      "\\\n",
      "[rule keywords]: [description in no more than one sentence]\n",
      "\\\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv('sample/no/sample_one.csv')\n",
    "\n",
    "neg_sample = sample[sample.link == 0].reset_index(drop=True)\n",
    "\n",
    "prompt = \"\"\"\n",
    "given some rules for judging whether there is a citation relationship between two papers:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i in range(len(neg_sample)):\n",
    "    neg_rule = neg_sample.loc[i, 'citation_rules']\n",
    "    prompt += f'{i+1}.{neg_rule} \\n'\n",
    "\n",
    "question = \"\"\"\n",
    "Please help me summarise the above rules and merge similar ones\n",
    "your answer format:\n",
    "\\\\\n",
    "[rule keywords]: [description in no more than one sentence]\n",
    "\\\\\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt+question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
