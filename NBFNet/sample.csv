pid,title,abs,label,keywords,h0,g
32083,Title: Back Propagation is Sensitive to Initial Conditions,"Abstract: This paper explores the effect of initial weight selection on feed-forward networks learning simple functions with the back-propagation technique. We first demonstrate, through the use of Monte Carlo techniques, that the magnitude of the initial condition vector (in weight space) is a very significant parameter in convergence time variability. In order to further understand this result, additional deterministic experiments were performed. The results of these experiments demonstrate the extreme sensitivity of back propagation to initial weight configuration.",Neural_Networks,"Back propagation, initial weight selection, feed-forward networks, convergence time, Monte Carlo experiments, deterministic experiments.","Title: 4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.: Discusses implementing application-specific routines in genetic algorithms, emphasizing changes in the 'app.c' file, utilizing 'sga.h' and 'external.h' for variable declaration, and offering examples of applications within the SGA-C distribution.
Title: Back Propagation is Sensitive to Initial Conditions: Explores the impact of initial weight selection on feed-forward networks using the back-propagation technique, highlighting through Monte Carlo and deterministic experiments the significant influence of initial weight configuration on convergence time variability.","
Summarization: ""4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning"" discusses application-specific routines in genetic algorithms, focusing on code changes and examples within the SGA-C distribution. ""Back Propagation is Sensitive to Initial Conditions"" explores the impact of initial weight selection on feed-forward networks using back-propagation, emphasizing the influence on convergence time variability through Monte Carlo and deterministic experiments.

Citation possibility: There's a moderate likelihood of these papers referencing each other, given their shared focus on algorithm implementation and experimentation in different domains of machine learning. However, without specific citations or direct mentions, it's challenging to confirm a direct reference between the two."
273152,Title: Evolutionary Module Acquisition,"Abstract: A critical issue for users of Markov Chain Monte Carlo (MCMC) methods in applications is how to determine when it is safe to stop sampling and use the samples to estimate characteristics of the distribution of interest. Research into methods of computing theoretical convergence bounds holds promise for the future but currently has yielded relatively little that is of practical use in applied work. Consequently, most MCMC users address the convergence problem by applying diagnostic tools to the output produced by running their samplers. After giving a brief overview of the area, we provide an expository review of thirteen convergence diagnostics, describing the theoretical basis and practical implementation of each. We then compare their performance in two simple models and conclude that all the methods can fail to detect the sorts of convergence failure they were designed to identify. We thus recommend a combination of strategies aimed at evaluating and accelerating MCMC sampler convergence, including applying diagnostic procedures to a small number of parallel chains, monitoring autocorrelations and cross-correlations, and modifying parameterizations or sampling algorithms appropriately. We emphasize, however, that it is not possible to say with certainty that a finite sample from an MCMC algorithm is representative of an underlying stationary distribution. Mary Kathryn Cowles is Assistant Professor of Biostatistics, Harvard School of Public Health, Boston, MA 02115. Bradley P. Carlin is Associate Professor, Division of Biostatistics, School of Public Health, University of Minnesota, Minneapolis, MN 55455. Much of the work was done while the first author was a graduate student in the Divison of Biostatistics at the University of Minnesota and then Assistant Professor, Biostatistics Section, Department of Preventive and Societal Medicine, University of Nebraska Medical Center, Omaha, NE 68198. The work of both authors was supported in part by National Institute of Allergy and Infectious Diseases FIRST Award 1-R29-AI33466. The authors thank the developers of the diagnostics studied here for sharing their insights, experiences, and software, and Drs. Thomas Louis and Luke Tierney for helpful discussions and suggestions which greatly improved the manuscript.",Genetic_Algorithms,"convergence diagnostics, convergence assessment, parallel chains, parameter modifications, limitations in detecting convergence failures.","Title: 4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.: Discusses the implementation of application-specific routines in genetic algorithms, highlighting 'app.c', variable declaration methods in 'sga.h' and 'external.h', providing examples within the SGA-C distribution, and emphasizing simplicity for first-time GA experimentation.
Title: Evolutionary Module Acquisition: Focuses on Markov Chain Monte Carlo (MCMC) methods, addressing the challenge of determining convergence in practical applications, discussing thirteen convergence diagnostics, their limitations in detecting convergence failures, and recommending strategies including parallel chains and parameter modifications to assess MCMC sampler convergence.","Summarization: ""4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning"" focuses on implementing application-specific routines in genetic algorithms, emphasizing simplicity for newcomers and providing practical examples within the SGA-C distribution. ""Evolutionary Module Acquisition"" addresses Markov Chain Monte Carlo (MCMC) methods, discussing convergence diagnostics, limitations, and strategies like parallel chains and parameter modifications for assessing MCMC sampler convergence.

Citation possibility: There seems to be a low likelihood of direct references between these papers. While they both delve into algorithmic methodologies and convergence-related discussions, their primary focuses¡ªgenetic algorithms and MCMC methods¡ªare distinct, suggesting limited cross-referencing."
35061,Title: Evolving Networks: Using the Genetic Algorithm with Connectionist Learning,"Abstract: A pilot study is described on the practical application of artificial neural networks. The limit cycle of the attitude control of a satellite is selected as the test case. One of the sources of the limit cycle is a position dependent error in the observed attitude. A Reinforcement Learning method is selected, which is able to adapt a controller such that a cost function is optimised. An estimate of the cost function is learned by a neural `critic'. In our approach, the estimated cost function is directly represented as a function of the parameters of a linear controller. The critic is implemented as a CMAC network. Results from simulations show that the method is able to find optimal parameters without unstable behaviour. In particular in the case of large discontinuities in the attitude measurements, the method shows a clear improvement compared to the conventional approach: the RMS attitude error decreases approximately 30%.",Genetic_Algorithms,"Artificial neural networks, attitude control, satellite, Reinforcement Learning, neural critic, CMAC network, optimization, stability in simulations.","Title: 4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.: Discusses implementing application-specific routines in genetic algorithms using 'app.c', 'sga.h', and 'external.h', providing examples in SGA-C, aimed at simplifying GA experimentation.
Title: Evolving Networks: Using the Genetic Algorithm with Connectionist Learning: Describes a pilot study applying artificial neural networks for attitude control of a satellite using Reinforcement Learning, a neural 'critic', and CMAC network implementation, showcasing improved optimization without instability in simulations.","Summarization: ""4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning"" focuses on simplifying genetic algorithm experimentation through application-specific routines, while ""Evolving Networks: Using the Genetic Algorithm with Connectionist Learning"" details the application of artificial neural networks for satellite attitude control via Reinforcement Learning and neural network implementations, showcasing improved optimization without instability in simulations.

Citation possibility: There's a slim chance of direct references between these papers. While both touch on genetic algorithms, their core emphases diverge¡ªthe former on simplifying GA experimentation and the latter on neural network applications for satellite control. The distinct areas of focus may limit direct cross-referencing."
44514,Title: The Observers Paradox: Apparent Computational Complexity in Physical Systems,"Abstract: Many connectionist approaches to musical expectancy and music composition let the question of What next? overshadow the equally important question of When next?. One cannot escape the latter question, one of temporal structure, when considering the perception of musical meter. We view the perception of metrical structure as a dynamic process where the temporal organization of external musical events synchronizes, or entrains, a listeners internal processing mechanisms. This article introduces a novel connectionist unit, based upon a mathematical model of entrainment, capable of phase and frequency-locking to periodic components of incoming rhythmic patterns. Networks of these units can self-organize temporally structured responses to rhythmic patterns. The resulting network behavior embodies the perception of metrical structure. The article concludes with a discussion of the implications of our approach for theories of metrical structure and musical expectancy.",Neural_Networks,"Connectionist approaches, musical expectancy, temporal structure perception, musical meter, connectionist unit, rhythmic patterns, self-organization, metrical structure theories.","Title: 4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.: Discusses the implementation of application-specific routines in genetic algorithms, emphasizing 'app.c', 'sga.h', and 'external.h', providing examples in SGA-C, aimed at simplifying GA experimentation.
Title: The Observers Paradox: Apparent Computational Complexity in Physical Systems: Explores connectionist approaches to musical expectancy, focusing on temporal structure perception in musical meter, introducing a novel connectionist unit for phase and frequency-locking to rhythmic patterns, enabling networks to self-organize temporally structured responses, concluding with implications for theories of metrical structure and musical expectancy.","Summarization: ""4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning"" focuses on simplifying genetic algorithm experimentation, while ""The Observers Paradox: Apparent Computational Complexity in Physical Systems"" delves into connectionist approaches for musical expectancy, specifically addressing temporal structure perception in musical meter through novel connectionist units for rhythmic pattern recognition and self-organization.

Citation possibility: There's a low probability of direct references between these papers. They explore disparate domains¡ªgenetic algorithms and connectionist approaches in music perception¡ªindicating minimal overlap in subject matter. Their distinct focuses make direct citations or references less likely."
35,"Title: 4 Implementing Application Specific Routines  Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.","Abstract: To implement a specific application, you should only have to change the file app.c. Section 2 describes the routines in app.c in detail. If you use additional variables for your specific problem, the easiest method of making them available to other program units is to declare them in sga.h and external.h. However, take care that you do not redeclare existing variables. Two example applications files are included in the SGA-C distribution. The file app1.c performs the simple example problem included with the Pascal version; finding the maximum of x 10 , where x is an integer interpretation of a chromosome. A slightly more complex application is include in app2.c. This application illustrates two features that have been added to SGA-C. The first of these is the ithruj2int function, which converts bits i through j in a chromosome to an integer. The second new feature is the utility pointer that is associated with each population member. The example application interprets each chromosome as a set of concatenated integers in binary form. The lengths of these integer fields is determined by the user-specified value of field size, which is read in by the function app data(). The field size must be less than the smallest of the chromosome length and the length of an unsigned integer. An integer array for storing the interpreted form of each chromosome is dynamically allocated and assigned to the chromosome's utility pointer in app malloc(). The ithruj2int routine (see utility.c) is used to translate each chromosome into its associated vector. The fitness for each chromosome is simply the sum of the squares of these integers. This example application will function for any chromosome length. SGA-C is intended to be a simple program for first-time GA experimentation. It is not intended to be definitive in terms of its efficiency or the grace of its implementation. The authors are interested in the comments, criticisms, and bug reports from SGA-C users, so that the code can be refined for easier use in subsequent versions. Please email your comments to rob@galab2.mh.ua.edu, or write to TCGA: The authors gratefully acknowledge support provided by NASA under Grant NGT-50224 and support provided by the National Science Foundation under Grant CTS-8451610. We also thank Hillol Kargupta for donating his tournament selection implementation. Booker, L. B. (1982). Intelligent behavior as an adaptation to the task environment (Doctoral dissertation, Technical Report No. 243. Ann Arbor: University of Michigan, Logic of Computers Group). Dissertations Abstracts International, 43(2), 469B. (University Microfilms No. 8214966)",Genetic_Algorithms,"Genetic algorithms, application-specific routines, 'app.c', 'sga.h', 'external.h', SGA-C distribution.",,
210871,Title: Coevolving High-Level Representations,"Abstract: Rules extracted from trained feedforward networks can be used for explanation, validation, and cross-referencing of network output decisions. This paper introduces a rule evaluation and ordering mechanism that orders rules extracted from feedforward networks based on three performance measures. Detailed experiments using three rule extraction techniques as applied to the Wisconsin breast cancer database, illustrate the power of the proposed methods. Moreover, a method of integrating the output decisions of both the extracted rule-based system and the corresponding trained network is proposed. The integrated system provides further improvements.",Genetic_Algorithms,"Rule extraction, feedforward networks, performance measures, Wisconsin breast cancer database, integrated system, rule-based systems, trained networks, improvement in output decisions.","Title: 4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.: Discusses the implementation of application-specific routines in genetic algorithms, highlighting changes in 'app.c', 'sga.h', and 'external.h', providing examples within the SGA-C distribution, aimed at simplifying GA experimentation.
Title: Coevolving High-Level Representations: Introduces a mechanism to evaluate and order rules extracted from trained feedforward networks based on performance measures, conducts experiments on rule extraction techniques applied to the Wisconsin breast cancer database, and proposes integrating the output decisions of rule-based systems and trained networks for further improvements.","
Summarization: ""4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning"" focuses on simplifying genetic algorithm experimentation, while ""Coevolving High-Level Representations"" introduces a mechanism for evaluating and ordering rules extracted from trained networks, conducting experiments on rule extraction techniques with the Wisconsin breast cancer database, and proposing integration strategies for rule-based systems and trained networks to enhance output decisions.

Citation possibility: There's a moderate likelihood of these papers referencing each other. While they explore different aspects¡ªgenetic algorithms and rule extraction from networks¡ªtheir shared emphasis on optimization and techniques for improving system outputs suggests a potential for cross-referencing or inspiration from the methodologies presented in the other paper."
210872,Title: An Evolutionary Algorithm that Constructs Recurrent Neural Networks,"Abstract: Standard methods for inducing both the structure and weight values of recurrent neural networks fit an assumed class of architectures to every task. This simplification is necessary because the interactions between network structure and function are not well understood. Evolutionary computation, which includes genetic algorithms and evolutionary programming, is a population-based search method that has shown promise in such complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. This algorithms empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods.",Genetic_Algorithms,"Recurrent neural networks, genetic algorithms, evolutionary computation, GNARL, network acquisition, complex behaviors, architectural constraints, topology emergence.




","Title: 4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.: Discusses implementing application-specific routines in genetic algorithms, emphasizing 'app.c', 'sga.h', 'external.h', providing examples within the SGA-C distribution, aimed at simplifying GA experimentation.
Title: An Evolutionary Algorithm that Constructs Recurrent Neural Networks: Challenges the standard methods of inducing recurrent neural networks by proposing GNARL, an evolutionary program acquiring both structure and weights simultaneously, enabling the emergence of complex behaviors and topologies not constrained by artificial architectural limitations.","Summarization: ""4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning"" focuses on simplifying genetic algorithm experimentation, while ""An Evolutionary Algorithm that Constructs Recurrent Neural Networks"" introduces GNARL, an evolutionary program enabling the simultaneous acquisition of structure and weights in recurrent neural networks, allowing for the emergence of diverse behaviors and topologies unconstrained by artificial limitations.

Citation possibility: There's a significant likelihood of these papers referencing each other. Both delve into evolutionary approaches¡ªgenetic algorithms in one and GNARL in the other¡ªwith a focus on optimizing neural network structures. The shared emphasis on evolutionary techniques for constructing networks suggests a high possibility of cross-referencing or influence between these papers."
82920,Title: A Survey of Evolution Strategies,Abstract:,Genetic_Algorithms,"Evolution strategies, optimization, machine learning, methodologies, algorithms, applications.","Title: 4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.: Discusses the implementation of application-specific routines in genetic algorithms, emphasizing changes in 'app.c', 'sga.h', and 'external.h', providing examples within the SGA-C distribution, aimed at simplifying GA experimentation.
Title: A Survey of Evolution Strategies: A comprehensive overview of various evolution strategies used in optimization and machine learning, potentially covering different methodologies, algorithms, and applications under the evolution strategies umbrella.","Summarization: ""4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning"" focuses on implementing application-specific routines in genetic algorithms for simplified experimentation, while ""A Survey of Evolution Strategies"" provides a comprehensive overview of various evolution strategies within optimization and machine learning, encompassing methodologies, algorithms, and diverse applications.

Citation possibility: There's a moderate likelihood of these papers referencing each other. While they explore related domains¡ªgenetic algorithms and evolution strategies¡ªthe latter is a broad survey covering various methodologies, potentially including genetic algorithms. However, direct citations might not be prevalent due to the survey nature of the second paper, encompassing a wide array of techniques."
141342,Title: Dynamic Parameter Encoding for Genetic Algorithms,Abstract: The common use of static binary place-value codes for real-valued parameters of the phenotype in Holland's genetic algorithm (GA) forces either the sacrifice of representational precision for efficiency of search or vice versa. Dynamic Parameter Encoding (DPE) is a mechanism that avoids this dilemma by using convergence statistics derived from the GA population to adaptively control the mapping from fixed-length binary genes to real values. DPE is shown to be empirically effective and amenable to analysis; we explore the problem of premature convergence in GAs through two convergence models.,Genetic_Algorithms,"Genetic algorithms, Dynamic Parameter Encoding (DPE), real-valued parameters, convergence statistics, binary-to-real value mapping, search efficiency, representational precision.","Title: 4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley.: Provides a guide for implementing application-specific routines in genetic algorithms, emphasizing changes in 'app.c', 'sga.h', and 'external.h', with examples within the SGA-C distribution, aimed at simplifying GA experimentation.
Title: Dynamic Parameter Encoding for Genetic Algorithms: Introduces Dynamic Parameter Encoding (DPE) to manage the representation of real-valued parameters in genetic algorithms, avoiding the trade-off between search efficiency and representational precision by using convergence statistics to adaptively control binary-to-real value mappings. It delves into empirical effectiveness and convergence models.","Summarization: ""4 Implementing Application Specific Routines Genetic algorithms in search, optimization, and machine learning"" focuses on implementing application-specific routines in genetic algorithms for simplified experimentation, while ""Dynamic Parameter Encoding for Genetic Algorithms"" introduces Dynamic Parameter Encoding (DPE) to manage real-valued parameters in GAs, using adaptive binary-to-real mappings based on convergence statistics to balance search efficiency and representational precision.

Citation possibility: There's a high likelihood of these papers referencing each other. Both papers delve into genetic algorithms and optimization techniques, with the latter introducing a novel method¡ªDynamic Parameter Encoding¡ªwhich aligns with the theme of implementing specific routines within GAs discussed in the former paper. The specific technique introduced in the second paper might be cited or referenced in the context of genetic algorithm customization."
