{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pair representations $h_v^{(0)}$  赋值子图中每个节点和源节点信息\n",
    "2. Message $M_v^{(1)}$: {$h_v^{(0)}$, 所有指向v的节点x的$h_x$和(x, v)总结后的信息}\n",
    "3. 总结所有Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample.csv')\n",
    "cite = pd.read_csv('sample_cite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_pid</th>\n",
       "      <th>src_title</th>\n",
       "      <th>src_abs</th>\n",
       "      <th>src_label</th>\n",
       "      <th>v_pid</th>\n",
       "      <th>v_title</th>\n",
       "      <th>v_abs</th>\n",
       "      <th>v_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>63486</td>\n",
       "      <td>Title: Evaluation and Selection of Biases in M...</td>\n",
       "      <td>Abstract: In this introduction, we define the ...</td>\n",
       "      <td>Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>2440</td>\n",
       "      <td>Title: Quinlan, 1990 J.R. Quinlan. Learning lo...</td>\n",
       "      <td>Abstract: We describe a ranked-model semantics...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>3231</td>\n",
       "      <td>Title: Irrelevant Features and the Subset Sele...</td>\n",
       "      <td>Abstract: We address the problem of finding a ...</td>\n",
       "      <td>Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>954315</td>\n",
       "      <td>Title: Inductive Database Design</td>\n",
       "      <td>Abstract: When designing a (deductive) databas...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>144330</td>\n",
       "      <td>Title: Utilizing Prior Concepts for Learning</td>\n",
       "      <td>Abstract: The inductive learning problem consi...</td>\n",
       "      <td>Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>56115</td>\n",
       "      <td>Title: Improving Tactical Plans with Genetic A...</td>\n",
       "      <td>Abstract:</td>\n",
       "      <td>Genetic_Algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>72908</td>\n",
       "      <td>Title: Applications of a logical discovery engine</td>\n",
       "      <td>Abstract: The clausal discovery engine claudie...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>83461</td>\n",
       "      <td>Title: Dlab: A Declarative Language Bias Forma...</td>\n",
       "      <td>Abstract: We describe the principles and funct...</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>17811</td>\n",
       "      <td>Title: #1 Robust Feature Selection Algorithms</td>\n",
       "      <td>Abstract: Selecting a set of features which is...</td>\n",
       "      <td>Genetic_Algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10169</td>\n",
       "      <td>Title: Learning Boolean Concepts in the Presen...</td>\n",
       "      <td>Abstract: In this paper, we address the proble...</td>\n",
       "      <td>Theory</td>\n",
       "      <td>37483</td>\n",
       "      <td>Title: Learning Approximate Control Rules Of H...</td>\n",
       "      <td>Abstract: One of the difficult problems in the...</td>\n",
       "      <td>Case_Based</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   src_pid                                          src_title  \\\n",
       "0    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "1    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "2    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "3    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "4    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "5    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "6    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "7    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "8    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "9    10169  Title: Learning Boolean Concepts in the Presen...   \n",
       "\n",
       "                                             src_abs src_label   v_pid  \\\n",
       "0  Abstract: In this paper, we address the proble...    Theory   63486   \n",
       "1  Abstract: In this paper, we address the proble...    Theory    2440   \n",
       "2  Abstract: In this paper, we address the proble...    Theory    3231   \n",
       "3  Abstract: In this paper, we address the proble...    Theory  954315   \n",
       "4  Abstract: In this paper, we address the proble...    Theory  144330   \n",
       "5  Abstract: In this paper, we address the proble...    Theory   56115   \n",
       "6  Abstract: In this paper, we address the proble...    Theory   72908   \n",
       "7  Abstract: In this paper, we address the proble...    Theory   83461   \n",
       "8  Abstract: In this paper, we address the proble...    Theory   17811   \n",
       "9  Abstract: In this paper, we address the proble...    Theory   37483   \n",
       "\n",
       "                                             v_title  \\\n",
       "0  Title: Evaluation and Selection of Biases in M...   \n",
       "1  Title: Quinlan, 1990 J.R. Quinlan. Learning lo...   \n",
       "2  Title: Irrelevant Features and the Subset Sele...   \n",
       "3                   Title: Inductive Database Design   \n",
       "4       Title: Utilizing Prior Concepts for Learning   \n",
       "5  Title: Improving Tactical Plans with Genetic A...   \n",
       "6  Title: Applications of a logical discovery engine   \n",
       "7  Title: Dlab: A Declarative Language Bias Forma...   \n",
       "8      Title: #1 Robust Feature Selection Algorithms   \n",
       "9  Title: Learning Approximate Control Rules Of H...   \n",
       "\n",
       "                                               v_abs             v_label  \n",
       "0  Abstract: In this introduction, we define the ...              Theory  \n",
       "1  Abstract: We describe a ranked-model semantics...       Rule_Learning  \n",
       "2  Abstract: We address the problem of finding a ...              Theory  \n",
       "3  Abstract: When designing a (deductive) databas...       Rule_Learning  \n",
       "4  Abstract: The inductive learning problem consi...              Theory  \n",
       "5                                          Abstract:  Genetic_Algorithms  \n",
       "6  Abstract: The clausal discovery engine claudie...       Rule_Learning  \n",
       "7  Abstract: We describe the principles and funct...       Rule_Learning  \n",
       "8  Abstract: Selecting a set of features which is...  Genetic_Algorithms  \n",
       "9  Abstract: One of the difficult problems in the...          Case_Based  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一步\n",
    "src = sample[sample.pid == 10169]\n",
    "tar = sample[sample.pid == 63486]\n",
    "v_df = sample[sample.pid != src.pid.values[0]].reset_index(drop=True)\n",
    "\n",
    "h_v = pd.DataFrame({\n",
    "    'src_pid': [src.pid.values[0]] * len(v_df),\n",
    "    'src_title': [src.title.values[0]] * len(v_df),\n",
    "    'src_abs': [src['abs'].values[0]] * len(v_df),\n",
    "    'src_label': [src.label.values[0]] * len(v_df),\n",
    "    'v_pid': v_df.pid,\n",
    "    'v_title': v_df.title,\n",
    "    'v_abs': v_df['abs'],\n",
    "    'v_label': v_df.label\n",
    "})\n",
    "h_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3    10169\n",
       " Name: pid, dtype: int64,\n",
       " 63486)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.pid, tar['pid'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator\n",
    "\n",
    "hv_list_orig = []\n",
    "for i in range(len(h_v)):\n",
    "\n",
    "    src_title, src_abs, src_label = h_v.loc[i, 'src_title'], h_v.loc[i, 'src_abs'], h_v.loc[i, 'src_label']\n",
    "    tar_pid, tar_title, tar_abs, tar_label = h_v.loc[i, 'v_pid'], h_v.loc[i, 'v_title'], h_v.loc[i, 'v_abs'], h_v.loc[i, 'v_label']\n",
    "\n",
    "    # if tar['pid'].values[0] == tar_pid:\n",
    "    #     continue\n",
    "    \n",
    "    # print(tar_pid)\n",
    "\n",
    "    Indicator_prompt = f\"\"\"\n",
    "Given two papers information shown as follow:\n",
    "paper1:\n",
    "title:{src_title}\n",
    "abstract:{src_abs}\n",
    "label:{src_label}\n",
    "paper2:\n",
    "title:{tar_title}\n",
    "abstract:{tar_abs}\n",
    "label:{tar_label}\n",
    "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
    "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
    "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
    "\"\"\"\n",
    "\n",
    "    hv_list_orig.append(Indicator_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Evaluation and Selection of Biases in Machine Learning\n",
      "abstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.\n",
      "label:Theory\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Quinlan, 1990 J.R. Quinlan. Learning logical definitions from relations. Machine Learning, First-order theory revision. In\n",
      "abstract:Abstract: We describe a ranked-model semantics for if-then rules admitting exceptions, which provides a coherent framework for many facets of evidential and causal reasoning. Rule priorities are automatically extracted form the knowledge base to facilitate the construction and retraction of plausible beliefs. To represent causation, the formalism incorporates the principle of Markov shielding which imposes a stratified set of independence constraints on rankings of interpretations. We show how this formalism resolves some classical problems associated with specificity, prediction and abduction, and how it offers a natural way of unifying belief revision, belief update, and reasoning about actions.\n",
      "label:Rule_Learning\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Irrelevant Features and the Subset Selection Problem\n",
      "abstract:Abstract: We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.\n",
      "label:Theory\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Inductive Database Design\n",
      "abstract:Abstract: When designing a (deductive) database, the designer has to decide for each predicate (or relation) whether it should be defined extensionally or intensionally, and what the definition should look like. An intelligent system is presented to assist the designer in this task. It starts from an example database in which all predicates are defined extensionally. It then tries to compact the database by transforming extensionally defined predicates into intensionally defined ones. The intelligent system employs techniques from the area of inductive logic programming.\n",
      "label:Rule_Learning\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Utilizing Prior Concepts for Learning\n",
      "abstract:Abstract: The inductive learning problem consists of learning a concept given examples and non-examples of the concept. To perform this learning task, inductive learning algorithms bias their learning method. Here we discuss biasing the learning method to use previously learned concepts from the same domain. These learned concepts highlight useful information for other concepts in the domain. We describe a transference bias and present M-FOCL, a Horn clause relational learning algorithm, that utilizes this bias to learn multiple concepts. We provide preliminary empirical evaluation to show the effects of biasing previous information on noise-free and noisy data.\n",
      "label:Theory\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Improving Tactical Plans with Genetic Algorithms\n",
      "abstract:Abstract:\n",
      "label:Genetic_Algorithms\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Applications of a logical discovery engine\n",
      "abstract:Abstract: The clausal discovery engine claudien is presented. claudien discovers regularities in data and is a representative of the inductive logic programming paradigm. As such, it represents data and regularities by means of first order clausal theories. Because the search space of clausal theories is larger than that of attribute value representation, claudien also accepts as input a declarative specification of the language bias, which determines the set of syntactically well-formed regularities. Whereas other papers on claudien focuss on the semantics or logical problem specification of claudien, on the discovery algorithm, or the PAC-learning aspects, this paper wants to illustrate the power of the resulting technique. In order to achieve this aim, we show how claudien can be used to learn 1) integrity constraints in databases, 2) functional dependencies and determinations, 3) properties of sequences, 4) mixed quantitative and qualitative laws, 5) reverse engineering, and 6) classification rules.\n",
      "label:Rule_Learning\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Dlab: A Declarative Language Bias Formalism\n",
      "abstract:Abstract: We describe the principles and functionalities of Dlab (Declarative LAnguage Bias). Dlab can be used in inductive learning systems to define syntactically and traverse efficiently finite subspaces of first order clausal logic, be it a set of propositional formulae, association rules, Horn clauses, or full clauses. A Prolog implementation of Dlab is available by ftp access. Keywords: declarative language bias, concept learning, knowledge dis covery\n",
      "label:Rule_Learning\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: #1 Robust Feature Selection Algorithms\n",
      "abstract:Abstract: Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency.\n",
      "label:Genetic_Algorithms\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n",
      "\n",
      "Given two papers information shown as follow:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "paper2:\n",
      "title:Title: Learning Approximate Control Rules Of High Utility\n",
      "abstract:Abstract: One of the difficult problems in the area of explanation based learning is the utility problem; learning too many rules of low utility can lead to swamping, or degradation of performance. This paper introduces two new techniques for improving the utility of learned rules. The first technique is to combine EBL with inductive learning techniques to learn a better set of control rules; the second technique is to use these inductive techniques to learn approximate control rules. The two techniques are synthesized in an algorithm called approximating abductive explanation based learning (AxA-EBL). AxA-EBL is shown to improve substantially over standard EBL in several domains.\n",
      "label:Case_Based\n",
      "please summarize information of these two papers in no more than three sentences, and then generate some keywords of this information.\n",
      "Do not generate ordinal words like first, second, third, just provide the title of paper when mention a paper.\n",
      "Do not generate paper number like paper1, paper2, just provide the title of paper when mention a paper.\n",
      "\n",
      "======================================================================================\n"
     ]
    }
   ],
   "source": [
    "for hv in hv_list_orig:\n",
    "    print(hv)\n",
    "    print('======================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2440 72908\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Quinlan, 1990 J.R. Quinlan. Learning logical definitions from relations. Machine Learning, First-order theory revision. In\n",
      "abstract:Abstract: We describe a ranked-model semantics for if-then rules admitting exceptions, which provides a coherent framework for many facets of evidential and causal reasoning. Rule priorities are automatically extracted form the knowledge base to facilitate the construction and retraction of plausible beliefs. To represent causation, the formalism incorporates the principle of Markov shielding which imposes a stratified set of independence constraints on rankings of interpretations. We show how this formalism resolves some classical problems associated with specificity, prediction and abduction, and how it offers a natural way of unifying belief revision, belief update, and reasoning about actions.\n",
      "label:Rule_Learning\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Applications of a logical discovery engine\n",
      "abstract:Abstract: The clausal discovery engine claudien is presented. claudien discovers regularities in data and is a representative of the inductive logic programming paradigm. As such, it represents data and regularities by means of first order clausal theories. Because the search space of clausal theories is larger than that of attribute value representation, claudien also accepts as input a declarative specification of the language bias, which determines the set of syntactically well-formed regularities. Whereas other papers on claudien focuss on the semantics or logical problem specification of claudien, on the discovery algorithm, or the PAC-learning aspects, this paper wants to illustrate the power of the resulting technique. In order to achieve this aim, we show how claudien can be used to learn 1) integrity constraints in databases, 2) functional dependencies and determinations, 3) properties of sequences, 4) mixed quantitative and qualitative laws, 5) reverse engineering, and 6) classification rules.\n",
      "label:Rule_Learning\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "3231 63486\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Irrelevant Features and the Subset Selection Problem\n",
      "abstract:Abstract: We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.\n",
      "label:Theory\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Evaluation and Selection of Biases in Machine Learning\n",
      "abstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.\n",
      "label:Theory\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "10169 3231\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Features\n",
      "abstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.\n",
      "label:Theory\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Irrelevant Features and the Subset Selection Problem\n",
      "abstract:Abstract: We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.\n",
      "label:Theory\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "17811 63486\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: #1 Robust Feature Selection Algorithms\n",
      "abstract:Abstract: Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency.\n",
      "label:Genetic_Algorithms\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Evaluation and Selection of Biases in Machine Learning\n",
      "abstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.\n",
      "label:Theory\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "37483 2440\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Learning Approximate Control Rules Of High Utility\n",
      "abstract:Abstract: One of the difficult problems in the area of explanation based learning is the utility problem; learning too many rules of low utility can lead to swamping, or degradation of performance. This paper introduces two new techniques for improving the utility of learned rules. The first technique is to combine EBL with inductive learning techniques to learn a better set of control rules; the second technique is to use these inductive techniques to learn approximate control rules. The two techniques are synthesized in an algorithm called approximating abductive explanation based learning (AxA-EBL). AxA-EBL is shown to improve substantially over standard EBL in several domains.\n",
      "label:Case_Based\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Quinlan, 1990 J.R. Quinlan. Learning logical definitions from relations. Machine Learning, First-order theory revision. In\n",
      "abstract:Abstract: We describe a ranked-model semantics for if-then rules admitting exceptions, which provides a coherent framework for many facets of evidential and causal reasoning. Rule priorities are automatically extracted form the knowledge base to facilitate the construction and retraction of plausible beliefs. To represent causation, the formalism incorporates the principle of Markov shielding which imposes a stratified set of independence constraints on rankings of interpretations. We show how this formalism resolves some classical problems associated with specificity, prediction and abduction, and how it offers a natural way of unifying belief revision, belief update, and reasoning about actions.\n",
      "label:Rule_Learning\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "56115 63486\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Improving Tactical Plans with Genetic Algorithms\n",
      "abstract:Abstract:\n",
      "label:Genetic_Algorithms\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Evaluation and Selection of Biases in Machine Learning\n",
      "abstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.\n",
      "label:Theory\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "63486 83461\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Evaluation and Selection of Biases in Machine Learning\n",
      "abstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.\n",
      "label:Theory\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Dlab: A Declarative Language Bias Formalism\n",
      "abstract:Abstract: We describe the principles and functionalities of Dlab (Declarative LAnguage Bias). Dlab can be used in inductive learning systems to define syntactically and traverse efficiently finite subspaces of first order clausal logic, be it a set of propositional formulae, association rules, Horn clauses, or full clauses. A Prolog implementation of Dlab is available by ftp access. Keywords: declarative language bias, concept learning, knowledge dis covery\n",
      "label:Rule_Learning\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "72908 954315\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Applications of a logical discovery engine\n",
      "abstract:Abstract: The clausal discovery engine claudien is presented. claudien discovers regularities in data and is a representative of the inductive logic programming paradigm. As such, it represents data and regularities by means of first order clausal theories. Because the search space of clausal theories is larger than that of attribute value representation, claudien also accepts as input a declarative specification of the language bias, which determines the set of syntactically well-formed regularities. Whereas other papers on claudien focuss on the semantics or logical problem specification of claudien, on the discovery algorithm, or the PAC-learning aspects, this paper wants to illustrate the power of the resulting technique. In order to achieve this aim, we show how claudien can be used to learn 1) integrity constraints in databases, 2) functional dependencies and determinations, 3) properties of sequences, 4) mixed quantitative and qualitative laws, 5) reverse engineering, and 6) classification rules.\n",
      "label:Rule_Learning\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Inductive Database Design\n",
      "abstract:Abstract: When designing a (deductive) database, the designer has to decide for each predicate (or relation) whether it should be defined extensionally or intensionally, and what the definition should look like. An intelligent system is presented to assist the designer in this task. It starts from an example database in which all predicates are defined extensionally. It then tries to compact the database by transforming extensionally defined predicates into intensionally defined ones. The intelligent system employs techniques from the area of inductive logic programming.\n",
      "label:Rule_Learning\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "83461 954315\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Dlab: A Declarative Language Bias Formalism\n",
      "abstract:Abstract: We describe the principles and functionalities of Dlab (Declarative LAnguage Bias). Dlab can be used in inductive learning systems to define syntactically and traverse efficiently finite subspaces of first order clausal logic, be it a set of propositional formulae, association rules, Horn clauses, or full clauses. A Prolog implementation of Dlab is available by ftp access. Keywords: declarative language bias, concept learning, knowledge dis covery\n",
      "label:Rule_Learning\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Inductive Database Design\n",
      "abstract:Abstract: When designing a (deductive) database, the designer has to decide for each predicate (or relation) whether it should be defined extensionally or intensionally, and what the definition should look like. An intelligent system is presented to assist the designer in this task. It starts from an example database in which all predicates are defined extensionally. It then tries to compact the database by transforming extensionally defined predicates into intensionally defined ones. The intelligent system employs techniques from the area of inductive logic programming.\n",
      "label:Rule_Learning\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n",
      "\n",
      "144330 63486\n",
      "given two papers and their citation relationship information:\n",
      "paper1:\n",
      "title:Title: Utilizing Prior Concepts for Learning\n",
      "abstract:Abstract: The inductive learning problem consists of learning a concept given examples and non-examples of the concept. To perform this learning task, inductive learning algorithms bias their learning method. Here we discuss biasing the learning method to use previously learned concepts from the same domain. These learned concepts highlight useful information for other concepts in the domain. We describe a transference bias and present M-FOCL, a Horn clause relational learning algorithm, that utilizes this bias to learn multiple concepts. We provide preliminary empirical evaluation to show the effects of biasing previous information on noise-free and noisy data.\n",
      "label:Theory\n",
      "cites\n",
      "paper2:\n",
      "title:Title: Evaluation and Selection of Biases in Machine Learning\n",
      "abstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.\n",
      "label:Theory\n",
      "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
      "    \n",
      "\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "# w(x, r, v)\n",
    "# cite, src, tar\n",
    "w_list_orig = []\n",
    "for i in range(len(cite)):\n",
    "    src_id, tar_id = cite.loc[i, 'src_id'], cite.loc[i, 'tar_id']\n",
    "\n",
    "    if src.pid.values[0] == src_id and tar.pid.values[0] == tar_id:\n",
    "        # print(src_id, tar_id)\n",
    "        continue\n",
    "    \n",
    "    src_title, src_abs, src_label = sample[sample.pid == src_id].title.values[0], sample[sample.pid == src_id]['abs'].values[0], sample[sample.pid == src_id].label.values[0]\n",
    "    tar_title, tar_abs, tar_label = sample[sample.pid == tar_id].title.values[0], sample[sample.pid == tar_id]['abs'].values[0], sample[sample.pid == tar_id].label.values[0]\n",
    "    \n",
    "\n",
    "    w_prompt = f\"\"\"\n",
    "{src_id} {tar_id}\n",
    "given two papers and their citation relationship information:\n",
    "paper1:\n",
    "title:{src_title}\n",
    "abstract:{src_abs}\n",
    "label:{src_label}\n",
    "cites\n",
    "paper2:\n",
    "title:{tar_title}\n",
    "abstract:{tar_abs}\n",
    "label:{tar_label}\n",
    "please summarize the citation relationship in no more than three sentences and then generate some keywords of this information.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    w_list_orig.append(w_prompt)\n",
    "\n",
    "for w in w_list_orig:\n",
    "    print(w)\n",
    "    print('=========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{63486: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" introduces the Oblivion algorithm for feature selection in case-based learning amidst irrelevant features, focusing on parity concepts. It presents experimental results on artificial and natural domains, discussing implications and future research directions. \"Evaluation and Selection of Biases in Machine Learning\" defines bias in ML systems, advocates for automated bias evaluation, selection methods, and summarizes recent research in the field.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature SelectionExperimental ResultsBias in Machine LearningAutomated Bias EvaluationBias SelectionMeta-bias SpacesResearch Summary'],\n",
       "  'w': ['paper1:title:Title: Irrelevant Features and the Subset Selection Problemabstract:Abstract: We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.label:Theorycitespaper2:title:Title: Evaluation and Selection of Biases in Machine Learningabstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.label:Theory',\n",
       "   'paper1:title:Title: #1 Robust Feature Selection Algorithmsabstract:Abstract: Selecting a set of features which is optimal for a given task is a problem which plays an important role in a wide variety of contexts including pattern recognition, adaptive control, and machine learning. Our experience with traditional feature selection algorithms in the domain of machine learning lead to an appreciation for their computational efficiency and a concern for their brittleness. This paper describes an alternate approach to feature selection which uses genetic algorithms as the primary search component. Results are presented which suggest that genetic algorithms can be used to increase the robustness of feature selection algorithms without a significant decrease in computational efficiency.label:Genetic_Algorithmscitespaper2:title:Title: Evaluation and Selection of Biases in Machine Learningabstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.label:Theory',\n",
       "   'paper1:title:Title: Improving Tactical Plans with Genetic Algorithmsabstract:Abstract:label:Genetic_Algorithmscitespaper2:title:Title: Evaluation and Selection of Biases in Machine Learningabstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.label:Theory',\n",
       "   'paper1:title:Title: Utilizing Prior Concepts for Learningabstract:Abstract: The inductive learning problem consists of learning a concept given examples and non-examples of the concept. To perform this learning task, inductive learning algorithms bias their learning method. Here we discuss biasing the learning method to use previously learned concepts from the same domain. These learned concepts highlight useful information for other concepts in the domain. We describe a transference bias and present M-FOCL, a Horn clause relational learning algorithm, that utilizes this bias to learn multiple concepts. We provide preliminary empirical evaluation to show the effects of biasing previous information on noise-free and noisy data.label:Theorycitespaper2:title:Title: Evaluation and Selection of Biases in Machine Learningabstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.label:Theory']},\n",
       " 2440: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" focuses on an algorithm, Oblivion, for feature selection in case-based learning amid irrelevant features, particularly emphasizing parity concepts. It presents experimental outcomes on artificial and natural domains, discussing implications and future research directions. \"Quinlan, 1990 J.R. Quinlan. Learning logical definitions from relations\" introduces a ranked-model semantics for if-then rules with exceptions, addressing evidential and causal reasoning by extracting rule priorities and incorporating the principle of Markov shielding, resolving issues in specificity, prediction, abduction, and unifying belief-related processes.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature SelectionExperimental ResultsRanked-model SemanticsIf-Then Rules with ExceptionsEvidential ReasoningCausal ReasoningRule PrioritiesMarkov ShieldingBelief RevisionBelief UpdateReasoning about Actions'],\n",
       "  'w': ['paper1:title:Title: Learning Approximate Control Rules Of High Utilityabstract:Abstract: One of the difficult problems in the area of explanation based learning is the utility problem; learning too many rules of low utility can lead to swamping, or degradation of performance. This paper introduces two new techniques for improving the utility of learned rules. The first technique is to combine EBL with inductive learning techniques to learn a better set of control rules; the second technique is to use these inductive techniques to learn approximate control rules. The two techniques are synthesized in an algorithm called approximating abductive explanation based learning (AxA-EBL). AxA-EBL is shown to improve substantially over standard EBL in several domains.label:Case_Basedcitespaper2:title:Title: Quinlan, 1990 J.R. Quinlan. Learning logical definitions from relations. Machine Learning, First-order theory revision. Inabstract:Abstract: We describe a ranked-model semantics for if-then rules admitting exceptions, which provides a coherent framework for many facets of evidential and causal reasoning. Rule priorities are automatically extracted form the knowledge base to facilitate the construction and retraction of plausible beliefs. To represent causation, the formalism incorporates the principle of Markov shielding which imposes a stratified set of independence constraints on rankings of interpretations. We show how this formalism resolves some classical problems associated with specificity, prediction and abduction, and how it offers a natural way of unifying belief revision, belief update, and reasoning about actions.label:Rule_Learning']},\n",
       " 3231: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" introduces the Oblivion algorithm for feature selection in case-based learning amid irrelevant features, focusing on parity concepts. The paper discusses experimental results on artificial and natural domains, outlining implications and future research directions. \"Irrelevant Features and the Subset Selection Problem\" explores defining relevance and irrelevance in feature subsets for supervised induction algorithms, proposing improved categorization methods. It presents a feature selection method using cross-validation applicable to various induction algorithms, showcasing experiments with ID3 and C4.5 on artificial and real datasets.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature SelectionExperimental ResultsSubset Selection ProblemSupervised Induction AlgorithmsRelevance DefinitionsFeature CategorizationCross-validationID3C4.5'],\n",
       "  'w': ['paper1:title:Title: Learning Boolean Concepts in the Presence of Many Irrelevant Featuresabstract:Abstract: In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.label:Theorycitespaper2:title:Title: Irrelevant Features and the Subset Selection Problemabstract:Abstract: We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.label:Theory']},\n",
       " 954315: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" introduces the Oblivion algorithm for case-based learning amidst irrelevant features, emphasizing the efficient identification of relevant features, especially in parity concepts. The paper discusses experimental results on artificial and natural domains and suggests future research directions. \"Inductive Database Design\" proposes an intelligent system aiding in the design of deductive databases by transforming extensionally defined predicates into intensionally defined ones, employing techniques from inductive logic programming.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature IdentificationExperimental ResultsDatabase DesignDeductive DatabasesExtensional DefinitionsIntensional DefinitionsInductive Logic ProgrammingIntelligent Systems'],\n",
       "  'w': ['paper1:title:Title: Applications of a logical discovery engineabstract:Abstract: The clausal discovery engine claudien is presented. claudien discovers regularities in data and is a representative of the inductive logic programming paradigm. As such, it represents data and regularities by means of first order clausal theories. Because the search space of clausal theories is larger than that of attribute value representation, claudien also accepts as input a declarative specification of the language bias, which determines the set of syntactically well-formed regularities. Whereas other papers on claudien focuss on the semantics or logical problem specification of claudien, on the discovery algorithm, or the PAC-learning aspects, this paper wants to illustrate the power of the resulting technique. In order to achieve this aim, we show how claudien can be used to learn 1) integrity constraints in databases, 2) functional dependencies and determinations, 3) properties of sequences, 4) mixed quantitative and qualitative laws, 5) reverse engineering, and 6) classification rules.label:Rule_Learningcitespaper2:title:Title: Inductive Database Designabstract:Abstract: When designing a (deductive) database, the designer has to decide for each predicate (or relation) whether it should be defined extensionally or intensionally, and what the definition should look like. An intelligent system is presented to assist the designer in this task. It starts from an example database in which all predicates are defined extensionally. It then tries to compact the database by transforming extensionally defined predicates into intensionally defined ones. The intelligent system employs techniques from the area of inductive logic programming.label:Rule_Learning',\n",
       "   'paper1:title:Title: Dlab: A Declarative Language Bias Formalismabstract:Abstract: We describe the principles and functionalities of Dlab (Declarative LAnguage Bias). Dlab can be used in inductive learning systems to define syntactically and traverse efficiently finite subspaces of first order clausal logic, be it a set of propositional formulae, association rules, Horn clauses, or full clauses. A Prolog implementation of Dlab is available by ftp access. Keywords: declarative language bias, concept learning, knowledge dis coverylabel:Rule_Learningcitespaper2:title:Title: Inductive Database Designabstract:Abstract: When designing a (deductive) database, the designer has to decide for each predicate (or relation) whether it should be defined extensionally or intensionally, and what the definition should look like. An intelligent system is presented to assist the designer in this task. It starts from an example database in which all predicates are defined extensionally. It then tries to compact the database by transforming extensionally defined predicates into intensionally defined ones. The intelligent system employs techniques from the area of inductive logic programming.label:Rule_Learning']},\n",
       " 144330: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" introduces the Oblivion algorithm for efficient feature identification in case-based learning amid irrelevant features, emphasizing the interaction of relevant features, particularly in parity concepts. The paper presents experimental results on artificial and natural domains, discussing implications and future research avenues. \"Utilizing Prior Concepts for Learning\" discusses biasing learning algorithms with previously acquired concepts within the same domain, introducing a transference bias and M-FOCL, a Horn clause relational learning algorithm that leverages this bias for learning multiple concepts. It showcases preliminary empirical evaluations on noise-free and noisy data to demonstrate the impact of biasing previous information.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature IdentificationExperimental ResultsPrior ConceptsInductive LearningLearning AlgorithmsTransference BiasM-FOCL AlgorithmHorn Clause Relational LearningDomain Knowledge Bias'],\n",
       "  'w': []},\n",
       " 56115: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" addresses case-based learning amidst irrelevant features, introducing the Oblivion algorithm for feature identification, especially in interacting scenarios like parity concepts. It reports experimental outcomes on artificial and natural domains, discussing implications and future research directions. \"Improving Tactical Plans with Genetic Algorithms\" lacks an abstract or information for summary.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature IdentificationExperimental ResultsGenetic AlgorithmsTactical PlansOptimizationEvolutionary Algorithms'],\n",
       "  'w': []},\n",
       " 72908: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" focuses on case-based learning amidst irrelevant features, introducing the Oblivion algorithm for effective feature identification, particularly in parity concepts. It reports experimental outcomes on artificial and natural domains, discussing implications and future research directions. \"Applications of a logical discovery engine\" presents claudien, an inductive logic programming paradigm for discovering regularities in data using first-order clausal theories. The paper illustrates claudien\\'s capabilities in learning integrity constraints, functional dependencies, sequences, mixed quantitative and qualitative laws, reverse engineering, and classification rules.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature IdentificationExperimental ResultsLogical Discovery EngineInductive Logic ProgrammingFirst-order Clausal TheoriesIntegrity ConstraintsFunctional DependenciesReverse EngineeringClassification Rules'],\n",
       "  'w': ['paper1:title:Title: Quinlan, 1990 J.R. Quinlan. Learning logical definitions from relations. Machine Learning, First-order theory revision. Inabstract:Abstract: We describe a ranked-model semantics for if-then rules admitting exceptions, which provides a coherent framework for many facets of evidential and causal reasoning. Rule priorities are automatically extracted form the knowledge base to facilitate the construction and retraction of plausible beliefs. To represent causation, the formalism incorporates the principle of Markov shielding which imposes a stratified set of independence constraints on rankings of interpretations. We show how this formalism resolves some classical problems associated with specificity, prediction and abduction, and how it offers a natural way of unifying belief revision, belief update, and reasoning about actions.label:Rule_Learningcitespaper2:title:Title: Applications of a logical discovery engineabstract:Abstract: The clausal discovery engine claudien is presented. claudien discovers regularities in data and is a representative of the inductive logic programming paradigm. As such, it represents data and regularities by means of first order clausal theories. Because the search space of clausal theories is larger than that of attribute value representation, claudien also accepts as input a declarative specification of the language bias, which determines the set of syntactically well-formed regularities. Whereas other papers on claudien focuss on the semantics or logical problem specification of claudien, on the discovery algorithm, or the PAC-learning aspects, this paper wants to illustrate the power of the resulting technique. In order to achieve this aim, we show how claudien can be used to learn 1) integrity constraints in databases, 2) functional dependencies and determinations, 3) properties of sequences, 4) mixed quantitative and qualitative laws, 5) reverse engineering, and 6) classification rules.label:Rule_Learning']},\n",
       " 83461: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" introduces the Oblivion algorithm, addressing case-based learning amidst irrelevant features by employing greedy pruning of oblivious decision trees. The paper discusses experimental results on artificial and natural domains, highlighting improved feature identification in specific cases and outlines future research directions. \"Dlab: A Declarative Language Bias Formalism\" details Dlab\\'s functionalities in inductive learning systems, enabling the definition and efficient traversal of finite subspaces of first-order clausal logic. It emphasizes syntactic definition, traversal efficiency, and offers a Prolog implementation.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature IdentificationExperimental ResultsDeclarative Language BiasInductive Learning SystemsFirst-order Clausal LogicSubspace DefinitionTraversal EfficiencyProlog Implementation'],\n",
       "  'w': ['paper1:title:Title: Evaluation and Selection of Biases in Machine Learningabstract:Abstract: In this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.label:Theorycitespaper2:title:Title: Dlab: A Declarative Language Bias Formalismabstract:Abstract: We describe the principles and functionalities of Dlab (Declarative LAnguage Bias). Dlab can be used in inductive learning systems to define syntactically and traverse efficiently finite subspaces of first order clausal logic, be it a set of propositional formulae, association rules, Horn clauses, or full clauses. A Prolog implementation of Dlab is available by ftp access. Keywords: declarative language bias, concept learning, knowledge dis coverylabel:Rule_Learning']},\n",
       " 17811: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" addresses case-based learning amid irrelevant features, introducing the Oblivion algorithm for efficient feature identification, particularly in scenarios involving interacting relevant features like parity concepts. The paper discusses experimental outcomes on artificial and natural domains and proposes future research paths. \"#1 Robust Feature Selection Algorithms\" highlights the significance of optimal feature selection across pattern recognition, adaptive control, and machine learning, discussing the brittleness of traditional algorithms and proposing a genetic algorithm-based approach for increased robustness without compromising computational efficiency.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature IdentificationExperimental ResultsGenetic AlgorithmsFeature SelectionComputational EfficiencyRobustnessPattern RecognitionAdaptive ControlMachine Learning'],\n",
       "  'w': []},\n",
       " 37483: {'h': ['\"Learning Boolean Concepts in the Presence of Many Irrelevant Features\" introduces the Oblivion algorithm for case-based learning amidst irrelevant features, focusing on efficient feature identification, especially in scenarios involving interacting relevant features like parity concepts. It reports experimental outcomes on artificial and natural domains and proposes future research directions. \"Learning Approximate Control Rules Of High Utility\" addresses the utility problem in explanation-based learning, introducing techniques that combine EBL with inductive learning to improve control rules\\' utility and presenting AxA-EBL, an algorithm that substantially enhances EBL performance across domains.Keywords:Case-based LearningIrrelevant FeaturesOblivion AlgorithmParity ConceptsFeature IdentificationExperimental ResultsExplanation-based LearningUtility ProblemControl RulesInductive Learning TechniquesApproximating Abductive Explanation-based Learning (AxA-EBL)Performance ImprovementHigh Utility Rules'],\n",
       "  'w': []}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# message\n",
    "\n",
    "src_id = src.pid.values[0]\n",
    "tar_id = tar.pid.values[0]\n",
    "\n",
    "message = {}\n",
    "for i in range(len(sample)):\n",
    "\n",
    "    node_id = sample.loc[i, 'pid']\n",
    "\n",
    "    if node_id == src_id:\n",
    "        continue\n",
    "\n",
    "\n",
    "    h0 = sample.loc[i, 'h0']\n",
    "    message[node_id] = {'h': [h0.replace('\\n', '')]}\n",
    "\n",
    "message\n",
    "\n",
    "w\n",
    "for key in message.keys():\n",
    "    x_df = cite[cite.tar_id == key].reset_index(drop = True)\n",
    "    message[key]['w'] = []\n",
    "    for i in range(len(x_df)):\n",
    "        if x_df.loc[i, 'src_id'] == src_id and x_df.loc[i, 'tar_id'] == tar_id:\n",
    "            continue\n",
    "\n",
    "        message[key]['w'].append(x_df.loc[i, 'w'].replace('\\r', '').replace('\\n', ''))\n",
    "\n",
    "\n",
    "message\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7838d3e8c8aafe99a49219d607d1f980c3b3b376b8b5bfa134f72b2f876f82e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
