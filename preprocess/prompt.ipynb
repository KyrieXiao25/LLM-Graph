{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "**Path Embedding**\n",
    "- 一个可解释的，简洁的路径描述\n",
    "- 增量式增加路径和节点的能力\n",
    "\n",
    "\n",
    "**Link Prediction**\n",
    "- 判断path头和path尾是否有连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>new_title</th>\n",
       "      <th>new_abs</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340299</td>\n",
       "      <td>Title: Neural Network Applicability: Classifyi...</td>\n",
       "      <td>Abstract: tremendous effort propose neurally i...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1365</td>\n",
       "      <td>Title: Cholinergic suppression transmission co...</td>\n",
       "      <td>Abstract: Selective suppression transmission f...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34257</td>\n",
       "      <td>Title: Self-Adjusting Dynamic Logic Module</td>\n",
       "      <td>Abstract: ASOCS (Adaptive Self-Organizing Conc...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34263</td>\n",
       "      <td>Title: Word Perfect TRANSFORMATION IMPLEMENTIN...</td>\n",
       "      <td>Abstract: Artificial Neural Networks (ANNs) fi...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34266</td>\n",
       "      <td>Title: VLSI Implementation Parallel, Self-Orga...</td>\n",
       "      <td>Abstract: VLSI implementation Priority Adaptiv...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1122642</td>\n",
       "      <td>Title: Word Perfect LIA: Location-Independent ...</td>\n",
       "      <td>Abstract: Artificial Neural Networks (ANNs) fi...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>87482</td>\n",
       "      <td>Title: Multi-Chip Module Implementation Neural...</td>\n",
       "      <td>Abstract: requirement dense interconnect artif...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74427</td>\n",
       "      <td>Title: Self-Organizing Binary Decision Tree In...</td>\n",
       "      <td>Abstract: ASOCS (adaptive self-organizing conc...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45533</td>\n",
       "      <td>Title: Priority ASOCS ASOCS models two advanta...</td>\n",
       "      <td>Abstract: ASOCS (Adaptive Self-Organizing Conc...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>213246</td>\n",
       "      <td>Title: Growing Layers Perceptrons: Introducing...</td>\n",
       "      <td>Abstract: vations perceptrons: (1) perceptron ...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90655</td>\n",
       "      <td>Title: Word Perfect TRANSFORMATION IMPLEMENTIN...</td>\n",
       "      <td>Abstract: Artificial Neural Networks (ANNs) fi...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42156</td>\n",
       "      <td>Title: Models Parallel Adaptive Logic</td>\n",
       "      <td>Abstract: overviews architecture adaptive para...</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pid                                          new_title  \\\n",
       "0    340299  Title: Neural Network Applicability: Classifyi...   \n",
       "1      1365  Title: Cholinergic suppression transmission co...   \n",
       "2     34257         Title: Self-Adjusting Dynamic Logic Module   \n",
       "3     34263  Title: Word Perfect TRANSFORMATION IMPLEMENTIN...   \n",
       "4     34266  Title: VLSI Implementation Parallel, Self-Orga...   \n",
       "5   1122642  Title: Word Perfect LIA: Location-Independent ...   \n",
       "6     87482  Title: Multi-Chip Module Implementation Neural...   \n",
       "7     74427  Title: Self-Organizing Binary Decision Tree In...   \n",
       "8     45533  Title: Priority ASOCS ASOCS models two advanta...   \n",
       "9    213246  Title: Growing Layers Perceptrons: Introducing...   \n",
       "10    90655  Title: Word Perfect TRANSFORMATION IMPLEMENTIN...   \n",
       "11    42156              Title: Models Parallel Adaptive Logic   \n",
       "\n",
       "                                              new_abs            label  \n",
       "0   Abstract: tremendous effort propose neurally i...  Neural_Networks  \n",
       "1   Abstract: Selective suppression transmission f...  Neural_Networks  \n",
       "2   Abstract: ASOCS (Adaptive Self-Organizing Conc...  Neural_Networks  \n",
       "3   Abstract: Artificial Neural Networks (ANNs) fi...  Neural_Networks  \n",
       "4   Abstract: VLSI implementation Priority Adaptiv...  Neural_Networks  \n",
       "5   Abstract: Artificial Neural Networks (ANNs) fi...  Neural_Networks  \n",
       "6   Abstract: requirement dense interconnect artif...  Neural_Networks  \n",
       "7   Abstract: ASOCS (adaptive self-organizing conc...  Neural_Networks  \n",
       "8   Abstract: ASOCS (Adaptive Self-Organizing Conc...  Neural_Networks  \n",
       "9   Abstract: vations perceptrons: (1) perceptron ...  Neural_Networks  \n",
       "10  Abstract: Artificial Neural Networks (ANNs) fi...  Neural_Networks  \n",
       "11  Abstract: overviews architecture adaptive para...  Neural_Networks  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'subgraph'\n",
    "src_list = os.listdir(path)\n",
    "src_node = src_list[0]\n",
    "\n",
    "\n",
    "nodes = pd.read_csv(os.path.join(path, src_node, f'sample_{src_node}.csv'))\n",
    "edges = pd.read_csv(os.path.join(path, src_node, f'edge_{src_node}.csv'))\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](subgraph/1122642/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper format:\n",
    "- title:\n",
    "- problems:\n",
    "- methods:\n",
    "- discovery:\n",
    "- keywords\n",
    "\n",
    "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个思路，单纯做一条路径的文本表示，不去管其他邻居\n",
    "# query path [1122642, 34257, 87482, 34263, 34266]\n",
    "\n",
    "# 第二个思路，对于每个节点，嵌入他们社群子图的信息，然后做路径表示，然后做LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340299\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Neural Network Applicability: Classifying Space\n",
      "abstract: Abstract: tremendous effort propose neurally inspired methods computation forces closer scrutiny potential models. categorizes applications classes discusses features applications efficiently amenable neural network methods. Computational machines deterministic mappings inputs outputs computational mechanisms solutions. Neural network features parallel execution, adaptive learning, generalization, fault tolerance. Often, effort model applications implemented efficient alternate technology. Neural networks powerful devices classes applications, all. However, class applications neural networks efficient commonly occurring nature. Comparison supervised, unsupervised, generalizing systems included.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "1365\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Cholinergic suppression transmission combined associative memory function self-organization neocortex.\n",
      "abstract: Abstract: Selective suppression transmission feedback synapses learning mechanism combining associative feedback self-organization feedforward synapses. Experimental data demonstrates cholinergic suppression synaptic transmission layer (feedback synapses), suppression layer (feed-forward synapses). network feature local rules learn mappings linearly separable. learning, sensory stimuli desired response simultaneously input. Feedforward connections form self-organized representations input, suppressed feedback connections learn transpose feedfor-ward connectivity. recall, suppression removed, sensory input activates self-organized representation, activity generates learned response.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "34257\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Self-Adjusting Dynamic Logic Module\n",
      "abstract: Abstract: ASOCS (Adaptive Self-Organizing Concurrent System) model massively parallel processing incrementally defined rule systems adaptive logic, robotics, logical inference, dynamic control. ASOCS adaptive network composed computing elements operating asynchronously parallel. focuses Adaptive Algorithm (AA2) details architecture learning algorithm. AA2 memory knowledge maintenance advantages ASOCS models. ASOCS operate data processing mode learning mode. learning mode, ASOCS rule expressed boolean conjunction. AA2 learning algorithm incorporates rule distributed fashion short, bounded time. data processing mode, ASOCS acts parallel hardware circuit.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "34263\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Word Perfect TRANSFORMATION IMPLEMENTING EFFICIENT DYNAMIC BACKPROPAGATION NEURAL NETWORKS\n",
      "abstract: Abstract: Artificial Neural Networks (ANNs) fixed topology learning, suffer shortcomings result. Variations ANNs dynamic topologies ability overcome problems. introduces Location-Independent Transformations (LITs) strategy implementing distributed feedforward networks dynamic topologies (dynamic ANNs) efficiently parallel hardware. LIT creates location-independent nodes, node computes network output independent nodes, local information. transformation efficient port adding deleting nodes dynamically learning. particular, LIT standard Backpropagation two layers weights, dynamic extensions Backpropagation supported.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "34266\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: VLSI Implementation Parallel, Self-Organizing Learning Model\n",
      "abstract: Abstract: VLSI implementation Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model built multi-chip module (MCM) substrate. hardware implementations neural network learning models direct implementations classical neural network structures|a computing nodes connected dense weighted links. PASOCS one class ASOCS (Adaptive Self-Organizing Concurrent System) connectionist models goal classical neural networks models, functional mechanisms significantly. model potential pattern recognition, robotics, logical inference, dynamic control.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "1122642\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm\n",
      "abstract: Abstract: Artificial Neural Networks (ANNs) fixed topology learning, suffer shortcomings result. ANNs dynamic topologies ability overcome problems. Adaptive Organizing Concurrent Systems (ASOCS) class learning models inherently dynamic topologies. introduces Location-Independent Transformations (LITs) strategy implementing learning models dynamic topologies efficiently parallel hardware. LIT creates location-independent nodes, node computes network output independent nodes, local information. transformation efficient support adding deleting nodes dynamically learning. particular, Location Independent ASOCS (LIA) model LIT ASOCS Adaptive Algorithm 2. description LIA formal definitions LIA algorithms. LIA implements basic ASOCS mechanisms, definitions formal description basic ASOCS mechanisms general, addition LIA.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "87482\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Multi-Chip Module Implementation Neural Network\n",
      "abstract: Abstract: requirement dense interconnect artificial neural network systems researchers high-density interconnect technologies. reports implementation multi-chip modules (MCMs) interconnect medium. specific self-organizing, parallel, dynamic learning model dense interconnect technology effective implementation; requirement fulfilled exploiting MCM technology. ideas MCM implementation artificial neural networks versatile adapted neural network connectionist models.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "74427\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Self-Organizing Binary Decision Tree Incrementally Defined Rule\n",
      "abstract: Abstract: ASOCS (adaptive self-organizing concurrent system) model massively parallel processing incrementally defined rule systems adaptive logic, robotics, logical inference, dynamic control. ASOCS adaptive network composed computing elements operating asynchronously parallel. focuses adaptive algorithm (AA3) details architecture learning algorithm. advantages ASOCS models simplicity, implementability, cost. ASOCS operate data processing mode learning mode. data processing mode, ASOCS acts parallel hardware circuit. learning mode, rules expressed boolean conjunctions incrementally ASOCS. ASOCS learning algorithms incorporate rule distributed fashion short, bounded time.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "45533\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Priority ASOCS ASOCS models two advantages learning models:\n",
      "abstract: Abstract: ASOCS (Adaptive Self-Organizing Concurrent System) model massively parallel processing incrementally defined rule systems adaptive logic, robotics, logical inference, dynamic control. ASOCS adaptive network composed computing elements operating asynchronously parallel. ASOCS operate data processing mode learning mode. data processing mode, ASOCS acts parallel hardware circuit. learning mode, ASOCS incorporates rule expressed Boolean conjunction distributed fashion logarithmic rules. proposes learning algorithm architecture Priority ASOCS. ASOCS model rules priorities. model learning space complexity improvements models. Non-von Neumann architectures neural networks attack word-at-a-time bottleneck traditional computing systems [1]. Neural networks learn input-output mappings highly distributed processing memory [10,11,12]. numerous processing elements modifiable weighted permit degree parallelism. typical neural network fixed topology. learns modifying weighted nodes. class connectionist architectures ASOCS (Adaptive Self-Organizing Concurrent Systems) [4,5]. ASOCS models support efficient computation self-organized learning parallel execution. Learning incremental presentation rules examples. ASOCS models learn modifying topology. Data Boolean multi-state variables; models support analog variables. model incorporates rules adaptive logic network parallel organizing fashion. processing mode, ASOCS supports parallel execution inputs learned rules. adaptive logic network acts parallel hardware circuit execution, mapping input boolean vectors output boolean vectors, combinatoric fashion. philosophy ASOCS level goals neural network models. However, mechanisms learning execution vary significantly. ASOCS logic network topologically dynamic network growing efficiently fit specific application. ASOCS models digital nodes. ASOCS supports symbolic heuristic learning mechanisms, combining parallelism distributed nature connectionist computing potential power symbolic learning. proof ASOCS chip developed [2].\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "213246\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Growing Layers Perceptrons: Introducing Extentron Algorithm\n",
      "abstract: Abstract: vations perceptrons: (1) perceptron learning algorithm cycles hyperplanes, hyperplanes select one split examples, (2) perceptron build hyper- plane separates one rest. Extentron grows multi-layer networks capable distinguishing non- linearly-separable data perceptron rule linear threshold units. algorithm simple, fast, scales prob lems, retains convergence properties perceptron, two parameters. comparing Extentron neural network paradigms symbolic learning systems.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "90655\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Word Perfect TRANSFORMATION IMPLEMENTING NEURAL NETWORKS LOCALIST PROPERTIES\n",
      "abstract: Abstract: Artificial Neural Networks (ANNs) fixed topology learning, typically suffer shortcomings result. Variations ANNs dynamic topologies ability overcome problems. introduces Location-Independent Transformations (LITs) strategy implementing feedforward networks dynamic topologies. LIT creates location-independent nodes, node computes network output independent nodes, local information. transformation efficient support adding deleting nodes dynamically learning. particular, LITs single-layer competitve learning network, counterpropagation network, combines elements supervised learning competitive learning. two networks localist sense ultimately one node responsible output. LITs models papers.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n",
      "42156\n",
      "\n",
      "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
      "title:\n",
      "problems:\n",
      "methods:\n",
      "discovery:\n",
      "keywords:\n",
      "\n",
      "paper information:\n",
      "title: Title: Models Parallel Adaptive Logic\n",
      "abstract: Abstract: overviews architecture adaptive parallel logic referred ASOCS (Adaptive Self-Organizing Concurrent System). ASOCS adaptive network composed computing elements operate parallel asynchronous fashion. specification if-then rules form boolean conjunctions. Rules incrementally adapts changing rule-base. Adaptation data processing form two separate phases operation. processing acts parallel hardware circuit. adaptation distributed computing elements efficiently exploits parallelism. Adaptation self-organizing fashion linear depth network. summarizes ASOCS overviews three specific architectures.\n",
      "label: Neural_Networks\n",
      "\n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# summary_prompt\n",
    "\n",
    "for i in range(len(nodes)):\n",
    "    pid, title, abstract, label = nodes.loc[i, 'pid'], nodes.loc[i, 'new_title'], nodes.loc[i, 'new_abs'], nodes.loc[i, 'label']\n",
    "\n",
    "    prompt = f'''\n",
    "You are an expert in summarising the information in a paper, now given the title, abstract and labels of the paper, you need to summarise the information in the paper and organise the content into the following format:\n",
    "title:\n",
    "problems:\n",
    "methods:\n",
    "discovery:\n",
    "keywords:\n",
    "\n",
    "paper information:\n",
    "title: {title}\n",
    "abstract: {abstract}\n",
    "label: {label}\n",
    "'''\n",
    "    print(pid)\n",
    "    print(prompt)\n",
    "    print('==============================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 340299\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Neural Network Applicability: Classifying Space\n",
      "problems:\n",
      "\n",
      "Efficient classification of various applications using neural networks.\n",
      "Comparison of neural network-based systems with supervised, unsupervised, and generalizing systems.\n",
      "methods:\n",
      "Utilization of neural network features such as parallel execution, adaptive learning, generalization, and fault tolerance.\n",
      "Categorization of applications into classes and discussion of features amenable to neural network methods.\n",
      "discovery:\n",
      "Neural networks offer powerful computational capabilities for various applications.\n",
      "Efficiency concerns arise in applying neural networks to commonly occurring tasks.\n",
      "keywords: Neural Networks, Classification, Efficiency, Parallel Execution, Adaptive Learning, Generalization, Fault Tolerance, Supervised Learning, Unsupervised Learning, Generalizing Systems.\n",
      "paper B information: \n",
      "title: Cholinergic Suppression of Transmission Combined with Associative Memory Function in Self-Organization of Neocortex\n",
      "problems:\n",
      "\n",
      "Understanding the mechanism of selective suppression of transmission in feedback synapses during learning.\n",
      "Investigating how associative feedback contributes to self-organization in the neocortex.\n",
      "methods:\n",
      "Experimental demonstration of cholinergic suppression of synaptic transmission in specific layers (feedback synapses).\n",
      "Exploration of local learning rules for mapping linearly separable patterns.\n",
      "Study of sensory input and desired response integration in the learning process.\n",
      "discovery:\n",
      "Cholinergic suppression of transmission plays a crucial role in combining associative feedback with self-organization in the neocortex.\n",
      "Feedback synapses are suppressed while feedforward synapses form self-organized representations of input.\n",
      "Sensory input triggers the recall of learned responses when suppression is removed.\n",
      "keywords: Cholinergic Suppression, Transmission, Associative Memory, Self-Organization, Neocortex, Feedback Synapses, Feedforward Synapses, Learning Mechanism, Sensory Input, Recall.\n",
      "\n",
      "================================================================================================================\n",
      "34257 1122642\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm\n",
      "problems:\n",
      "\n",
      "Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for efficiently implementing dynamic topologies in neural network learning models on parallel hardware.\n",
      "Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and support dynamic addition and deletion during learning.\n",
      "Presenting the Location Independent ASOCS (LIA) model, which incorporates LITs into the ASOCS Adaptive Algorithm 2, and providing formal definitions and descriptions of LIA algorithms.\n",
      "discovery:\n",
      "LIA model enhances ASOCS mechanisms by employing LITs, enabling dynamic topology adjustments in neural network learning models.\n",
      "The incorporation of LITs into ASOCS facilitates efficient learning and supports dynamic addition and deletion of nodes during the learning process.\n",
      "LIA offers a flexible and efficient approach for implementing dynamic topologies in neural networks, potentially improving their performance and adaptability.\n",
      "keywords: Word Perfect LIA, Location-Independent Transformation, ASOCS, Adaptive Algorithm, Dynamic Topologies, Neural Networks, LITs, Parallel Hardware.\n",
      "paper B information: \n",
      "title: Self-Adjusting Dynamic Logic Module\n",
      "problems:\n",
      "\n",
      "Developing a self-adjusting dynamic logic module for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "methods:\n",
      "Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "Implementing the Adaptive Algorithm (AA2) for learning and memory knowledge maintenance within ASOCS.\n",
      "Operating ASOCS in both learning mode, where rules are expressed as boolean conjunctions, and data processing mode, where it acts as a parallel hardware circuit.\n",
      "discovery:\n",
      "The ASOCS model offers advantages in adaptive logic systems, robotics, logical inference, and dynamic control due to its parallel processing capabilities and adaptive nature.\n",
      "The AA2 learning algorithm enables distributed learning of rules in a short, bounded time frame, enhancing the efficiency of ASOCS models.\n",
      "keywords: Self-Adjusting, Dynamic Logic Module, ASOCS, Adaptive Self-Organizing Concurrent System, Adaptive Algorithm, Massively Parallel Processing, Incremental Rule Systems, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "34257 34263\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect: Implementing Efficient Dynamic Backpropagation Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the shortcomings of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming limitations through dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for implementing distributed feedforward networks with dynamic topologies efficiently on parallel hardware.\n",
      "Utilizing LITs to create location-independent nodes that compute network output using local information, allowing for efficient addition and deletion of nodes during learning.\n",
      "Extending standard Backpropagation with LITs to support dynamic topology adjustments.\n",
      "discovery:\n",
      "LITs offer a method to implement dynamic topologies in feedforward neural networks efficiently.\n",
      "The approach enables the addition and deletion of nodes during learning, addressing the limitations of fixed-topology ANNs.\n",
      "LITs enhance Backpropagation by supporting dynamic extensions, improving adaptability and performance.\n",
      "keywords: Word Perfect, Dynamic Backpropagation, Neural Networks, Location-Independent Transformations, Dynamic Topologies, Fixed Topology, Parallel Hardware, Feedforward Networks.\n",
      "paper B information: \n",
      "title: Self-Adjusting Dynamic Logic Module\n",
      "problems:\n",
      "\n",
      "Developing a self-adjusting dynamic logic module for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "methods:\n",
      "Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "Implementing the Adaptive Algorithm (AA2) for learning and memory knowledge maintenance within ASOCS.\n",
      "Operating ASOCS in both learning mode, where rules are expressed as boolean conjunctions, and data processing mode, where it acts as a parallel hardware circuit.\n",
      "discovery:\n",
      "The ASOCS model offers advantages in adaptive logic systems, robotics, logical inference, and dynamic control due to its parallel processing capabilities and adaptive nature.\n",
      "The AA2 learning algorithm enables distributed learning of rules in a short, bounded time frame, enhancing the efficiency of ASOCS models.\n",
      "keywords: Self-Adjusting, Dynamic Logic Module, ASOCS, Adaptive Self-Organizing Concurrent System, Adaptive Algorithm, Massively Parallel Processing, Incremental Rule Systems, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "34257 34266\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: VLSI Implementation of a Parallel, Self-Organizing Learning Model\n",
      "problems:\n",
      "\n",
      "Developing efficient hardware implementations for neural network learning models.\n",
      "Integrating classical neural network structures into multi-chip module (MCM) substrates.\n",
      "methods:\n",
      "Implementing the Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model on VLSI chips.\n",
      "Utilizing dense weighted links to connect computing nodes in the neural network.\n",
      "Designing PASOCS as a class of Adaptive Self-Organizing Concurrent System (ASOCS) models, aimed at enhancing functional mechanisms compared to classical neural networks.\n",
      "discovery:\n",
      "The PASOCS learning model, implemented on VLSI chips, offers potential applications in pattern recognition, robotics, logical inference, and dynamic control.\n",
      "Hardware implementations of neural network learning models provide efficient solutions for parallel, self-organizing learning tasks.\n",
      "keywords: VLSI Implementation, Parallel Learning Model, Self-Organizing, Neural Networks, Multi-Chip Module (MCM), Priority Adaptive Self-Organizing Concurrent (PASOCS), ASOCS, Pattern Recognition, Robotics, Logical Inference, Dynamic Control.\n",
      "paper B information: \n",
      "title: Self-Adjusting Dynamic Logic Module\n",
      "problems:\n",
      "\n",
      "Developing a self-adjusting dynamic logic module for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "methods:\n",
      "Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "Implementing the Adaptive Algorithm (AA2) for learning and memory knowledge maintenance within ASOCS.\n",
      "Operating ASOCS in both learning mode, where rules are expressed as boolean conjunctions, and data processing mode, where it acts as a parallel hardware circuit.\n",
      "discovery:\n",
      "The ASOCS model offers advantages in adaptive logic systems, robotics, logical inference, and dynamic control due to its parallel processing capabilities and adaptive nature.\n",
      "The AA2 learning algorithm enables distributed learning of rules in a short, bounded time frame, enhancing the efficiency of ASOCS models.\n",
      "keywords: Self-Adjusting, Dynamic Logic Module, ASOCS, Adaptive Self-Organizing Concurrent System, Adaptive Algorithm, Massively Parallel Processing, Incremental Rule Systems, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "34257 87482\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Multi-Chip Module Implementation of Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the requirement for dense interconnects in artificial neural network systems.\n",
      "Achieving high-density interconnects using advanced interconnect technologies.\n",
      "methods:\n",
      "Implementing multi-chip modules (MCMs) as an interconnect medium for neural network systems.\n",
      "Utilizing specific self-organizing, parallel, dynamic learning models that are well-suited for dense interconnect technology.\n",
      "Exploiting MCM technology to fulfill the requirements of dense interconnects in neural network implementations.\n",
      "discovery:\n",
      "MCM implementation provides an effective solution for achieving dense interconnects in neural network systems.\n",
      "The proposed approach enables the implementation of versatile neural network connectionist models.\n",
      "By leveraging MCM technology, researchers can adapt neural network architectures to meet various application requirements.\n",
      "keywords: Multi-Chip Module, Neural Network, Dense Interconnect, Self-Organizing Learning, Parallel Learning, Dynamic Learning, Interconnect Technology.\n",
      "paper B information: \n",
      "title: Self-Adjusting Dynamic Logic Module\n",
      "problems:\n",
      "\n",
      "Developing a self-adjusting dynamic logic module for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "methods:\n",
      "Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "Implementing the Adaptive Algorithm (AA2) for learning and memory knowledge maintenance within ASOCS.\n",
      "Operating ASOCS in both learning mode, where rules are expressed as boolean conjunctions, and data processing mode, where it acts as a parallel hardware circuit.\n",
      "discovery:\n",
      "The ASOCS model offers advantages in adaptive logic systems, robotics, logical inference, and dynamic control due to its parallel processing capabilities and adaptive nature.\n",
      "The AA2 learning algorithm enables distributed learning of rules in a short, bounded time frame, enhancing the efficiency of ASOCS models.\n",
      "keywords: Self-Adjusting, Dynamic Logic Module, ASOCS, Adaptive Self-Organizing Concurrent System, Adaptive Algorithm, Massively Parallel Processing, Incremental Rule Systems, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "34257 90655\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect Transformation: Implementing Neural Networks with Localist Properties\n",
      "problems:\n",
      "- Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "- Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "- Introducing Location-Independent Transformations (LITs) as a strategy for implementing feedforward networks with dynamic topologies.\n",
      "- Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and supporting dynamic addition and deletion of nodes during learning.\n",
      "- Implementing LITs in single-layer competitive learning networks and counterpropagation networks, combining elements of supervised learning and competitive learning to achieve localist properties.\n",
      "discovery:\n",
      "- LITs facilitate the creation of neural networks with localist properties by combining elements of supervised and competitive learning.\n",
      "- These networks utilize dynamic topologies and employ location-independent nodes to improve efficiency and adaptability.\n",
      "- The integration of LITs enhances the capabilities of neural networks for various applications.\n",
      "keywords: Word Perfect Transformation, Neural Networks, Localist Properties, Location-Independent Transformations, Dynamic Topologies, Competitive Learning, Counterpropagation Networks, Supervised Learning.\n",
      "paper B information: \n",
      "title: Self-Adjusting Dynamic Logic Module\n",
      "problems:\n",
      "\n",
      "Developing a self-adjusting dynamic logic module for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "methods:\n",
      "Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "Implementing the Adaptive Algorithm (AA2) for learning and memory knowledge maintenance within ASOCS.\n",
      "Operating ASOCS in both learning mode, where rules are expressed as boolean conjunctions, and data processing mode, where it acts as a parallel hardware circuit.\n",
      "discovery:\n",
      "The ASOCS model offers advantages in adaptive logic systems, robotics, logical inference, and dynamic control due to its parallel processing capabilities and adaptive nature.\n",
      "The AA2 learning algorithm enables distributed learning of rules in a short, bounded time frame, enhancing the efficiency of ASOCS models.\n",
      "keywords: Self-Adjusting, Dynamic Logic Module, ASOCS, Adaptive Self-Organizing Concurrent System, Adaptive Algorithm, Massively Parallel Processing, Incremental Rule Systems, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "34263 1122642\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm\n",
      "problems:\n",
      "\n",
      "Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for efficiently implementing dynamic topologies in neural network learning models on parallel hardware.\n",
      "Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and support dynamic addition and deletion during learning.\n",
      "Presenting the Location Independent ASOCS (LIA) model, which incorporates LITs into the ASOCS Adaptive Algorithm 2, and providing formal definitions and descriptions of LIA algorithms.\n",
      "discovery:\n",
      "LIA model enhances ASOCS mechanisms by employing LITs, enabling dynamic topology adjustments in neural network learning models.\n",
      "The incorporation of LITs into ASOCS facilitates efficient learning and supports dynamic addition and deletion of nodes during the learning process.\n",
      "LIA offers a flexible and efficient approach for implementing dynamic topologies in neural networks, potentially improving their performance and adaptability.\n",
      "keywords: Word Perfect LIA, Location-Independent Transformation, ASOCS, Adaptive Algorithm, Dynamic Topologies, Neural Networks, LITs, Parallel Hardware.\n",
      "paper B information: \n",
      "title: Word Perfect: Implementing Efficient Dynamic Backpropagation Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the shortcomings of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming limitations through dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for implementing distributed feedforward networks with dynamic topologies efficiently on parallel hardware.\n",
      "Utilizing LITs to create location-independent nodes that compute network output using local information, allowing for efficient addition and deletion of nodes during learning.\n",
      "Extending standard Backpropagation with LITs to support dynamic topology adjustments.\n",
      "discovery:\n",
      "LITs offer a method to implement dynamic topologies in feedforward neural networks efficiently.\n",
      "The approach enables the addition and deletion of nodes during learning, addressing the limitations of fixed-topology ANNs.\n",
      "LITs enhance Backpropagation by supporting dynamic extensions, improving adaptability and performance.\n",
      "keywords: Word Perfect, Dynamic Backpropagation, Neural Networks, Location-Independent Transformations, Dynamic Topologies, Fixed Topology, Parallel Hardware, Feedforward Networks.\n",
      "\n",
      "================================================================================================================\n",
      "34263 87482\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Multi-Chip Module Implementation of Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the requirement for dense interconnects in artificial neural network systems.\n",
      "Achieving high-density interconnects using advanced interconnect technologies.\n",
      "methods:\n",
      "Implementing multi-chip modules (MCMs) as an interconnect medium for neural network systems.\n",
      "Utilizing specific self-organizing, parallel, dynamic learning models that are well-suited for dense interconnect technology.\n",
      "Exploiting MCM technology to fulfill the requirements of dense interconnects in neural network implementations.\n",
      "discovery:\n",
      "MCM implementation provides an effective solution for achieving dense interconnects in neural network systems.\n",
      "The proposed approach enables the implementation of versatile neural network connectionist models.\n",
      "By leveraging MCM technology, researchers can adapt neural network architectures to meet various application requirements.\n",
      "keywords: Multi-Chip Module, Neural Network, Dense Interconnect, Self-Organizing Learning, Parallel Learning, Dynamic Learning, Interconnect Technology.\n",
      "paper B information: \n",
      "title: Word Perfect: Implementing Efficient Dynamic Backpropagation Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the shortcomings of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming limitations through dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for implementing distributed feedforward networks with dynamic topologies efficiently on parallel hardware.\n",
      "Utilizing LITs to create location-independent nodes that compute network output using local information, allowing for efficient addition and deletion of nodes during learning.\n",
      "Extending standard Backpropagation with LITs to support dynamic topology adjustments.\n",
      "discovery:\n",
      "LITs offer a method to implement dynamic topologies in feedforward neural networks efficiently.\n",
      "The approach enables the addition and deletion of nodes during learning, addressing the limitations of fixed-topology ANNs.\n",
      "LITs enhance Backpropagation by supporting dynamic extensions, improving adaptability and performance.\n",
      "keywords: Word Perfect, Dynamic Backpropagation, Neural Networks, Location-Independent Transformations, Dynamic Topologies, Fixed Topology, Parallel Hardware, Feedforward Networks.\n",
      "\n",
      "================================================================================================================\n",
      "34263 90655\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect Transformation: Implementing Neural Networks with Localist Properties\n",
      "problems:\n",
      "- Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "- Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "- Introducing Location-Independent Transformations (LITs) as a strategy for implementing feedforward networks with dynamic topologies.\n",
      "- Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and supporting dynamic addition and deletion of nodes during learning.\n",
      "- Implementing LITs in single-layer competitive learning networks and counterpropagation networks, combining elements of supervised learning and competitive learning to achieve localist properties.\n",
      "discovery:\n",
      "- LITs facilitate the creation of neural networks with localist properties by combining elements of supervised and competitive learning.\n",
      "- These networks utilize dynamic topologies and employ location-independent nodes to improve efficiency and adaptability.\n",
      "- The integration of LITs enhances the capabilities of neural networks for various applications.\n",
      "keywords: Word Perfect Transformation, Neural Networks, Localist Properties, Location-Independent Transformations, Dynamic Topologies, Competitive Learning, Counterpropagation Networks, Supervised Learning.\n",
      "paper B information: \n",
      "title: Word Perfect: Implementing Efficient Dynamic Backpropagation Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the shortcomings of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming limitations through dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for implementing distributed feedforward networks with dynamic topologies efficiently on parallel hardware.\n",
      "Utilizing LITs to create location-independent nodes that compute network output using local information, allowing for efficient addition and deletion of nodes during learning.\n",
      "Extending standard Backpropagation with LITs to support dynamic topology adjustments.\n",
      "discovery:\n",
      "LITs offer a method to implement dynamic topologies in feedforward neural networks efficiently.\n",
      "The approach enables the addition and deletion of nodes during learning, addressing the limitations of fixed-topology ANNs.\n",
      "LITs enhance Backpropagation by supporting dynamic extensions, improving adaptability and performance.\n",
      "keywords: Word Perfect, Dynamic Backpropagation, Neural Networks, Location-Independent Transformations, Dynamic Topologies, Fixed Topology, Parallel Hardware, Feedforward Networks.\n",
      "\n",
      "================================================================================================================\n",
      "34266 1122642\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm\n",
      "problems:\n",
      "\n",
      "Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for efficiently implementing dynamic topologies in neural network learning models on parallel hardware.\n",
      "Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and support dynamic addition and deletion during learning.\n",
      "Presenting the Location Independent ASOCS (LIA) model, which incorporates LITs into the ASOCS Adaptive Algorithm 2, and providing formal definitions and descriptions of LIA algorithms.\n",
      "discovery:\n",
      "LIA model enhances ASOCS mechanisms by employing LITs, enabling dynamic topology adjustments in neural network learning models.\n",
      "The incorporation of LITs into ASOCS facilitates efficient learning and supports dynamic addition and deletion of nodes during the learning process.\n",
      "LIA offers a flexible and efficient approach for implementing dynamic topologies in neural networks, potentially improving their performance and adaptability.\n",
      "keywords: Word Perfect LIA, Location-Independent Transformation, ASOCS, Adaptive Algorithm, Dynamic Topologies, Neural Networks, LITs, Parallel Hardware.\n",
      "paper B information: \n",
      "title: VLSI Implementation of a Parallel, Self-Organizing Learning Model\n",
      "problems:\n",
      "\n",
      "Developing efficient hardware implementations for neural network learning models.\n",
      "Integrating classical neural network structures into multi-chip module (MCM) substrates.\n",
      "methods:\n",
      "Implementing the Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model on VLSI chips.\n",
      "Utilizing dense weighted links to connect computing nodes in the neural network.\n",
      "Designing PASOCS as a class of Adaptive Self-Organizing Concurrent System (ASOCS) models, aimed at enhancing functional mechanisms compared to classical neural networks.\n",
      "discovery:\n",
      "The PASOCS learning model, implemented on VLSI chips, offers potential applications in pattern recognition, robotics, logical inference, and dynamic control.\n",
      "Hardware implementations of neural network learning models provide efficient solutions for parallel, self-organizing learning tasks.\n",
      "keywords: VLSI Implementation, Parallel Learning Model, Self-Organizing, Neural Networks, Multi-Chip Module (MCM), Priority Adaptive Self-Organizing Concurrent (PASOCS), ASOCS, Pattern Recognition, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "34266 34263\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect: Implementing Efficient Dynamic Backpropagation Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the shortcomings of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming limitations through dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for implementing distributed feedforward networks with dynamic topologies efficiently on parallel hardware.\n",
      "Utilizing LITs to create location-independent nodes that compute network output using local information, allowing for efficient addition and deletion of nodes during learning.\n",
      "Extending standard Backpropagation with LITs to support dynamic topology adjustments.\n",
      "discovery:\n",
      "LITs offer a method to implement dynamic topologies in feedforward neural networks efficiently.\n",
      "The approach enables the addition and deletion of nodes during learning, addressing the limitations of fixed-topology ANNs.\n",
      "LITs enhance Backpropagation by supporting dynamic extensions, improving adaptability and performance.\n",
      "keywords: Word Perfect, Dynamic Backpropagation, Neural Networks, Location-Independent Transformations, Dynamic Topologies, Fixed Topology, Parallel Hardware, Feedforward Networks.\n",
      "paper B information: \n",
      "title: VLSI Implementation of a Parallel, Self-Organizing Learning Model\n",
      "problems:\n",
      "\n",
      "Developing efficient hardware implementations for neural network learning models.\n",
      "Integrating classical neural network structures into multi-chip module (MCM) substrates.\n",
      "methods:\n",
      "Implementing the Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model on VLSI chips.\n",
      "Utilizing dense weighted links to connect computing nodes in the neural network.\n",
      "Designing PASOCS as a class of Adaptive Self-Organizing Concurrent System (ASOCS) models, aimed at enhancing functional mechanisms compared to classical neural networks.\n",
      "discovery:\n",
      "The PASOCS learning model, implemented on VLSI chips, offers potential applications in pattern recognition, robotics, logical inference, and dynamic control.\n",
      "Hardware implementations of neural network learning models provide efficient solutions for parallel, self-organizing learning tasks.\n",
      "keywords: VLSI Implementation, Parallel Learning Model, Self-Organizing, Neural Networks, Multi-Chip Module (MCM), Priority Adaptive Self-Organizing Concurrent (PASOCS), ASOCS, Pattern Recognition, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "34266 90655\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect Transformation: Implementing Neural Networks with Localist Properties\n",
      "problems:\n",
      "- Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "- Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "- Introducing Location-Independent Transformations (LITs) as a strategy for implementing feedforward networks with dynamic topologies.\n",
      "- Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and supporting dynamic addition and deletion of nodes during learning.\n",
      "- Implementing LITs in single-layer competitive learning networks and counterpropagation networks, combining elements of supervised learning and competitive learning to achieve localist properties.\n",
      "discovery:\n",
      "- LITs facilitate the creation of neural networks with localist properties by combining elements of supervised and competitive learning.\n",
      "- These networks utilize dynamic topologies and employ location-independent nodes to improve efficiency and adaptability.\n",
      "- The integration of LITs enhances the capabilities of neural networks for various applications.\n",
      "keywords: Word Perfect Transformation, Neural Networks, Localist Properties, Location-Independent Transformations, Dynamic Topologies, Competitive Learning, Counterpropagation Networks, Supervised Learning.\n",
      "paper B information: \n",
      "title: VLSI Implementation of a Parallel, Self-Organizing Learning Model\n",
      "problems:\n",
      "\n",
      "Developing efficient hardware implementations for neural network learning models.\n",
      "Integrating classical neural network structures into multi-chip module (MCM) substrates.\n",
      "methods:\n",
      "Implementing the Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model on VLSI chips.\n",
      "Utilizing dense weighted links to connect computing nodes in the neural network.\n",
      "Designing PASOCS as a class of Adaptive Self-Organizing Concurrent System (ASOCS) models, aimed at enhancing functional mechanisms compared to classical neural networks.\n",
      "discovery:\n",
      "The PASOCS learning model, implemented on VLSI chips, offers potential applications in pattern recognition, robotics, logical inference, and dynamic control.\n",
      "Hardware implementations of neural network learning models provide efficient solutions for parallel, self-organizing learning tasks.\n",
      "keywords: VLSI Implementation, Parallel Learning Model, Self-Organizing, Neural Networks, Multi-Chip Module (MCM), Priority Adaptive Self-Organizing Concurrent (PASOCS), ASOCS, Pattern Recognition, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "42156 340299\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Neural Network Applicability: Classifying Space\n",
      "problems:\n",
      "\n",
      "Efficient classification of various applications using neural networks.\n",
      "Comparison of neural network-based systems with supervised, unsupervised, and generalizing systems.\n",
      "methods:\n",
      "Utilization of neural network features such as parallel execution, adaptive learning, generalization, and fault tolerance.\n",
      "Categorization of applications into classes and discussion of features amenable to neural network methods.\n",
      "discovery:\n",
      "Neural networks offer powerful computational capabilities for various applications.\n",
      "Efficiency concerns arise in applying neural networks to commonly occurring tasks.\n",
      "keywords: Neural Networks, Classification, Efficiency, Parallel Execution, Adaptive Learning, Generalization, Fault Tolerance, Supervised Learning, Unsupervised Learning, Generalizing Systems.\n",
      "paper B information: \n",
      "title: Models of Parallel Adaptive Logic\n",
      "problems:\n",
      "\n",
      "Developing architectures for adaptive parallel logic systems.\n",
      "Addressing the challenges of adaptability and parallelism in logic systems.\n",
      "methods:\n",
      "Introducing the architecture of Adaptive Self-Organizing Concurrent Systems (ASOCS) for adaptive parallel logic.\n",
      "Utilizing computing elements operating in parallel and asynchronously within ASOCS.\n",
      "Specifying if-then rules forming boolean conjunctions and adapting them incrementally to changing rule-bases.\n",
      "discovery:\n",
      "ASOCS is an adaptive network architecture composed of computing elements operating in parallel and asynchronously.\n",
      "The adaptation process in ASOCS involves two separate phases: data processing and rule-base adaptation.\n",
      "Adaptation occurs in a self-organizing fashion with linear depth in the network architecture.\n",
      "keywords: Models, Parallel, Adaptive Logic, ASOCS, Adaptive Self-Organizing Concurrent System, Computing Elements, Boolean Conjunctions, Parallelism, Adaptation.\n",
      "\n",
      "================================================================================================================\n",
      "45533 34266\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: VLSI Implementation of a Parallel, Self-Organizing Learning Model\n",
      "problems:\n",
      "\n",
      "Developing efficient hardware implementations for neural network learning models.\n",
      "Integrating classical neural network structures into multi-chip module (MCM) substrates.\n",
      "methods:\n",
      "Implementing the Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model on VLSI chips.\n",
      "Utilizing dense weighted links to connect computing nodes in the neural network.\n",
      "Designing PASOCS as a class of Adaptive Self-Organizing Concurrent System (ASOCS) models, aimed at enhancing functional mechanisms compared to classical neural networks.\n",
      "discovery:\n",
      "The PASOCS learning model, implemented on VLSI chips, offers potential applications in pattern recognition, robotics, logical inference, and dynamic control.\n",
      "Hardware implementations of neural network learning models provide efficient solutions for parallel, self-organizing learning tasks.\n",
      "keywords: VLSI Implementation, Parallel Learning Model, Self-Organizing, Neural Networks, Multi-Chip Module (MCM), Priority Adaptive Self-Organizing Concurrent (PASOCS), ASOCS, Pattern Recognition, Robotics, Logical Inference, Dynamic Control.\n",
      "paper B information: \n",
      "title: Priority ASOCS: Advantages in Adaptive Learning Models\n",
      "problems:\n",
      "- Developing efficient models for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "- Addressing the complexities associated with parallel processing in adaptive systems.\n",
      "methods:\n",
      "- Implementing the Adaptive Self-Organizing Concurrent System (ASOCS) model, which utilizes massively parallel processing and incrementally defined rule systems.\n",
      "- Proposing the Priority ASOCS learning algorithm and architecture, which prioritizes rules in a distributed fashion.\n",
      "discovery:\n",
      "- ASOCS models offer advantages in space complexity improvements and efficient computation for self-organized learning and parallel execution.\n",
      "- The Priority ASOCS model enhances learning by incorporating rule priorities and supporting parallel execution of inputs based on learned rules.\n",
      "- ASOCS models exhibit dynamic topologies and support both digital and symbolic heuristic learning mechanisms.\n",
      "keywords: Priority ASOCS, Adaptive Self-Organizing Concurrent System, Parallel Processing, Incremental Rule Systems, Adaptive Logic, Robotics, Logical Inference, Dynamic Control, Rule Prioritization, Parallel Execution.\n",
      "\n",
      "================================================================================================================\n",
      "45533 87482\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Multi-Chip Module Implementation of Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the requirement for dense interconnects in artificial neural network systems.\n",
      "Achieving high-density interconnects using advanced interconnect technologies.\n",
      "methods:\n",
      "Implementing multi-chip modules (MCMs) as an interconnect medium for neural network systems.\n",
      "Utilizing specific self-organizing, parallel, dynamic learning models that are well-suited for dense interconnect technology.\n",
      "Exploiting MCM technology to fulfill the requirements of dense interconnects in neural network implementations.\n",
      "discovery:\n",
      "MCM implementation provides an effective solution for achieving dense interconnects in neural network systems.\n",
      "The proposed approach enables the implementation of versatile neural network connectionist models.\n",
      "By leveraging MCM technology, researchers can adapt neural network architectures to meet various application requirements.\n",
      "keywords: Multi-Chip Module, Neural Network, Dense Interconnect, Self-Organizing Learning, Parallel Learning, Dynamic Learning, Interconnect Technology.\n",
      "paper B information: \n",
      "title: Priority ASOCS: Advantages in Adaptive Learning Models\n",
      "problems:\n",
      "- Developing efficient models for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "- Addressing the complexities associated with parallel processing in adaptive systems.\n",
      "methods:\n",
      "- Implementing the Adaptive Self-Organizing Concurrent System (ASOCS) model, which utilizes massively parallel processing and incrementally defined rule systems.\n",
      "- Proposing the Priority ASOCS learning algorithm and architecture, which prioritizes rules in a distributed fashion.\n",
      "discovery:\n",
      "- ASOCS models offer advantages in space complexity improvements and efficient computation for self-organized learning and parallel execution.\n",
      "- The Priority ASOCS model enhances learning by incorporating rule priorities and supporting parallel execution of inputs based on learned rules.\n",
      "- ASOCS models exhibit dynamic topologies and support both digital and symbolic heuristic learning mechanisms.\n",
      "keywords: Priority ASOCS, Adaptive Self-Organizing Concurrent System, Parallel Processing, Incremental Rule Systems, Adaptive Logic, Robotics, Logical Inference, Dynamic Control, Rule Prioritization, Parallel Execution.\n",
      "\n",
      "================================================================================================================\n",
      "74427 34257\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Self-Adjusting Dynamic Logic Module\n",
      "problems:\n",
      "\n",
      "Developing a self-adjusting dynamic logic module for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "methods:\n",
      "Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "Implementing the Adaptive Algorithm (AA2) for learning and memory knowledge maintenance within ASOCS.\n",
      "Operating ASOCS in both learning mode, where rules are expressed as boolean conjunctions, and data processing mode, where it acts as a parallel hardware circuit.\n",
      "discovery:\n",
      "The ASOCS model offers advantages in adaptive logic systems, robotics, logical inference, and dynamic control due to its parallel processing capabilities and adaptive nature.\n",
      "The AA2 learning algorithm enables distributed learning of rules in a short, bounded time frame, enhancing the efficiency of ASOCS models.\n",
      "keywords: Self-Adjusting, Dynamic Logic Module, ASOCS, Adaptive Self-Organizing Concurrent System, Adaptive Algorithm, Massively Parallel Processing, Incremental Rule Systems, Robotics, Logical Inference, Dynamic Control.\n",
      "paper B information: \n",
      "title: Self-Organizing Binary Decision Tree with Incrementally Defined Rules\n",
      "problems:\n",
      "- Developing efficient models for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "- Addressing the complexities associated with parallel processing in adaptive systems.\n",
      "methods:\n",
      "- Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "- Implementing the Adaptive Algorithm (AA3) to facilitate learning in ASOCS, focusing on architecture and learning algorithm details.\n",
      "- Operating ASOCS in both data processing mode, where it acts as a parallel hardware circuit, and learning mode, where rules are incrementally expressed as boolean conjunctions.\n",
      "discovery:\n",
      "- ASOCS models offer advantages in simplicity, implementability, and cost-effectiveness for various applications.\n",
      "- The ASOCS learning algorithms incorporate rules in a distributed fashion within a short, bounded time frame.\n",
      "- Self-organizing binary decision trees with incrementally defined rules provide a flexible approach for adaptive logic and other applications.\n",
      "keywords: Self-Organizing, Binary Decision Tree, Incrementally Defined Rules, ASOCS, Adaptive Algorithm, Parallel Processing, Adaptive Logic, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "74427 34266\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: VLSI Implementation of a Parallel, Self-Organizing Learning Model\n",
      "problems:\n",
      "\n",
      "Developing efficient hardware implementations for neural network learning models.\n",
      "Integrating classical neural network structures into multi-chip module (MCM) substrates.\n",
      "methods:\n",
      "Implementing the Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model on VLSI chips.\n",
      "Utilizing dense weighted links to connect computing nodes in the neural network.\n",
      "Designing PASOCS as a class of Adaptive Self-Organizing Concurrent System (ASOCS) models, aimed at enhancing functional mechanisms compared to classical neural networks.\n",
      "discovery:\n",
      "The PASOCS learning model, implemented on VLSI chips, offers potential applications in pattern recognition, robotics, logical inference, and dynamic control.\n",
      "Hardware implementations of neural network learning models provide efficient solutions for parallel, self-organizing learning tasks.\n",
      "keywords: VLSI Implementation, Parallel Learning Model, Self-Organizing, Neural Networks, Multi-Chip Module (MCM), Priority Adaptive Self-Organizing Concurrent (PASOCS), ASOCS, Pattern Recognition, Robotics, Logical Inference, Dynamic Control.\n",
      "paper B information: \n",
      "title: Self-Organizing Binary Decision Tree with Incrementally Defined Rules\n",
      "problems:\n",
      "- Developing efficient models for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "- Addressing the complexities associated with parallel processing in adaptive systems.\n",
      "methods:\n",
      "- Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "- Implementing the Adaptive Algorithm (AA3) to facilitate learning in ASOCS, focusing on architecture and learning algorithm details.\n",
      "- Operating ASOCS in both data processing mode, where it acts as a parallel hardware circuit, and learning mode, where rules are incrementally expressed as boolean conjunctions.\n",
      "discovery:\n",
      "- ASOCS models offer advantages in simplicity, implementability, and cost-effectiveness for various applications.\n",
      "- The ASOCS learning algorithms incorporate rules in a distributed fashion within a short, bounded time frame.\n",
      "- Self-organizing binary decision trees with incrementally defined rules provide a flexible approach for adaptive logic and other applications.\n",
      "keywords: Self-Organizing, Binary Decision Tree, Incrementally Defined Rules, ASOCS, Adaptive Algorithm, Parallel Processing, Adaptive Logic, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "74427 87482\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Multi-Chip Module Implementation of Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the requirement for dense interconnects in artificial neural network systems.\n",
      "Achieving high-density interconnects using advanced interconnect technologies.\n",
      "methods:\n",
      "Implementing multi-chip modules (MCMs) as an interconnect medium for neural network systems.\n",
      "Utilizing specific self-organizing, parallel, dynamic learning models that are well-suited for dense interconnect technology.\n",
      "Exploiting MCM technology to fulfill the requirements of dense interconnects in neural network implementations.\n",
      "discovery:\n",
      "MCM implementation provides an effective solution for achieving dense interconnects in neural network systems.\n",
      "The proposed approach enables the implementation of versatile neural network connectionist models.\n",
      "By leveraging MCM technology, researchers can adapt neural network architectures to meet various application requirements.\n",
      "keywords: Multi-Chip Module, Neural Network, Dense Interconnect, Self-Organizing Learning, Parallel Learning, Dynamic Learning, Interconnect Technology.\n",
      "paper B information: \n",
      "title: Self-Organizing Binary Decision Tree with Incrementally Defined Rules\n",
      "problems:\n",
      "- Developing efficient models for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "- Addressing the complexities associated with parallel processing in adaptive systems.\n",
      "methods:\n",
      "- Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "- Implementing the Adaptive Algorithm (AA3) to facilitate learning in ASOCS, focusing on architecture and learning algorithm details.\n",
      "- Operating ASOCS in both data processing mode, where it acts as a parallel hardware circuit, and learning mode, where rules are incrementally expressed as boolean conjunctions.\n",
      "discovery:\n",
      "- ASOCS models offer advantages in simplicity, implementability, and cost-effectiveness for various applications.\n",
      "- The ASOCS learning algorithms incorporate rules in a distributed fashion within a short, bounded time frame.\n",
      "- Self-organizing binary decision trees with incrementally defined rules provide a flexible approach for adaptive logic and other applications.\n",
      "keywords: Self-Organizing, Binary Decision Tree, Incrementally Defined Rules, ASOCS, Adaptive Algorithm, Parallel Processing, Adaptive Logic, Robotics, Logical Inference, Dynamic Control.\n",
      "\n",
      "================================================================================================================\n",
      "87482 34266\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: VLSI Implementation of a Parallel, Self-Organizing Learning Model\n",
      "problems:\n",
      "\n",
      "Developing efficient hardware implementations for neural network learning models.\n",
      "Integrating classical neural network structures into multi-chip module (MCM) substrates.\n",
      "methods:\n",
      "Implementing the Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model on VLSI chips.\n",
      "Utilizing dense weighted links to connect computing nodes in the neural network.\n",
      "Designing PASOCS as a class of Adaptive Self-Organizing Concurrent System (ASOCS) models, aimed at enhancing functional mechanisms compared to classical neural networks.\n",
      "discovery:\n",
      "The PASOCS learning model, implemented on VLSI chips, offers potential applications in pattern recognition, robotics, logical inference, and dynamic control.\n",
      "Hardware implementations of neural network learning models provide efficient solutions for parallel, self-organizing learning tasks.\n",
      "keywords: VLSI Implementation, Parallel Learning Model, Self-Organizing, Neural Networks, Multi-Chip Module (MCM), Priority Adaptive Self-Organizing Concurrent (PASOCS), ASOCS, Pattern Recognition, Robotics, Logical Inference, Dynamic Control.\n",
      "paper B information: \n",
      "title: Multi-Chip Module Implementation of Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the requirement for dense interconnects in artificial neural network systems.\n",
      "Achieving high-density interconnects using advanced interconnect technologies.\n",
      "methods:\n",
      "Implementing multi-chip modules (MCMs) as an interconnect medium for neural network systems.\n",
      "Utilizing specific self-organizing, parallel, dynamic learning models that are well-suited for dense interconnect technology.\n",
      "Exploiting MCM technology to fulfill the requirements of dense interconnects in neural network implementations.\n",
      "discovery:\n",
      "MCM implementation provides an effective solution for achieving dense interconnects in neural network systems.\n",
      "The proposed approach enables the implementation of versatile neural network connectionist models.\n",
      "By leveraging MCM technology, researchers can adapt neural network architectures to meet various application requirements.\n",
      "keywords: Multi-Chip Module, Neural Network, Dense Interconnect, Self-Organizing Learning, Parallel Learning, Dynamic Learning, Interconnect Technology.\n",
      "\n",
      "================================================================================================================\n",
      "90655 1122642\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm\n",
      "problems:\n",
      "\n",
      "Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for efficiently implementing dynamic topologies in neural network learning models on parallel hardware.\n",
      "Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and support dynamic addition and deletion during learning.\n",
      "Presenting the Location Independent ASOCS (LIA) model, which incorporates LITs into the ASOCS Adaptive Algorithm 2, and providing formal definitions and descriptions of LIA algorithms.\n",
      "discovery:\n",
      "LIA model enhances ASOCS mechanisms by employing LITs, enabling dynamic topology adjustments in neural network learning models.\n",
      "The incorporation of LITs into ASOCS facilitates efficient learning and supports dynamic addition and deletion of nodes during the learning process.\n",
      "LIA offers a flexible and efficient approach for implementing dynamic topologies in neural networks, potentially improving their performance and adaptability.\n",
      "keywords: Word Perfect LIA, Location-Independent Transformation, ASOCS, Adaptive Algorithm, Dynamic Topologies, Neural Networks, LITs, Parallel Hardware.\n",
      "paper B information: \n",
      "title: Word Perfect Transformation: Implementing Neural Networks with Localist Properties\n",
      "problems:\n",
      "- Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "- Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "- Introducing Location-Independent Transformations (LITs) as a strategy for implementing feedforward networks with dynamic topologies.\n",
      "- Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and supporting dynamic addition and deletion of nodes during learning.\n",
      "- Implementing LITs in single-layer competitive learning networks and counterpropagation networks, combining elements of supervised learning and competitive learning to achieve localist properties.\n",
      "discovery:\n",
      "- LITs facilitate the creation of neural networks with localist properties by combining elements of supervised and competitive learning.\n",
      "- These networks utilize dynamic topologies and employ location-independent nodes to improve efficiency and adaptability.\n",
      "- The integration of LITs enhances the capabilities of neural networks for various applications.\n",
      "keywords: Word Perfect Transformation, Neural Networks, Localist Properties, Location-Independent Transformations, Dynamic Topologies, Competitive Learning, Counterpropagation Networks, Supervised Learning.\n",
      "\n",
      "================================================================================================================\n",
      "90655 34263\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect: Implementing Efficient Dynamic Backpropagation Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the shortcomings of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming limitations through dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for implementing distributed feedforward networks with dynamic topologies efficiently on parallel hardware.\n",
      "Utilizing LITs to create location-independent nodes that compute network output using local information, allowing for efficient addition and deletion of nodes during learning.\n",
      "Extending standard Backpropagation with LITs to support dynamic topology adjustments.\n",
      "discovery:\n",
      "LITs offer a method to implement dynamic topologies in feedforward neural networks efficiently.\n",
      "The approach enables the addition and deletion of nodes during learning, addressing the limitations of fixed-topology ANNs.\n",
      "LITs enhance Backpropagation by supporting dynamic extensions, improving adaptability and performance.\n",
      "keywords: Word Perfect, Dynamic Backpropagation, Neural Networks, Location-Independent Transformations, Dynamic Topologies, Fixed Topology, Parallel Hardware, Feedforward Networks.\n",
      "paper B information: \n",
      "title: Word Perfect Transformation: Implementing Neural Networks with Localist Properties\n",
      "problems:\n",
      "- Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "- Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "- Introducing Location-Independent Transformations (LITs) as a strategy for implementing feedforward networks with dynamic topologies.\n",
      "- Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and supporting dynamic addition and deletion of nodes during learning.\n",
      "- Implementing LITs in single-layer competitive learning networks and counterpropagation networks, combining elements of supervised learning and competitive learning to achieve localist properties.\n",
      "discovery:\n",
      "- LITs facilitate the creation of neural networks with localist properties by combining elements of supervised and competitive learning.\n",
      "- These networks utilize dynamic topologies and employ location-independent nodes to improve efficiency and adaptability.\n",
      "- The integration of LITs enhances the capabilities of neural networks for various applications.\n",
      "keywords: Word Perfect Transformation, Neural Networks, Localist Properties, Location-Independent Transformations, Dynamic Topologies, Competitive Learning, Counterpropagation Networks, Supervised Learning.\n",
      "\n",
      "================================================================================================================\n",
      "213246 1122642\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm\n",
      "problems:\n",
      "\n",
      "Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for efficiently implementing dynamic topologies in neural network learning models on parallel hardware.\n",
      "Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and support dynamic addition and deletion during learning.\n",
      "Presenting the Location Independent ASOCS (LIA) model, which incorporates LITs into the ASOCS Adaptive Algorithm 2, and providing formal definitions and descriptions of LIA algorithms.\n",
      "discovery:\n",
      "LIA model enhances ASOCS mechanisms by employing LITs, enabling dynamic topology adjustments in neural network learning models.\n",
      "The incorporation of LITs into ASOCS facilitates efficient learning and supports dynamic addition and deletion of nodes during the learning process.\n",
      "LIA offers a flexible and efficient approach for implementing dynamic topologies in neural networks, potentially improving their performance and adaptability.\n",
      "keywords: Word Perfect LIA, Location-Independent Transformation, ASOCS, Adaptive Algorithm, Dynamic Topologies, Neural Networks, LITs, Parallel Hardware.\n",
      "paper B information: \n",
      "title: Growing Layers Perceptrons: Introducing Extentron Algorithm\n",
      "problems:\n",
      "- Addressing the limitations of perceptron learning algorithms in handling non-linearly separable data.\n",
      "- Introducing a method to grow multi-layer networks capable of distinguishing non-linearly separable patterns.\n",
      "methods:\n",
      "- Proposing the Extentron algorithm, which extends perceptrons to build multi-layer networks using linear threshold units.\n",
      "- Demonstrating the simplicity, speed, scalability, and convergence properties of the Extentron algorithm, which requires only two parameters.\n",
      "discovery:\n",
      "- The Extentron algorithm enables the construction of multi-layer networks capable of handling non-linearly separable data.\n",
      "- By extending perceptrons, the algorithm provides a simple and efficient approach to building neural networks.\n",
      "- Comparing the Extentron algorithm with other neural network paradigms and symbolic learning systems reveals its effectiveness.\n",
      "keywords: Growing Layers Perceptrons, Extentron Algorithm, Perceptron Learning, Multi-Layer Networks, Linear Threshold Units, Non-linear Separability, Symbolic Learning.\n",
      "\n",
      "================================================================================================================\n",
      "213246 34263\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Word Perfect: Implementing Efficient Dynamic Backpropagation Neural Networks\n",
      "problems:\n",
      "\n",
      "Addressing the shortcomings of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming limitations through dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for implementing distributed feedforward networks with dynamic topologies efficiently on parallel hardware.\n",
      "Utilizing LITs to create location-independent nodes that compute network output using local information, allowing for efficient addition and deletion of nodes during learning.\n",
      "Extending standard Backpropagation with LITs to support dynamic topology adjustments.\n",
      "discovery:\n",
      "LITs offer a method to implement dynamic topologies in feedforward neural networks efficiently.\n",
      "The approach enables the addition and deletion of nodes during learning, addressing the limitations of fixed-topology ANNs.\n",
      "LITs enhance Backpropagation by supporting dynamic extensions, improving adaptability and performance.\n",
      "keywords: Word Perfect, Dynamic Backpropagation, Neural Networks, Location-Independent Transformations, Dynamic Topologies, Fixed Topology, Parallel Hardware, Feedforward Networks.\n",
      "paper B information: \n",
      "title: Growing Layers Perceptrons: Introducing Extentron Algorithm\n",
      "problems:\n",
      "- Addressing the limitations of perceptron learning algorithms in handling non-linearly separable data.\n",
      "- Introducing a method to grow multi-layer networks capable of distinguishing non-linearly separable patterns.\n",
      "methods:\n",
      "- Proposing the Extentron algorithm, which extends perceptrons to build multi-layer networks using linear threshold units.\n",
      "- Demonstrating the simplicity, speed, scalability, and convergence properties of the Extentron algorithm, which requires only two parameters.\n",
      "discovery:\n",
      "- The Extentron algorithm enables the construction of multi-layer networks capable of handling non-linearly separable data.\n",
      "- By extending perceptrons, the algorithm provides a simple and efficient approach to building neural networks.\n",
      "- Comparing the Extentron algorithm with other neural network paradigms and symbolic learning systems reveals its effectiveness.\n",
      "keywords: Growing Layers Perceptrons, Extentron Algorithm, Perceptron Learning, Multi-Layer Networks, Linear Threshold Units, Non-linear Separability, Symbolic Learning.\n",
      "\n",
      "================================================================================================================\n",
      "340299 74427\n",
      "\n",
      "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
      "paper A title:\n",
      "paper B title:\n",
      "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
      "\n",
      "paper A information: \n",
      "title: Self-Organizing Binary Decision Tree with Incrementally Defined Rules\n",
      "problems:\n",
      "- Developing efficient models for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "- Addressing the complexities associated with parallel processing in adaptive systems.\n",
      "methods:\n",
      "- Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "- Implementing the Adaptive Algorithm (AA3) to facilitate learning in ASOCS, focusing on architecture and learning algorithm details.\n",
      "- Operating ASOCS in both data processing mode, where it acts as a parallel hardware circuit, and learning mode, where rules are incrementally expressed as boolean conjunctions.\n",
      "discovery:\n",
      "- ASOCS models offer advantages in simplicity, implementability, and cost-effectiveness for various applications.\n",
      "- The ASOCS learning algorithms incorporate rules in a distributed fashion within a short, bounded time frame.\n",
      "- Self-organizing binary decision trees with incrementally defined rules provide a flexible approach for adaptive logic and other applications.\n",
      "keywords: Self-Organizing, Binary Decision Tree, Incrementally Defined Rules, ASOCS, Adaptive Algorithm, Parallel Processing, Adaptive Logic, Robotics, Logical Inference, Dynamic Control.\n",
      "paper B information: \n",
      "title: Neural Network Applicability: Classifying Space\n",
      "problems:\n",
      "\n",
      "Efficient classification of various applications using neural networks.\n",
      "Comparison of neural network-based systems with supervised, unsupervised, and generalizing systems.\n",
      "methods:\n",
      "Utilization of neural network features such as parallel execution, adaptive learning, generalization, and fault tolerance.\n",
      "Categorization of applications into classes and discussion of features amenable to neural network methods.\n",
      "discovery:\n",
      "Neural networks offer powerful computational capabilities for various applications.\n",
      "Efficiency concerns arise in applying neural networks to commonly occurring tasks.\n",
      "keywords: Neural Networks, Classification, Efficiency, Parallel Execution, Adaptive Learning, Generalization, Fault Tolerance, Supervised Learning, Unsupervised Learning, Generalizing Systems.\n",
      "\n",
      "================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# transition prompt\n",
    "\n",
    "nodes = pd.read_excel('subgraph/1122642/sample.xlsx')\n",
    "edges = pd.read_csv('subgraph/1122642/edge_1122642.csv')\n",
    "\n",
    "for i in range(len(edges)):\n",
    "    new_paper, old_paper = edges.loc[i, 'cite_id'], edges.loc[i, 'cited_id']\n",
    "\n",
    "    # old_paper information\n",
    "    old_paper_info = nodes.loc[nodes.pid == old_paper, 'summary'].values[0]\n",
    "    # new_paper information\n",
    "    new_paper_info = nodes.loc[nodes.pid == new_paper, 'summary'].values[0]\n",
    "\n",
    "    prompt = f'''\n",
    "You are an expert in summarising the evolution of papers, and are given information about two papers A and B, and B cites A. You need to summarise them in the following format:\n",
    "paper A title:\n",
    "paper B title:\n",
    "Evolution A to B: B builds on A by introducing a new technology, X, which delves into problem Y and presents viewpoint Z.\n",
    "\n",
    "paper A information: \n",
    "{old_paper_info}\n",
    "paper B information: \n",
    "{new_paper_info}\n",
    "'''\n",
    "    print(new_paper, old_paper)\n",
    "    print(prompt)\n",
    "    print('================================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head paper:title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm\n",
      "problems:\n",
      "\n",
      "Addressing the limitations of fixed-topology learning in Artificial Neural Networks (ANNs).\n",
      "Overcoming these limitations by employing dynamic topologies in ANNs.\n",
      "methods:\n",
      "Introducing Location-Independent Transformations (LITs) as a strategy for efficiently implementing dynamic topologies in neural network learning models on parallel hardware.\n",
      "Creating location-independent nodes through LITs, allowing nodes to compute network output using local information and support dynamic addition and deletion during learning.\n",
      "Presenting the Location Independent ASOCS (LIA) model, which incorporates LITs into the ASOCS Adaptive Algorithm 2, and providing formal definitions and descriptions of LIA algorithms.\n",
      "discovery:\n",
      "LIA model enhances ASOCS mechanisms by employing LITs, enabling dynamic topology adjustments in neural network learning models.\n",
      "The incorporation of LITs into ASOCS facilitates efficient learning and supports dynamic addition and deletion of nodes during the learning process.\n",
      "LIA offers a flexible and efficient approach for implementing dynamic topologies in neural networks, potentially improving their performance and adaptability.\n",
      "keywords: Word Perfect LIA, Location-Independent Transformation, ASOCS, Adaptive Algorithm, Dynamic Topologies, Neural Networks, LITs, Parallel Hardware.tail paper:title: Self-Adjusting Dynamic Logic Module\n",
      "problems:\n",
      "\n",
      "Developing a self-adjusting dynamic logic module for adaptive logic, robotics, logical inference, and dynamic control.\n",
      "methods:\n",
      "Utilizing the ASOCS (Adaptive Self-Organizing Concurrent System) model, which employs massively parallel processing and incrementally defined rule systems.\n",
      "Implementing the Adaptive Algorithm (AA2) for learning and memory knowledge maintenance within ASOCS.\n",
      "Operating ASOCS in both learning mode, where rules are expressed as boolean conjunctions, and data processing mode, where it acts as a parallel hardware circuit.\n",
      "discovery:\n",
      "The ASOCS model offers advantages in adaptive logic systems, robotics, logical inference, and dynamic control due to its parallel processing capabilities and adaptive nature.\n",
      "The AA2 learning algorithm enables distributed learning of rules in a short, bounded time frame, enhancing the efficiency of ASOCS models.\n",
      "keywords: Self-Adjusting, Dynamic Logic Module, ASOCS, Adaptive Self-Organizing Concurrent System, Adaptive Algorithm, Massively Parallel Processing, Incremental Rule Systems, Robotics, Logical Inference, Dynamic Control.evolution from head to tail:Paper A title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm\n",
      "\n",
      "Paper B title: Self-Adjusting Dynamic Logic Module\n",
      "\n",
      "Evolution A to B: Paper B builds on Paper A by introducing a new technology, the Self-Adjusting Dynamic Logic Module, which delves into the problem of developing adaptive logic, robotics, logical inference, and dynamic control. It presents the viewpoint that utilizing the ASOCS model, particularly the Adaptive Algorithm (AA2), enables the creation of self-adjusting dynamic logic modules that leverage massively parallel processing and incrementally defined rule systems for efficient learning and memory maintenance.\n"
     ]
    }
   ],
   "source": [
    "# Link Prediction prompt\n",
    "\n",
    "# query path [1122642, 34257, 87482, 34263, 34266]\n",
    "\n",
    "# [Head paper, paper1, paper2, ..., Tail paper]\n",
    "\n",
    "nodes = pd.read_excel('subgraph/1122642/sample.xlsx')\n",
    "edges = pd.read_excel('subgraph/1122642/edge.xlsx')\n",
    "\n",
    "# 确定头节点，尾节点\n",
    "def chain_desc(head, tail, query):\n",
    "    tail_idx = query.index(tail)\n",
    "\n",
    "    head_desc, tail_desc = nodes.loc[nodes.pid == head, 'summary'].values[0], nodes.loc[nodes.pid == tail, 'summary'].values[0]\n",
    "    prompt = \"\"\n",
    "    \n",
    "\n",
    "    if tail_idx == 1:\n",
    "        evo = edges[(edges.cited_id == head) & (edges.cite_id == tail)]['evolution'].values[0]\n",
    "        prompt += \"head paper:\" + head_desc\n",
    "        prompt += \"tail paper:\" + tail_desc\n",
    "        prompt += \"evolution from head to tail:\" + evo\n",
    "        return  prompt\n",
    "    \n",
    "    else:\n",
    "        prompt += \"head paper:\" + head_desc\n",
    "        for i in range(1, tail_idx):\n",
    "            node_desc = nodes.loc[nodes.pid == query[i], 'summary'].values[0]\n",
    "            evo = edges[((edges.cited_id == query[i-1]) & (edges.cite_id == query[i])) | ((edges.cited_id == query[i]) & (edges.cite_id == query[i-1]))]['evolution'].values[0]\n",
    "            prompt += f\"paper {i}:\" + node_desc \n",
    "            if i == 1:\n",
    "                prompt += f\"evolution from head to paper{i}:\" + evo\n",
    "            else:\n",
    "                prompt += f\"evolution from paper{i-1} to paper{i}:\" + evo\n",
    "        prompt += \"tail paper:\" + tail_desc\n",
    "        evo = \n",
    "        \n",
    "\n",
    "\n",
    "src_node = 1122642\n",
    "path = [1122642, 34257, 87482, 34263, 34266]\n",
    "\n",
    "print(chain_desc(src_node, 34257, path))\n",
    "\n",
    "# for node in path:\n",
    "#     if node == src_node:\n",
    "#         continue\n",
    "#     prompt = f'''\n",
    "# You are an expert in determining whether a citation relationship exists, and now given information about a chain of paper citations, you need to determine whether the trailing paper cites the head paper:\n",
    "# citation chain information:\n",
    "# '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
