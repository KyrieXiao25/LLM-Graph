pid,new_title,new_abs,label
340299,Title: Neural Network Applicability: Classifying Space,"Abstract: tremendous effort propose neurally inspired methods computation forces closer scrutiny potential models. categorizes applications classes discusses features applications efficiently amenable neural network methods. Computational machines deterministic mappings inputs outputs computational mechanisms solutions. Neural network features parallel execution, adaptive learning, generalization, fault tolerance. Often, effort model applications implemented efficient alternate technology. Neural networks powerful devices classes applications, all. However, class applications neural networks efficient commonly occurring nature. Comparison supervised, unsupervised, generalizing systems included.",Neural_Networks
1365,Title: Cholinergic suppression transmission combined associative memory function self-organization neocortex.,"Abstract: Selective suppression transmission feedback synapses learning mechanism combining associative feedback self-organization feedforward synapses. Experimental data demonstrates cholinergic suppression synaptic transmission layer (feedback synapses), suppression layer (feed-forward synapses). network feature local rules learn mappings linearly separable. learning, sensory stimuli desired response simultaneously input. Feedforward connections form self-organized representations input, suppressed feedback connections learn transpose feedfor-ward connectivity. recall, suppression removed, sensory input activates self-organized representation, activity generates learned response.",Neural_Networks
34257,Title: Self-Adjusting Dynamic Logic Module,"Abstract: ASOCS (Adaptive Self-Organizing Concurrent System) model massively parallel processing incrementally defined rule systems adaptive logic, robotics, logical inference, dynamic control. ASOCS adaptive network composed computing elements operating asynchronously parallel. focuses Adaptive Algorithm (AA2) details architecture learning algorithm. AA2 memory knowledge maintenance advantages ASOCS models. ASOCS operate data processing mode learning mode. learning mode, ASOCS rule expressed boolean conjunction. AA2 learning algorithm incorporates rule distributed fashion short, bounded time. data processing mode, ASOCS acts parallel hardware circuit.",Neural_Networks
34263,Title: Word Perfect TRANSFORMATION IMPLEMENTING EFFICIENT DYNAMIC BACKPROPAGATION NEURAL NETWORKS,"Abstract: Artificial Neural Networks (ANNs) fixed topology learning, suffer shortcomings result. Variations ANNs dynamic topologies ability overcome problems. introduces Location-Independent Transformations (LITs) strategy implementing distributed feedforward networks dynamic topologies (dynamic ANNs) efficiently parallel hardware. LIT creates location-independent nodes, node computes network output independent nodes, local information. transformation efficient port adding deleting nodes dynamically learning. particular, LIT standard Backpropagation two layers weights, dynamic extensions Backpropagation supported.",Neural_Networks
34266,"Title: VLSI Implementation Parallel, Self-Organizing Learning Model","Abstract: VLSI implementation Priority Adaptive Self-Organizing Concurrent (PASOCS) learning model built multi-chip module (MCM) substrate. hardware implementations neural network learning models direct implementations classical neural network structures|a computing nodes connected dense weighted links. PASOCS one class ASOCS (Adaptive Self-Organizing Concurrent System) connectionist models goal classical neural networks models, functional mechanisms significantly. model potential pattern recognition, robotics, logical inference, dynamic control.",Neural_Networks
1122642,Title: Word Perfect LIA: Location-Independent Transformation ASOCS Adaptive Algorithm,"Abstract: Artificial Neural Networks (ANNs) fixed topology learning, suffer shortcomings result. ANNs dynamic topologies ability overcome problems. Adaptive Organizing Concurrent Systems (ASOCS) class learning models inherently dynamic topologies. introduces Location-Independent Transformations (LITs) strategy implementing learning models dynamic topologies efficiently parallel hardware. LIT creates location-independent nodes, node computes network output independent nodes, local information. transformation efficient support adding deleting nodes dynamically learning. particular, Location Independent ASOCS (LIA) model LIT ASOCS Adaptive Algorithm 2. description LIA formal definitions LIA algorithms. LIA implements basic ASOCS mechanisms, definitions formal description basic ASOCS mechanisms general, addition LIA.",Neural_Networks
87482,Title: Multi-Chip Module Implementation Neural Network,"Abstract: requirement dense interconnect artificial neural network systems researchers high-density interconnect technologies. reports implementation multi-chip modules (MCMs) interconnect medium. specific self-organizing, parallel, dynamic learning model dense interconnect technology effective implementation; requirement fulfilled exploiting MCM technology. ideas MCM implementation artificial neural networks versatile adapted neural network connectionist models.",Neural_Networks
74427,Title: Self-Organizing Binary Decision Tree Incrementally Defined Rule,"Abstract: ASOCS (adaptive self-organizing concurrent system) model massively parallel processing incrementally defined rule systems adaptive logic, robotics, logical inference, dynamic control. ASOCS adaptive network composed computing elements operating asynchronously parallel. focuses adaptive algorithm (AA3) details architecture learning algorithm. advantages ASOCS models simplicity, implementability, cost. ASOCS operate data processing mode learning mode. data processing mode, ASOCS acts parallel hardware circuit. learning mode, rules expressed boolean conjunctions incrementally ASOCS. ASOCS learning algorithms incorporate rule distributed fashion short, bounded time.",Neural_Networks
45533,Title: Priority ASOCS ASOCS models two advantages learning models:,"Abstract: ASOCS (Adaptive Self-Organizing Concurrent System) model massively parallel processing incrementally defined rule systems adaptive logic, robotics, logical inference, dynamic control. ASOCS adaptive network composed computing elements operating asynchronously parallel. ASOCS operate data processing mode learning mode. data processing mode, ASOCS acts parallel hardware circuit. learning mode, ASOCS incorporates rule expressed Boolean conjunction distributed fashion logarithmic rules. proposes learning algorithm architecture Priority ASOCS. ASOCS model rules priorities. model learning space complexity improvements models. Non-von Neumann architectures neural networks attack word-at-a-time bottleneck traditional computing systems [1]. Neural networks learn input-output mappings highly distributed processing memory [10,11,12]. numerous processing elements modifiable weighted permit degree parallelism. typical neural network fixed topology. learns modifying weighted nodes. class connectionist architectures ASOCS (Adaptive Self-Organizing Concurrent Systems) [4,5]. ASOCS models support efficient computation self-organized learning parallel execution. Learning incremental presentation rules examples. ASOCS models learn modifying topology. Data Boolean multi-state variables; models support analog variables. model incorporates rules adaptive logic network parallel organizing fashion. processing mode, ASOCS supports parallel execution inputs learned rules. adaptive logic network acts parallel hardware circuit execution, mapping input boolean vectors output boolean vectors, combinatoric fashion. philosophy ASOCS level goals neural network models. However, mechanisms learning execution vary significantly. ASOCS logic network topologically dynamic network growing efficiently fit specific application. ASOCS models digital nodes. ASOCS supports symbolic heuristic learning mechanisms, combining parallelism distributed nature connectionist computing potential power symbolic learning. proof ASOCS chip developed [2].",Neural_Networks
213246,Title: Growing Layers Perceptrons: Introducing Extentron Algorithm,"Abstract: vations perceptrons: (1) perceptron learning algorithm cycles hyperplanes, hyperplanes select one split examples, (2) perceptron build hyper- plane separates one rest. Extentron grows multi-layer networks capable distinguishing non- linearly-separable data perceptron rule linear threshold units. algorithm simple, fast, scales prob lems, retains convergence properties perceptron, two parameters. comparing Extentron neural network paradigms symbolic learning systems.",Neural_Networks
90655,Title: Word Perfect TRANSFORMATION IMPLEMENTING NEURAL NETWORKS LOCALIST PROPERTIES,"Abstract: Artificial Neural Networks (ANNs) fixed topology learning, typically suffer shortcomings result. Variations ANNs dynamic topologies ability overcome problems. introduces Location-Independent Transformations (LITs) strategy implementing feedforward networks dynamic topologies. LIT creates location-independent nodes, node computes network output independent nodes, local information. transformation efficient support adding deleting nodes dynamically learning. particular, LITs single-layer competitve learning network, counterpropagation network, combines elements supervised learning competitive learning. two networks localist sense ultimately one node responsible output. LITs models papers.",Neural_Networks
42156,Title: Models Parallel Adaptive Logic,Abstract: overviews architecture adaptive parallel logic referred ASOCS (Adaptive Self-Organizing Concurrent System). ASOCS adaptive network composed computing elements operate parallel asynchronous fashion. specification if-then rules form boolean conjunctions. Rules incrementally adapts changing rule-base. Adaptation data processing form two separate phases operation. processing acts parallel hardware circuit. adaptation distributed computing elements efficiently exploits parallelism. Adaptation self-organizing fashion linear depth network. summarizes ASOCS overviews three specific architectures.,Neural_Networks
